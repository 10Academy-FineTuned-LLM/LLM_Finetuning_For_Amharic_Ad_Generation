{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>instruction</th>\n",
       "      <th>input</th>\n",
       "      <th>output</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1604</th>\n",
       "      <td>Categorize the input as advertisement or not a...</td>\n",
       "      <td>рІерѕЮріЦрѕФрЅБрІЇрІФріЋ рѕЏріЦрЅђрЅЦ ріарІГрЅарїЇрѕеріЋрѕЮ рЇЉрЅ▓ріЋ рІерѕЮріЦрѕФрЅБрІЇрІФріЋ рѕЏріЦрЅђрЅЦ рІѕрІ░ рЇірЅх ...</td>\n",
       "      <td>not advertisement</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1605</th>\n",
       "      <td>Categorize the input as advertisement or not a...</td>\n",
       "      <td>рІерѕХрѕхрЅхрІ«рѕй рІхрѕГрІхрѕЕ рІЏрѕг рЅаріарІ▓рѕх ріарЅарЅБ рЅ░рїђрѕЮрѕ»рѕЇрЇб рІерЅ│рѕІрЅЂ рѕЋрІ│рѕ┤ рїЇрІхрЅЦ рІеріб...</td>\n",
       "      <td>not advertisement</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1606</th>\n",
       "      <td>Categorize the input as advertisement or not a...</td>\n",
       "      <td>ріерѕърїє ріерЅ░рѕЏ рЅарІЇрѕхрїЦ рІерІ░рѕерѕ░ріЮ рѕўрѕЇріЦріГрЅх ріљрІЇрЇбрѕхрѕѕрѕўрѕерїЃрІЇ рЅхріГріГрѕѕріЮріљрЅх ріарѕерїІ...</td>\n",
       "      <td>not advertisement</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1607</th>\n",
       "      <td>Categorize the input as advertisement or not a...</td>\n",
       "      <td>Update ріеріарІ▓рѕх ріарЅарЅБ ріерЅ░рѕЏ ріарѕхрЅ░рІ│рІ░рѕГ рЇљрЅЦрѕіріГ рѕ░рѕГрЅфрѕх ріЦріЊ рІерѕ░рІЇ рѕђрЅЦ...</td>\n",
       "      <td>not advertisement</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1608</th>\n",
       "      <td>Categorize the input as advertisement or not a...</td>\n",
       "      <td>рїЁрїЇрїЁрїІ рїѕрЅЦрЅ░рІІрѕЇРђ╝ РюћрІерѕХрѕЏрѕїрѕІріЋрІх рЇЋрѕгрІюрІ│ріЋрЅх рѕЎрѕ┤ рЅБрѕѓ ріарЅЦрІ▓ рѕЇріАріФріЋ рЅАрІхріЊ...</td>\n",
       "      <td>not advertisement</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            instruction  \\\n",
       "1604  Categorize the input as advertisement or not a...   \n",
       "1605  Categorize the input as advertisement or not a...   \n",
       "1606  Categorize the input as advertisement or not a...   \n",
       "1607  Categorize the input as advertisement or not a...   \n",
       "1608  Categorize the input as advertisement or not a...   \n",
       "\n",
       "                                                  input             output  \n",
       "1604  рІерѕЮріЦрѕФрЅБрІЇрІФріЋ рѕЏріЦрЅђрЅЦ ріарІГрЅарїЇрѕеріЋрѕЮ рЇЉрЅ▓ріЋ рІерѕЮріЦрѕФрЅБрІЇрІФріЋ рѕЏріЦрЅђрЅЦ рІѕрІ░ рЇірЅх ...  not advertisement  \n",
       "1605  рІерѕХрѕхрЅхрІ«рѕй рІхрѕГрІхрѕЕ рІЏрѕг рЅаріарІ▓рѕх ріарЅарЅБ рЅ░рїђрѕЮрѕ»рѕЇрЇб рІерЅ│рѕІрЅЂ рѕЋрІ│рѕ┤ рїЇрІхрЅЦ рІеріб...  not advertisement  \n",
       "1606  ріерѕърїє ріерЅ░рѕЏ рЅарІЇрѕхрїЦ рІерІ░рѕерѕ░ріЮ рѕўрѕЇріЦріГрЅх ріљрІЇрЇбрѕхрѕѕрѕўрѕерїЃрІЇ рЅхріГріГрѕѕріЮріљрЅх ріарѕерїІ...  not advertisement  \n",
       "1607  Update ріеріарІ▓рѕх ріарЅарЅБ ріерЅ░рѕЏ ріарѕхрЅ░рІ│рІ░рѕГ рЇљрЅЦрѕіріГ рѕ░рѕГрЅфрѕх ріЦріЊ рІерѕ░рІЇ рѕђрЅЦ...  not advertisement  \n",
       "1608  рїЁрїЇрїЁрїІ рїѕрЅЦрЅ░рІІрѕЇРђ╝ РюћрІерѕХрѕЏрѕїрѕІріЋрІх рЇЋрѕгрІюрІ│ріЋрЅх рѕЎрѕ┤ рЅБрѕѓ ріарЅЦрІ▓ рѕЇріАріФріЋ рЅАрІхріЊ...  not advertisement  "
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_json(\"/data/fine_tun_data2.json\")\n",
    "df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'рѕѕрібрѕгрЅ╗ рЅаріБрѕЇ рІѕрІ░рЅбрѕЙрЇЇрЅ▒ рІерЅ░рїЊрІЎрЅх рІерѕ▓рІ│рѕЏ рІѕрїБрЅХрЅй(ріцрїёрЅХрІјрЅй) ріерѕ░ріБрЅ│рЅх рЅарЇірЅх рЅбрѕЙрЇЇрЅ▒ рїѕрЅЦрЅ░рІІрѕЇрЇб @tsegabwolde @tikvahethiopia'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[df['output']=='not advertisement'].iloc[1]['input']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'Identify whether the given text is an advertisement or not advertisement from the given input. Make sure you respond only with advertisment or not advertisment. NOTHING ELSE. Input: рѕѕрібрѕгрЅ╗ рЅаріБрѕЇ рІѕрІ░рЅбрѕЙрЇЇрЅ▒ рІерЅ░рїЊрІЎрЅх рІерѕ▓рІ│рѕЏ рІѕрїБрЅХрЅй(ріцрїёрЅХрІјрЅй) ріерѕ░ріБрЅ│рЅх рЅарЇірЅх рЅбрѕЙрЇЇрЅ▒ рїѕрЅЦрЅ░рІІрѕЇрЇб @tsegabwolde @tikvahethiopia''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>channel_id</th>\n",
       "      <th>message_id</th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "      <th>email</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1490216693</td>\n",
       "      <td>8</td>\n",
       "      <td>рѕўрѕ░рѕерЅх рЅ░рІ░рѕГрїј рїЦрЅЃрЅх рЅ░рЇѕрЇЁрѕърЅЦріЏрѕЇ рЅарѕџрѕЇ рЅарѕђрѕ░рЅх рІерѕйріЋрЅх рѕўрѕйріЏ рЅарЇІрѕ╗ріЊ рЇЋ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>g2@trainee.com</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1490216693</td>\n",
       "      <td>11</td>\n",
       "      <td>рі«рѕЮрЅдрѕЇрЅ╗­ЪћЮ рІерїЅрІъ ріарІхрІІ рЅ░рїЊрІдрЅй рі«рѕЮрЅдрѕЇрЅ╗ рїѕрЅЦрЅ░рІІрѕЇрЇб рѕ░рѕІрѕЮ ріЦріЋрІ░рѕєріљ ріЦріЊ ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>g2@trainee.com</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>1490216693</td>\n",
       "      <td>13</td>\n",
       "      <td>Рђ╝№ИЈ рЅарІ░рЅАрЅЦ ріГрѕЇрѕЇ рЅарЅ┤рЇњ ріерЅ░рѕЏ ріарІ░рЅБрЅБрІГ рЅарІѕрїА рІѕрїБрЅХрЅй рѕІрІГ рЇђрїЦрЅ│ ріарѕхріерЅБ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>g2@trainee.com</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>1490216693</td>\n",
       "      <td>14</td>\n",
       "      <td>Рђ╝№ИЈ ріерѕЂрѕѕрЅх рІѕрѕФрЅх рЅарЇірЅх рІерѕърЅхріЋ рЅЦрѕГрЅ▒ ріГріЋрІх ріарѕИріЋрЇѕрІЇ рЅ░ріљрѕ▒ рІерЅ░рЅБрѕЅрЅх ріа...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>g2@trainee.com</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>1490216693</td>\n",
       "      <td>15</td>\n",
       "      <td>рЅ┤рѕІрЅфрЅГРђ╝№ИЈ рЅарѕ║рѕЁрІјрЅй рІерѕџрЅєрїарѕЕ рЅцрЅ░ ріЦрѕхрѕФріцрѕІрІЇрІФріЋ ріаріЋрІх рЅаріЦрѕхрѕФріцрѕЇ рЇќрѕірѕх ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>g2@trainee.com</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0  channel_id  message_id  \\\n",
       "0           0  1490216693           8   \n",
       "1           1  1490216693          11   \n",
       "2           2  1490216693          13   \n",
       "3           3  1490216693          14   \n",
       "4           4  1490216693          15   \n",
       "\n",
       "                                                text label           email  \n",
       "0  рѕўрѕ░рѕерЅх рЅ░рІ░рѕГрїј рїЦрЅЃрЅх рЅ░рЇѕрЇЁрѕърЅЦріЏрѕЇ рЅарѕџрѕЇ рЅарѕђрѕ░рЅх рІерѕйріЋрЅх рѕўрѕйріЏ рЅарЇІрѕ╗ріЊ рЇЋ...   NaN  g2@trainee.com  \n",
       "1  рі«рѕЮрЅдрѕЇрЅ╗­ЪћЮ рІерїЅрІъ ріарІхрІІ рЅ░рїЊрІдрЅй рі«рѕЮрЅдрѕЇрЅ╗ рїѕрЅЦрЅ░рІІрѕЇрЇб рѕ░рѕІрѕЮ ріЦріЋрІ░рѕєріљ ріЦріЊ ...   NaN  g2@trainee.com  \n",
       "2  Рђ╝№ИЈ рЅарІ░рЅАрЅЦ ріГрѕЇрѕЇ рЅарЅ┤рЇњ ріерЅ░рѕЏ ріарІ░рЅБрЅБрІГ рЅарІѕрїА рІѕрїБрЅХрЅй рѕІрІГ рЇђрїЦрЅ│ ріарѕхріерЅБ...   NaN  g2@trainee.com  \n",
       "3  Рђ╝№ИЈ ріерѕЂрѕѕрЅх рІѕрѕФрЅх рЅарЇірЅх рІерѕърЅхріЋ рЅЦрѕГрЅ▒ ріГріЋрІх ріарѕИріЋрЇѕрІЇ рЅ░ріљрѕ▒ рІерЅ░рЅБрѕЅрЅх ріа...   NaN  g2@trainee.com  \n",
       "4  рЅ┤рѕІрЅфрЅГРђ╝№ИЈ рЅарѕ║рѕЁрІјрЅй рІерѕџрЅєрїарѕЕ рЅцрЅ░ ріЦрѕхрѕФріцрѕІрІЇрІФріЋ ріаріЋрІх рЅаріЦрѕхрѕФріцрѕЇ рЇќрѕірѕх ...   NaN  g2@trainee.com  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv(\"/data/wasu_mohammed_labeled.csv\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(16051, 6)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>channel_id</th>\n",
       "      <th>message_id</th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "      <th>email</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>16046</th>\n",
       "      <td>16046</td>\n",
       "      <td>1490216693</td>\n",
       "      <td>26113</td>\n",
       "      <td>ріаріЋрІ▓рЅх рібрЅхрІ«рїхрІФрІірЅх рЅарІ▒рЅБрІГ рІе1 рѕџрѕірІ«ріЋ рІХрѕІрѕГ рЅБрѕѕріЦрІхрѕЇ рѕєріЊрѕѕрЅй рЅ░рЅЦрѕЈрѕЇрЇб...</td>\n",
       "      <td>Not Avertisment</td>\n",
       "      <td>g2@trainee.com</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16047</th>\n",
       "      <td>16047</td>\n",
       "      <td>1490216693</td>\n",
       "      <td>26114</td>\n",
       "      <td>ADVERTISMENT ­ЪЈЉ рѕўрѕЇріФрѕЮ рїѕріЊ ­ЪЈЉ ­Ъћю ­ЪњЦрѕѕ3 рЅђріЊрЅх рЅЦрЅ╗ рІерѕџрЅєрІГ рЅ│рѕІрЅЁ...</td>\n",
       "      <td>financial service</td>\n",
       "      <td>g2@trainee.com</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16048</th>\n",
       "      <td>16048</td>\n",
       "      <td>1490216693</td>\n",
       "      <td>26115</td>\n",
       "      <td>рІхрѕгрІјрЅй­ЪЎЈ рІхрѕг ріЦрІЇріљрЅхрѕЮ рІерЇЇрЅЁрѕГ ріерЅ░рѕЏ рѕ│рѕЮріЋрЅ▒ріЋ рѕЎрѕЅ рЅарІЇрѕхрїЦрѕй ріЦріЋрїЇрІ│ рѕєріў...</td>\n",
       "      <td>Not Avertisment</td>\n",
       "      <td>g2@trainee.com</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16049</th>\n",
       "      <td>16049</td>\n",
       "      <td>1490216693</td>\n",
       "      <td>26116</td>\n",
       "      <td>ADVERTISMENT ріарѕхрЅИрі│рІГ рѕйрІФрїГРђдрЅарѕЇрІЕ рЅЁріЊрѕй ріЦріљрІџрѕЁріЋ рїЦрѕФрЅх рІФрѕІрЅИрІЇ ...</td>\n",
       "      <td>retail</td>\n",
       "      <td>g2@trainee.com</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16050</th>\n",
       "      <td>16050</td>\n",
       "      <td>1490216693</td>\n",
       "      <td>26117</td>\n",
       "      <td>рІеріарЇЇрѕфріФ рѕЁрЅЦрѕерЅх рі«рѕџрѕйріЋ рѕірЅђрѕўріЋрЅарѕГ рѕЎрѕ│ рЇІріф рѕЏрѕЃрѕЏрЅх рЅарібрЅхрІ«рїхрІФ ріЦріЊ рЅарѕХ...</td>\n",
       "      <td>Not Avertisment</td>\n",
       "      <td>g2@trainee.com</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Unnamed: 0  channel_id  message_id  \\\n",
       "16046       16046  1490216693       26113   \n",
       "16047       16047  1490216693       26114   \n",
       "16048       16048  1490216693       26115   \n",
       "16049       16049  1490216693       26116   \n",
       "16050       16050  1490216693       26117   \n",
       "\n",
       "                                                    text              label  \\\n",
       "16046  ріаріЋрІ▓рЅх рібрЅхрІ«рїхрІФрІірЅх рЅарІ▒рЅБрІГ рІе1 рѕџрѕірІ«ріЋ рІХрѕІрѕГ рЅБрѕѕріЦрІхрѕЇ рѕєріЊрѕѕрЅй рЅ░рЅЦрѕЈрѕЇрЇб...    Not Avertisment   \n",
       "16047  ADVERTISMENT ­ЪЈЉ рѕўрѕЇріФрѕЮ рїѕріЊ ­ЪЈЉ ­Ъћю ­ЪњЦрѕѕ3 рЅђріЊрЅх рЅЦрЅ╗ рІерѕџрЅєрІГ рЅ│рѕІрЅЁ...  financial service   \n",
       "16048  рІхрѕгрІјрЅй­ЪЎЈ рІхрѕг ріЦрІЇріљрЅхрѕЮ рІерЇЇрЅЁрѕГ ріерЅ░рѕЏ рѕ│рѕЮріЋрЅ▒ріЋ рѕЎрѕЅ рЅарІЇрѕхрїЦрѕй ріЦріЋрїЇрІ│ рѕєріў...    Not Avertisment   \n",
       "16049  ADVERTISMENT ріарѕхрЅИрі│рІГ рѕйрІФрїГРђдрЅарѕЇрІЕ рЅЁріЊрѕй ріЦріљрІџрѕЁріЋ рїЦрѕФрЅх рІФрѕІрЅИрІЇ ...             retail   \n",
       "16050  рІеріарЇЇрѕфріФ рѕЁрЅЦрѕерЅх рі«рѕџрѕйріЋ рѕірЅђрѕўріЋрЅарѕГ рѕЎрѕ│ рЇІріф рѕЏрѕЃрѕЏрЅх рЅарібрЅхрІ«рїхрІФ ріЦріЊ рЅарѕХ...    Not Avertisment   \n",
       "\n",
       "                email  \n",
       "16046  g2@trainee.com  \n",
       "16047  g2@trainee.com  \n",
       "16048  g2@trainee.com  \n",
       "16049  g2@trainee.com  \n",
       "16050  g2@trainee.com  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['label'] = df['label'].fillna(\"Not Advertisement\")\n",
    "df.tail(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset\n",
    "\n",
    "# Create a dictionary containing your Amharic text data\n",
    "data_dict = {\"text\": df['text'].tolist()}\n",
    "\n",
    "# Create a Dataset object\n",
    "dataset = Dataset.from_dict(data_dict)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c897f7ee43a4438e871e39359b139da1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/5357 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataset.save_to_disk(\"../data/train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copyright (c) Meta Platforms, Inc. and affiliates.\n",
    "# This software may be used and distributed according to the terms of the GNU General Public License version 3.\n",
    "\n",
    "from peft import PeftModel\n",
    "from transformers import LlamaForCausalLM, LlamaConfig\n",
    "\n",
    "# Function to load the main model for text generation\n",
    "def load_model(model_name, quantization):\n",
    "    model = LlamaForCausalLM.from_pretrained(\n",
    "        model_name,\n",
    "        return_dict=True,\n",
    "        load_in_8bit=quantization,\n",
    "        device_map=\"auto\",\n",
    "        low_cpu_mem_usage=True,\n",
    "    )\n",
    "    return model\n",
    "\n",
    "\n",
    "# Function to load the PeftModel for performance optimization\n",
    "def load_peft_model(model, peft_model):\n",
    "    peft_model = PeftModel.from_pretrained(model, peft_model)\n",
    "    return peft_model\n",
    "\n",
    "# Loading the model from config to load FSDP checkpoints into that\n",
    "def load_llama_from_config(config_path):\n",
    "    model_config = LlamaConfig.from_pretrained(config_path) \n",
    "    model = LlamaForCausalLM(config=model_config)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['РќЂріа', 'рЇќ', 'рѕј', 'РќЂріФрѕѕ', 'РќЂ\"', 'РќЂрібріЋрЅ░рѕГріћрЅх', 'РќЂрЅ░рЅІрѕГрїд', 'РќЂрїѕріЋрІўрЅЦ', 'РќЂрѕўрѕІріГ', 'рѕЮ', 'РќЂрѕўрЅђрЅарѕЇ', 'рѕЮ', 'РќЂріарѕЇрЅ╗рѕЇ', 'ріЕ', '\"', 'РќЂрѕЏрѕѕрЅх', 'РќЂрІерѕѕрѕЮрЇб', 'РќЂ*', '6', '85', '#', 'РќЂрЅарѕўрІ░', 'рІѕрѕЇ', 'РќЂріарїѕрѕЇрїЇрѕјрЅ▒ріЋ', 'РќЂрІГрїарЅђрѕЎ', 'рЇб']\n"
     ]
    }
   ],
   "source": [
    "from transformers import LlamaTokenizer\n",
    "\n",
    "\n",
    "MAIN_PATH = '/model/Llama-2-7b-hf'\n",
    "tokenizer = LlamaTokenizer.from_pretrained(MAIN_PATH)\n",
    "\n",
    "example = 'ріарЇќрѕј ріФрѕѕ \" рібріЋрЅ░рѕГріћрЅх рЅ░рЅІрѕГрїд рїѕріЋрІўрЅЦ рѕўрѕІріГрѕЮ рѕўрЅђрЅарѕЇрѕЮ ріарѕЇрЅ╗рѕЇріЕ\" рѕЏрѕѕрЅх рІерѕѕрѕЮрЇб *685# рЅарѕўрІ░рІѕрѕЇ ріарїѕрѕЇрїЇрѕјрЅ▒ріЋ рІГрїарЅђрѕЎрЇб'\n",
    "\n",
    "\n",
    "# garri_tokenizer = AutoTokenizer.from_pretrained(\"__the tokenizer__ you guys used\")\n",
    "tokens = tokenizer.tokenize(example)\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "51008\n"
     ]
    }
   ],
   "source": [
    "print(len(tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['РќЂріа', 'рЇќ', 'рѕј', 'РќЂріФрѕѕ', 'РќЂ\"', 'РќЂрібріЋрЅ░рѕГріћрЅх', 'РќЂрЅ░рЅІрѕГрїд', 'РќЂрїѕріЋрІўрЅЦ', 'РќЂрѕўрѕІріГ', 'рѕЮ', 'РќЂрѕўрЅђрЅарѕЇ', 'рѕЮ', 'РќЂріарѕЇрЅ╗рѕЇ', 'ріЕ', '\"', 'РќЂрѕЏрѕѕрЅх', 'РќЂрІерѕѕрѕЮрЇб', 'РќЂ*', '6', '85', '#', 'РќЂрЅарѕўрІ░', 'рІѕрѕЇ', 'РќЂріарїѕрѕЇрїЇрѕјрЅ▒ріЋ', 'РќЂрІГрїарЅђрѕЎ', 'рЇб']\n"
     ]
    }
   ],
   "source": [
    "example = 'ріарЇќрѕј ріФрѕѕ \" рібріЋрЅ░рѕГріћрЅх рЅ░рЅІрѕГрїд рїѕріЋрІўрЅЦ рѕўрѕІріГрѕЮ рѕўрЅђрЅарѕЇрѕЮ ріарѕЇрЅ╗рѕЇріЕ\" рѕЏрѕѕрЅх рІерѕѕрѕЮрЇб *685# рЅарѕўрІ░рІѕрѕЇ ріарїѕрѕЇрїЇрѕјрЅ▒ріЋ рІГрїарЅђрѕЎрЇб'\n",
    "\n",
    "\n",
    "tokens = tokenizer.tokenize(example)\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'рѕўрѕ░рѕерЅх рЅ░рІ░рѕГрїј рїЦрЅЃрЅх рЅ░рЇѕрЇЁрѕърЅЦріЏрѕЇ рЅарѕџрѕЇ рЅарѕђрѕ░рЅх рІерѕйріЋрЅх рѕўрѕйріЏ рЅарЇІрѕ╗ріЊ рЇЋрѕІрѕхрЅ░рѕГ рЅарѕўрїарЅЁрѕѕрѕЇ рїЇрїГрЅх рѕѕрѕўрЅђрѕхрЅђрѕх рІерѕъріерѕерІЇ рїЇрѕѕрѕхрЅЦ рЇќрѕірѕх рЅарЅЂрїЦрїЦрѕГ рѕхрѕГ рІІрѕѕрЇб рЅ░рїарѕГрїБрѕф рѕЮріЋрѕЮ ріарІГріљрЅх рїЅрІ│рЅх рѕ│рІГрІ░рѕГрѕхрЅарЅх рѕєріЋ рЅЦрѕј рЅЦрѕёрѕгріЋ рѕўрѕ░рѕерЅх ріарІхрѕГрїѕрІЇ рЅарЅбрѕІрІІ рІерѕйріЋрЅх рѕўрѕйріЏ рЅЦрѕЇрЅ┤ріЋ рЅєрѕГрїарІЇ рїЅрІ│рЅх ріарІхрѕГрѕ░рІЇрЅЦріЏрѕЇ рЅарѕџрѕЇ рІхрѕГрїірЅ▒ріЋ рІФрѕЇрЇѕрЇђрѕЎ рїЇрѕѕрѕ░рЅдрЅйріЋ рѕхрѕЮ рЅарѕўрїЦрЅђрѕх рѕѕрЇќрѕірѕх ріарЅцрЅ▒рЅ│ рІФрЅђрѕерЅарІЇрЇб рЇќрѕірѕх рІерЅђрѕерЅарІЇріЋ ріарЅцрЅ▒рЅ│ рЅ░рЅђрЅЦрѕј рѕЂріћрЅ│рІЇріЋ рѕѕрѕЏрїБрѕФрЅх рЅ░ріерѕ│рѕйріЋ рІѕрІ░ рѕЁріГрѕЮріЊ рІерѕІріе рѕ▓рѕєріЋ рЅарЅ░рІ░рѕерїѕ рІерѕЁріГрѕЮріЊ рѕЮрѕГрѕўрѕФ рїЇрѕѕрѕ░рЅА рѕІрІГ рѕЮріЋрѕЮ ріарІГріљрЅх рїЦрЅЃрЅх рѕЏрѕерїІрїѕрїЦ рѕўрЅ╗рѕЅріЋ рЅ░ріЊрїЇрѕ»рѕЇрЇб ріарѕЂріЋ рѕІрІГ рѕхрѕГ рІЇрѕј рѕЮрѕГрѕўрѕФрІЇ ріЦрІерЅ░рїБрѕФ рѕўрѕєріЉріЋ рЇќрѕірѕх рїерѕЮрѕ« рїѕрѕЇрЇє рѕЏріЋріЏрІЇрѕЮ рѕ░рІЇ рЅарІюрїјрЅй рѕўріФріерѕЇ рїЇрїГрЅХрЅйріЊ ріарѕѕрѕўрїЇрЅБрЅБрЅХрЅй ріЦріЋрІ▓рЇѕрїарѕЕ ріерѕџрІФрІ░рѕГрїЅ рЅ░рїЇрЅБрѕФрЅх рѕірЅєрїарЅЦріЊ рѕірѕГрЅЁ ріЦріЋрІ░рѕџрїѕрЅБ ріарѕ│рѕхрЅДрѕЇ:: рѕЮріЋрїГрЇд рІхрѕг рЇќрѕірѕх'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['text'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "unsupported operand type(s) for +=: 'int' and 'list'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 20\u001b[0m\n\u001b[1;32m     18\u001b[0m     total_word_count \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m word_count\n\u001b[1;32m     19\u001b[0m     tokens \u001b[38;5;241m=\u001b[39m tokenizer\u001b[38;5;241m.\u001b[39mtokenize(text)\n\u001b[0;32m---> 20\u001b[0m     total_tokens\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39mtokens\n\u001b[1;32m     21\u001b[0m     \u001b[38;5;28mprint\u001b[39m(tokens)\n\u001b[1;32m     25\u001b[0m \u001b[38;5;66;03m# Display the total word count\u001b[39;00m\n",
      "\u001b[0;31mTypeError\u001b[0m: unsupported operand type(s) for +=: 'int' and 'list'"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(\"/data/wasu_mohammed_labeled.csv\")\n",
    "\n",
    "# Initialize a variable to accumulate the total word count\n",
    "total_word_count = 0\n",
    "total_tokens = 0\n",
    "# Iterate through each row of the DataFrame\n",
    "for index, row in df.iterrows():\n",
    "    # Get the text from the specified column\n",
    "    text = row['text']\n",
    "    if not isinstance(text, str): \n",
    "        continue\n",
    "\n",
    "    \n",
    "    # Count the number of words in the text\n",
    "    word_count = len(text.split())\n",
    "\n",
    "    # Accumulate the word count\n",
    "    total_word_count += word_count\n",
    "    tokens = tokenizer.tokenize(text)\n",
    "    total_tokens+=tokens\n",
    "    print(tokens)\n",
    "    \n",
    "\n",
    "\n",
    "# Display the total word count\n",
    "print(\"Total Word Count:\", total_word_count)\n",
    "print(\"Total tokens count: \",total_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(16051, 6)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    BitsAndBytesConfig,\n",
    "    HfArgumentParser,\n",
    "    TrainingArguments,\n",
    "    pipeline,\n",
    "    LlamaForCausalLM, \n",
    "    LlamaTokenizer,\n",
    "    logging,\n",
    ")\n",
    "from peft import (\n",
    "    LoraConfig,\n",
    "    TaskType,\n",
    "    prepare_model_for_int8_training,\n",
    "    PeftModel\n",
    ")\n",
    "import torch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "783c2391b3494a82a3782f2d96ed894b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "resize the embedding size by the size of the tokenizer\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The model 'PeftModelForCausalLM' is not supported for text-generation. Supported models are ['BartForCausalLM', 'BertLMHeadModel', 'BertGenerationDecoder', 'BigBirdForCausalLM', 'BigBirdPegasusForCausalLM', 'BioGptForCausalLM', 'BlenderbotForCausalLM', 'BlenderbotSmallForCausalLM', 'BloomForCausalLM', 'CamembertForCausalLM', 'LlamaForCausalLM', 'CodeGenForCausalLM', 'CpmAntForCausalLM', 'CTRLLMHeadModel', 'Data2VecTextForCausalLM', 'ElectraForCausalLM', 'ErnieForCausalLM', 'FalconForCausalLM', 'FuyuForCausalLM', 'GitForCausalLM', 'GPT2LMHeadModel', 'GPT2LMHeadModel', 'GPTBigCodeForCausalLM', 'GPTNeoForCausalLM', 'GPTNeoXForCausalLM', 'GPTNeoXJapaneseForCausalLM', 'GPTJForCausalLM', 'LlamaForCausalLM', 'MarianForCausalLM', 'MBartForCausalLM', 'MegaForCausalLM', 'MegatronBertForCausalLM', 'MistralForCausalLM', 'MixtralForCausalLM', 'MptForCausalLM', 'MusicgenForCausalLM', 'MvpForCausalLM', 'OpenLlamaForCausalLM', 'OpenAIGPTLMHeadModel', 'OPTForCausalLM', 'PegasusForCausalLM', 'PersimmonForCausalLM', 'PhiForCausalLM', 'PLBartForCausalLM', 'ProphetNetForCausalLM', 'QDQBertLMHeadModel', 'Qwen2ForCausalLM', 'ReformerModelWithLMHead', 'RemBertForCausalLM', 'RobertaForCausalLM', 'RobertaPreLayerNormForCausalLM', 'RoCBertForCausalLM', 'RoFormerForCausalLM', 'RwkvForCausalLM', 'Speech2Text2ForCausalLM', 'TransfoXLLMHeadModel', 'TrOCRForCausalLM', 'WhisperForCausalLM', 'XGLMForCausalLM', 'XLMWithLMHeadModel', 'XLMProphetNetForCausalLM', 'XLMRobertaForCausalLM', 'XLMRobertaXLForCausalLM', 'XLNetLMHeadModel', 'XmodForCausalLM'].\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "LlamaForCausalLM.forward() got an unexpected keyword argument 'return_tensors'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 18\u001b[0m\n\u001b[1;32m     16\u001b[0m prompt \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWho is Leonardo Da Vinci?\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     17\u001b[0m pipe \u001b[38;5;241m=\u001b[39m pipeline(task\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtext-generation\u001b[39m\u001b[38;5;124m\"\u001b[39m, model\u001b[38;5;241m=\u001b[39mmodel, tokenizer\u001b[38;5;241m=\u001b[39mmodel, max_length\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m200\u001b[39m)\n\u001b[0;32m---> 18\u001b[0m result \u001b[38;5;241m=\u001b[39m pipe(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m<s>[INST] \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mprompt\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m [/INST]\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28mprint\u001b[39m(result[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgenerated_text\u001b[39m\u001b[38;5;124m'\u001b[39m])\n",
      "File \u001b[0;32m/opt/miniconda/lib/python3.11/site-packages/transformers/pipelines/text_generation.py:219\u001b[0m, in \u001b[0;36mTextGenerationPipeline.__call__\u001b[0;34m(self, text_inputs, **kwargs)\u001b[0m\n\u001b[1;32m    178\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, text_inputs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    179\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    180\u001b[0m \u001b[38;5;124;03m    Complete the prompt(s) given as inputs.\u001b[39;00m\n\u001b[1;32m    181\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    217\u001b[0m \u001b[38;5;124;03m          ids of the generated text.\u001b[39;00m\n\u001b[1;32m    218\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 219\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m(text_inputs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m/opt/miniconda/lib/python3.11/site-packages/transformers/pipelines/base.py:1162\u001b[0m, in \u001b[0;36mPipeline.__call__\u001b[0;34m(self, inputs, num_workers, batch_size, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1154\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mnext\u001b[39m(\n\u001b[1;32m   1155\u001b[0m         \u001b[38;5;28miter\u001b[39m(\n\u001b[1;32m   1156\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_iterator(\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1159\u001b[0m         )\n\u001b[1;32m   1160\u001b[0m     )\n\u001b[1;32m   1161\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1162\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrun_single(inputs, preprocess_params, forward_params, postprocess_params)\n",
      "File \u001b[0;32m/opt/miniconda/lib/python3.11/site-packages/transformers/pipelines/base.py:1168\u001b[0m, in \u001b[0;36mPipeline.run_single\u001b[0;34m(self, inputs, preprocess_params, forward_params, postprocess_params)\u001b[0m\n\u001b[1;32m   1167\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrun_single\u001b[39m(\u001b[38;5;28mself\u001b[39m, inputs, preprocess_params, forward_params, postprocess_params):\n\u001b[0;32m-> 1168\u001b[0m     model_inputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpreprocess(inputs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mpreprocess_params)\n\u001b[1;32m   1169\u001b[0m     model_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mforward(model_inputs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mforward_params)\n\u001b[1;32m   1170\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpostprocess(model_outputs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mpostprocess_params)\n",
      "File \u001b[0;32m/opt/miniconda/lib/python3.11/site-packages/transformers/pipelines/text_generation.py:232\u001b[0m, in \u001b[0;36mTextGenerationPipeline.preprocess\u001b[0;34m(self, prompt_text, prefix, handle_long_generation, add_special_tokens, truncation, padding, max_length, **generate_kwargs)\u001b[0m\n\u001b[1;32m    221\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpreprocess\u001b[39m(\n\u001b[1;32m    222\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    223\u001b[0m     prompt_text,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    230\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mgenerate_kwargs,\n\u001b[1;32m    231\u001b[0m ):\n\u001b[0;32m--> 232\u001b[0m     inputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtokenizer(\n\u001b[1;32m    233\u001b[0m         prefix \u001b[38;5;241m+\u001b[39m prompt_text,\n\u001b[1;32m    234\u001b[0m         return_tensors\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mframework,\n\u001b[1;32m    235\u001b[0m         truncation\u001b[38;5;241m=\u001b[39mtruncation,\n\u001b[1;32m    236\u001b[0m         padding\u001b[38;5;241m=\u001b[39mpadding,\n\u001b[1;32m    237\u001b[0m         max_length\u001b[38;5;241m=\u001b[39mmax_length,\n\u001b[1;32m    238\u001b[0m         add_special_tokens\u001b[38;5;241m=\u001b[39madd_special_tokens,\n\u001b[1;32m    239\u001b[0m     )\n\u001b[1;32m    240\u001b[0m     inputs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprompt_text\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m prompt_text\n\u001b[1;32m    242\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m handle_long_generation \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhole\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "File \u001b[0;32m/opt/miniconda/lib/python3.11/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m/opt/miniconda/lib/python3.11/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/opt/miniconda/lib/python3.11/site-packages/peft/peft_model.py:1083\u001b[0m, in \u001b[0;36mPeftModelForCausalLM.forward\u001b[0;34m(self, input_ids, attention_mask, inputs_embeds, labels, output_attentions, output_hidden_states, return_dict, task_ids, **kwargs)\u001b[0m\n\u001b[1;32m   1081\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m peft_config\u001b[38;5;241m.\u001b[39mpeft_type \u001b[38;5;241m==\u001b[39m PeftType\u001b[38;5;241m.\u001b[39mPOLY:\n\u001b[1;32m   1082\u001b[0m         kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtask_ids\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m task_ids\n\u001b[0;32m-> 1083\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbase_model(\n\u001b[1;32m   1084\u001b[0m         input_ids\u001b[38;5;241m=\u001b[39minput_ids,\n\u001b[1;32m   1085\u001b[0m         attention_mask\u001b[38;5;241m=\u001b[39mattention_mask,\n\u001b[1;32m   1086\u001b[0m         inputs_embeds\u001b[38;5;241m=\u001b[39minputs_embeds,\n\u001b[1;32m   1087\u001b[0m         labels\u001b[38;5;241m=\u001b[39mlabels,\n\u001b[1;32m   1088\u001b[0m         output_attentions\u001b[38;5;241m=\u001b[39moutput_attentions,\n\u001b[1;32m   1089\u001b[0m         output_hidden_states\u001b[38;5;241m=\u001b[39moutput_hidden_states,\n\u001b[1;32m   1090\u001b[0m         return_dict\u001b[38;5;241m=\u001b[39mreturn_dict,\n\u001b[1;32m   1091\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m   1092\u001b[0m     )\n\u001b[1;32m   1094\u001b[0m batch_size \u001b[38;5;241m=\u001b[39m _get_batch_size(input_ids, inputs_embeds)\n\u001b[1;32m   1095\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m attention_mask \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1096\u001b[0m     \u001b[38;5;66;03m# concat prompt attention mask\u001b[39;00m\n",
      "File \u001b[0;32m/opt/miniconda/lib/python3.11/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m/opt/miniconda/lib/python3.11/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/opt/miniconda/lib/python3.11/site-packages/peft/tuners/tuners_utils.py:161\u001b[0m, in \u001b[0;36mBaseTuner.forward\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    160\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any):\n\u001b[0;32m--> 161\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mforward(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m/opt/miniconda/lib/python3.11/site-packages/accelerate/hooks.py:166\u001b[0m, in \u001b[0;36madd_hook_to_module.<locals>.new_forward\u001b[0;34m(module, *args, **kwargs)\u001b[0m\n\u001b[1;32m    164\u001b[0m         output \u001b[38;5;241m=\u001b[39m module\u001b[38;5;241m.\u001b[39m_old_forward(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    165\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 166\u001b[0m     output \u001b[38;5;241m=\u001b[39m module\u001b[38;5;241m.\u001b[39m_old_forward(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    167\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m module\u001b[38;5;241m.\u001b[39m_hf_hook\u001b[38;5;241m.\u001b[39mpost_forward(module, output)\n",
      "\u001b[0;31mTypeError\u001b[0m: LlamaForCausalLM.forward() got an unexpected keyword argument 'return_tensors'"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "LLAMA_DIR = '/model/Llama-2-7b-hf'\n",
    "tokenizer = LlamaTokenizer.from_pretrained(LLAMA_DIR)\n",
    "\n",
    "model = LlamaForCausalLM.from_pretrained(LLAMA_DIR, load_in_8bit=True, device_map='auto', torch_dtype=torch.float16)\n",
    "embedding_size = model.get_input_embeddings().weight.shape[0]\n",
    "\n",
    "if len(tokenizer) != embedding_size:\n",
    "    print(\"resize the embedding size by the size of the tokenizer\")\n",
    "    model.resize_token_embeddings(len(tokenizer))\n",
    "\n",
    "\n",
    "new_model ='/home/abdulhamid_mussa/LLM_Finetuning_For_Amharic_Ad_Generation/output'\n",
    "model = PeftModel.from_pretrained(model, new_model)\n",
    "\n",
    "# Run text generation pipeline with our next model\n",
    "prompt = \"Who is Leonardo Da Vinci?\"\n",
    "pipe = pipeline(task=\"text-generation\", model=model, tokenizer=model, max_length=200)\n",
    "result = pipe(f\"<s>[INST] {prompt} [/INST]\")\n",
    "print(result[0]['generated_text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "рЅерѕГрЅ╣рІІрѕЇ рѕерІ│рЅХрЅй ріЦріЋрІ░ Amazon&#39;s AlexaрЇБ Apple&#39;s Siri ріЦріЊ Google Assistant рІФрѕЅ рІерЅ░рѕѕрІФрІЕ рЅБрѕЁрѕфрІФрЅх ріЦріЊ рЅйрѕјрЅ│рІјрЅй ріарѕЈрЅИрІЇрЇб ріљрїѕрѕГ рїЇріЋ рЅаріарїарЅЃрѕІрІГрЇБ рѕЮріЊрЅБрІі рѕерІ│рЅХрЅй ріЦріЋрІ░ рІерїЇрѕЇ рѕерІ│рЅХрЅй рѕєріљрІЇ рІФрїѕрѕѕрїЇрѕІрѕЅрЇБ рѕѕрЅ░рїарЅЃрѕџрІјрЅй ріЦріЋрІ░ ріарѕхрЅ│рІІрѕЙрЅйрЇБ рѕЎрІџрЅЃ рѕўрїФрІѕрЅхрЇБ рІеріарІерѕГ рѕЂріћрЅ│ рІЮрѕўріЊрІјрЅй ріЦріЊ рїЦрІФрЅёрІјрЅйріЋ рѕўрѕўрѕѕрѕх рІФрѕЅ рЅ░рїЇрЅБрѕФрЅхріЋ рІФріеріЊрІЇріЊрѕЅрЇб рѕЮріЊрЅБрІі рѕерІ│рЅХрЅй ріЦріЋрІ░ рІерїЇрѕЇ рѕерІ│рЅХрЅй рѕєріљрІЇ рІФрїѕрѕѕрїЇрѕІрѕЅрЇБ рѕѕрЅ░рїарЅЃрѕџрІјрЅй ріЦріЋрІ░ ріарѕхрЅ│рІІрѕЙрЅйрЇБ рѕЎрІџрЅЃ рѕўрїФрІѕрЅхрЇБ рІеріарІерѕГ рѕЂріћрЅ│ рІЮрѕўріЊрІјрЅй ріЦріЊ рїЦрІФрЅёрІјрЅйріЋ рѕўрѕўрѕѕрѕх рІФрѕЅ рЅ░рїЇрЅБрѕФрЅхріЋ рІФріеріЊрІЇріЊрѕЅрЇб рѕЮріЊрЅБрІі рѕерІ│рЅХрЅй ріЦріЋрІ░ рІерїЇрѕЇ рѕерІ│рЅХрЅй рѕєріљрІЇ рІФрїѕрѕѕрїЇрѕІрѕЅрЇБ рѕѕрЅ░рїарЅЃрѕџрІјрЅй ріЦріЋрІ░ ріарѕхрЅ│рІІрѕЙрЅйрЇБ рѕЎрІџрЅЃ рѕўрїФрІѕрЅхрЇБ рІеріарІерѕГ рѕЂріћрЅ│ рІЮрѕўріЊрІјрЅй ріЦріЊ рїЦрІФрЅёрІјрЅйріЋ рѕўрѕўрѕѕрѕх рІФрѕЅ рЅ░рїЇрЅБрѕФрЅхріЋ рІФріеріЊрІЇріЊрѕЅрЇб"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
