{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>instruction</th>\n",
       "      <th>input</th>\n",
       "      <th>output</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1604</th>\n",
       "      <td>Categorize the input as advertisement or not a...</td>\n",
       "      <td>á‹¨áˆáŠ¥áˆ«á‰£á‹á‹«áŠ• áˆ›áŠ¥á‰€á‰¥ áŠ á‹­á‰ áŒáˆ¨áŠ•áˆ á‘á‰²áŠ• á‹¨áˆáŠ¥áˆ«á‰£á‹á‹«áŠ• áˆ›áŠ¥á‰€á‰¥ á‹ˆá‹° áŠá‰µ ...</td>\n",
       "      <td>not advertisement</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1605</th>\n",
       "      <td>Categorize the input as advertisement or not a...</td>\n",
       "      <td>á‹¨áˆ¶áˆµá‰µá‹®áˆ½ á‹µáˆ­á‹µáˆ© á‹›áˆ¬ á‰ áŠ á‹²áˆµ áŠ á‰ á‰£ á‰°áŒ€áˆáˆ¯áˆá¢ á‹¨á‰³áˆ‹á‰ áˆ•á‹³áˆ´ áŒá‹µá‰¥ á‹¨áŠ¢...</td>\n",
       "      <td>not advertisement</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1606</th>\n",
       "      <td>Categorize the input as advertisement or not a...</td>\n",
       "      <td>áŠ¨áˆáŒ† áŠ¨á‰°áˆ› á‰ á‹áˆµáŒ¥ á‹¨á‹°áˆ¨áˆ°áŠ áˆ˜áˆáŠ¥áŠ­á‰µ áŠá‹á¢áˆµáˆˆáˆ˜áˆ¨áŒƒá‹ á‰µáŠ­áŠ­áˆˆáŠáŠá‰µ áŠ áˆ¨áŒ‹...</td>\n",
       "      <td>not advertisement</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1607</th>\n",
       "      <td>Categorize the input as advertisement or not a...</td>\n",
       "      <td>Update áŠ¨áŠ á‹²áˆµ áŠ á‰ á‰£ áŠ¨á‰°áˆ› áŠ áˆµá‰°á‹³á‹°áˆ­ áá‰¥áˆŠáŠ­ áˆ°áˆ­á‰ªáˆµ áŠ¥áŠ“ á‹¨áˆ°á‹ áˆ€á‰¥...</td>\n",
       "      <td>not advertisement</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1608</th>\n",
       "      <td>Categorize the input as advertisement or not a...</td>\n",
       "      <td>áŒ…áŒáŒ…áŒ‹ áŒˆá‰¥á‰°á‹‹áˆâ€¼ âœ”á‹¨áˆ¶áˆ›áˆŒáˆ‹áŠ•á‹µ á•áˆ¬á‹œá‹³áŠ•á‰µ áˆ™áˆ´ á‰£áˆ‚ áŠ á‰¥á‹² áˆáŠ¡áŠ«áŠ• á‰¡á‹µáŠ“...</td>\n",
       "      <td>not advertisement</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            instruction  \\\n",
       "1604  Categorize the input as advertisement or not a...   \n",
       "1605  Categorize the input as advertisement or not a...   \n",
       "1606  Categorize the input as advertisement or not a...   \n",
       "1607  Categorize the input as advertisement or not a...   \n",
       "1608  Categorize the input as advertisement or not a...   \n",
       "\n",
       "                                                  input             output  \n",
       "1604  á‹¨áˆáŠ¥áˆ«á‰£á‹á‹«áŠ• áˆ›áŠ¥á‰€á‰¥ áŠ á‹­á‰ áŒáˆ¨áŠ•áˆ á‘á‰²áŠ• á‹¨áˆáŠ¥áˆ«á‰£á‹á‹«áŠ• áˆ›áŠ¥á‰€á‰¥ á‹ˆá‹° áŠá‰µ ...  not advertisement  \n",
       "1605  á‹¨áˆ¶áˆµá‰µá‹®áˆ½ á‹µáˆ­á‹µáˆ© á‹›áˆ¬ á‰ áŠ á‹²áˆµ áŠ á‰ á‰£ á‰°áŒ€áˆáˆ¯áˆá¢ á‹¨á‰³áˆ‹á‰ áˆ•á‹³áˆ´ áŒá‹µá‰¥ á‹¨áŠ¢...  not advertisement  \n",
       "1606  áŠ¨áˆáŒ† áŠ¨á‰°áˆ› á‰ á‹áˆµáŒ¥ á‹¨á‹°áˆ¨áˆ°áŠ áˆ˜áˆáŠ¥áŠ­á‰µ áŠá‹á¢áˆµáˆˆáˆ˜áˆ¨áŒƒá‹ á‰µáŠ­áŠ­áˆˆáŠáŠá‰µ áŠ áˆ¨áŒ‹...  not advertisement  \n",
       "1607  Update áŠ¨áŠ á‹²áˆµ áŠ á‰ á‰£ áŠ¨á‰°áˆ› áŠ áˆµá‰°á‹³á‹°áˆ­ áá‰¥áˆŠáŠ­ áˆ°áˆ­á‰ªáˆµ áŠ¥áŠ“ á‹¨áˆ°á‹ áˆ€á‰¥...  not advertisement  \n",
       "1608  áŒ…áŒáŒ…áŒ‹ áŒˆá‰¥á‰°á‹‹áˆâ€¼ âœ”á‹¨áˆ¶áˆ›áˆŒáˆ‹áŠ•á‹µ á•áˆ¬á‹œá‹³áŠ•á‰µ áˆ™áˆ´ á‰£áˆ‚ áŠ á‰¥á‹² áˆáŠ¡áŠ«áŠ• á‰¡á‹µáŠ“...  not advertisement  "
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_json(\"/data/fine_tun_data2.json\")\n",
    "df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'áˆˆáŠ¢áˆ¬á‰» á‰ áŠ£áˆ á‹ˆá‹°á‰¢áˆ¾áá‰± á‹¨á‰°áŒ“á‹™á‰µ á‹¨áˆ²á‹³áˆ› á‹ˆáŒ£á‰¶á‰½(áŠ¤áŒ„á‰¶á‹á‰½) áŠ¨áˆ°áŠ£á‰³á‰µ á‰ áŠá‰µ á‰¢áˆ¾áá‰± áŒˆá‰¥á‰°á‹‹áˆá¢ @tsegabwolde @tikvahethiopia'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[df['output']=='not advertisement'].iloc[1]['input']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'Identify whether the given text is an advertisement or not advertisement from the given input. Make sure you respond only with advertisment or not advertisment. NOTHING ELSE. Input: áˆˆáŠ¢áˆ¬á‰» á‰ áŠ£áˆ á‹ˆá‹°á‰¢áˆ¾áá‰± á‹¨á‰°áŒ“á‹™á‰µ á‹¨áˆ²á‹³áˆ› á‹ˆáŒ£á‰¶á‰½(áŠ¤áŒ„á‰¶á‹á‰½) áŠ¨áˆ°áŠ£á‰³á‰µ á‰ áŠá‰µ á‰¢áˆ¾áá‰± áŒˆá‰¥á‰°á‹‹áˆá¢ @tsegabwolde @tikvahethiopia''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>channel_id</th>\n",
       "      <th>message_id</th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "      <th>email</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1490216693</td>\n",
       "      <td>8</td>\n",
       "      <td>áˆ˜áˆ°áˆ¨á‰µ á‰°á‹°áˆ­áŒ áŒ¥á‰ƒá‰µ á‰°áˆá…áˆá‰¥áŠ›áˆ á‰ áˆšáˆ á‰ áˆ€áˆ°á‰µ á‹¨áˆ½áŠ•á‰µ áˆ˜áˆ½áŠ› á‰ á‹áˆ»áŠ“ á•...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>g2@trainee.com</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1490216693</td>\n",
       "      <td>11</td>\n",
       "      <td>áŠ®áˆá‰¦áˆá‰»ğŸ” á‹¨áŒ‰á‹ áŠ á‹µá‹‹ á‰°áŒ“á‹¦á‰½ áŠ®áˆá‰¦áˆá‰» áŒˆá‰¥á‰°á‹‹áˆá¢ áˆ°áˆ‹áˆ áŠ¥áŠ•á‹°áˆ†áŠ áŠ¥áŠ“ ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>g2@trainee.com</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>1490216693</td>\n",
       "      <td>13</td>\n",
       "      <td>â€¼ï¸ á‰ á‹°á‰¡á‰¥ áŠ­áˆáˆ á‰ á‰´á’ áŠ¨á‰°áˆ› áŠ á‹°á‰£á‰£á‹­ á‰ á‹ˆáŒ¡ á‹ˆáŒ£á‰¶á‰½ áˆ‹á‹­ á€áŒ¥á‰³ áŠ áˆµáŠ¨á‰£...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>g2@trainee.com</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>1490216693</td>\n",
       "      <td>14</td>\n",
       "      <td>â€¼ï¸ áŠ¨áˆáˆˆá‰µ á‹ˆáˆ«á‰µ á‰ áŠá‰µ á‹¨áˆá‰µáŠ• á‰¥áˆ­á‰± áŠ­áŠ•á‹µ áŠ áˆ¸áŠ•áˆá‹ á‰°áŠáˆ± á‹¨á‰°á‰£áˆ‰á‰µ áŠ ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>g2@trainee.com</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>1490216693</td>\n",
       "      <td>15</td>\n",
       "      <td>á‰´áˆ‹á‰ªá‰­â€¼ï¸ á‰ áˆºáˆ…á‹á‰½ á‹¨áˆšá‰†áŒ áˆ© á‰¤á‰° áŠ¥áˆµáˆ«áŠ¤áˆ‹á‹á‹«áŠ• áŠ áŠ•á‹µ á‰ áŠ¥áˆµáˆ«áŠ¤áˆ á–áˆŠáˆµ ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>g2@trainee.com</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0  channel_id  message_id  \\\n",
       "0           0  1490216693           8   \n",
       "1           1  1490216693          11   \n",
       "2           2  1490216693          13   \n",
       "3           3  1490216693          14   \n",
       "4           4  1490216693          15   \n",
       "\n",
       "                                                text label           email  \n",
       "0  áˆ˜áˆ°áˆ¨á‰µ á‰°á‹°áˆ­áŒ áŒ¥á‰ƒá‰µ á‰°áˆá…áˆá‰¥áŠ›áˆ á‰ áˆšáˆ á‰ áˆ€áˆ°á‰µ á‹¨áˆ½áŠ•á‰µ áˆ˜áˆ½áŠ› á‰ á‹áˆ»áŠ“ á•...   NaN  g2@trainee.com  \n",
       "1  áŠ®áˆá‰¦áˆá‰»ğŸ” á‹¨áŒ‰á‹ áŠ á‹µá‹‹ á‰°áŒ“á‹¦á‰½ áŠ®áˆá‰¦áˆá‰» áŒˆá‰¥á‰°á‹‹áˆá¢ áˆ°áˆ‹áˆ áŠ¥áŠ•á‹°áˆ†áŠ áŠ¥áŠ“ ...   NaN  g2@trainee.com  \n",
       "2  â€¼ï¸ á‰ á‹°á‰¡á‰¥ áŠ­áˆáˆ á‰ á‰´á’ áŠ¨á‰°áˆ› áŠ á‹°á‰£á‰£á‹­ á‰ á‹ˆáŒ¡ á‹ˆáŒ£á‰¶á‰½ áˆ‹á‹­ á€áŒ¥á‰³ áŠ áˆµáŠ¨á‰£...   NaN  g2@trainee.com  \n",
       "3  â€¼ï¸ áŠ¨áˆáˆˆá‰µ á‹ˆáˆ«á‰µ á‰ áŠá‰µ á‹¨áˆá‰µáŠ• á‰¥áˆ­á‰± áŠ­áŠ•á‹µ áŠ áˆ¸áŠ•áˆá‹ á‰°áŠáˆ± á‹¨á‰°á‰£áˆ‰á‰µ áŠ ...   NaN  g2@trainee.com  \n",
       "4  á‰´áˆ‹á‰ªá‰­â€¼ï¸ á‰ áˆºáˆ…á‹á‰½ á‹¨áˆšá‰†áŒ áˆ© á‰¤á‰° áŠ¥áˆµáˆ«áŠ¤áˆ‹á‹á‹«áŠ• áŠ áŠ•á‹µ á‰ áŠ¥áˆµáˆ«áŠ¤áˆ á–áˆŠáˆµ ...   NaN  g2@trainee.com  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv(\"/data/wasu_mohammed_labeled.csv\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(16051, 6)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>channel_id</th>\n",
       "      <th>message_id</th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "      <th>email</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>16046</th>\n",
       "      <td>16046</td>\n",
       "      <td>1490216693</td>\n",
       "      <td>26113</td>\n",
       "      <td>áŠ áŠ•á‹²á‰µ áŠ¢á‰µá‹®áŒµá‹«á‹Šá‰µ á‰ á‹±á‰£á‹­ á‹¨1 áˆšáˆŠá‹®áŠ• á‹¶áˆ‹áˆ­ á‰£áˆˆáŠ¥á‹µáˆ áˆ†áŠ“áˆˆá‰½ á‰°á‰¥áˆáˆá¢...</td>\n",
       "      <td>Not Avertisment</td>\n",
       "      <td>g2@trainee.com</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16047</th>\n",
       "      <td>16047</td>\n",
       "      <td>1490216693</td>\n",
       "      <td>26114</td>\n",
       "      <td>ADVERTISMENT ğŸ‘ áˆ˜áˆáŠ«áˆ áŒˆáŠ“ ğŸ‘ ğŸ”œ ğŸ’¥áˆˆ3 á‰€áŠ“á‰µ á‰¥á‰» á‹¨áˆšá‰†á‹­ á‰³áˆ‹á‰…...</td>\n",
       "      <td>financial service</td>\n",
       "      <td>g2@trainee.com</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16048</th>\n",
       "      <td>16048</td>\n",
       "      <td>1490216693</td>\n",
       "      <td>26115</td>\n",
       "      <td>á‹µáˆ¬á‹á‰½ğŸ™ á‹µáˆ¬ áŠ¥á‹áŠá‰µáˆ á‹¨áá‰…áˆ­ áŠ¨á‰°áˆ› áˆ³áˆáŠ•á‰±áŠ• áˆ™áˆ‰ á‰ á‹áˆµáŒ¥áˆ½ áŠ¥áŠ•áŒá‹³ áˆ†áŠ˜...</td>\n",
       "      <td>Not Avertisment</td>\n",
       "      <td>g2@trainee.com</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16049</th>\n",
       "      <td>16049</td>\n",
       "      <td>1490216693</td>\n",
       "      <td>26116</td>\n",
       "      <td>ADVERTISMENT áŠ áˆµá‰¸áŠ³á‹­ áˆ½á‹«áŒ­â€¦á‰ áˆá‹© á‰…áŠ“áˆ½ áŠ¥áŠá‹šáˆ…áŠ• áŒ¥áˆ«á‰µ á‹«áˆ‹á‰¸á‹ ...</td>\n",
       "      <td>retail</td>\n",
       "      <td>g2@trainee.com</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16050</th>\n",
       "      <td>16050</td>\n",
       "      <td>1490216693</td>\n",
       "      <td>26117</td>\n",
       "      <td>á‹¨áŠ ááˆªáŠ« áˆ…á‰¥áˆ¨á‰µ áŠ®áˆšáˆ½áŠ• áˆŠá‰€áˆ˜áŠ•á‰ áˆ­ áˆ™áˆ³ á‹áŠª áˆ›áˆƒáˆ›á‰µ á‰ áŠ¢á‰µá‹®áŒµá‹« áŠ¥áŠ“ á‰ áˆ¶...</td>\n",
       "      <td>Not Avertisment</td>\n",
       "      <td>g2@trainee.com</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Unnamed: 0  channel_id  message_id  \\\n",
       "16046       16046  1490216693       26113   \n",
       "16047       16047  1490216693       26114   \n",
       "16048       16048  1490216693       26115   \n",
       "16049       16049  1490216693       26116   \n",
       "16050       16050  1490216693       26117   \n",
       "\n",
       "                                                    text              label  \\\n",
       "16046  áŠ áŠ•á‹²á‰µ áŠ¢á‰µá‹®áŒµá‹«á‹Šá‰µ á‰ á‹±á‰£á‹­ á‹¨1 áˆšáˆŠá‹®áŠ• á‹¶áˆ‹áˆ­ á‰£áˆˆáŠ¥á‹µáˆ áˆ†áŠ“áˆˆá‰½ á‰°á‰¥áˆáˆá¢...    Not Avertisment   \n",
       "16047  ADVERTISMENT ğŸ‘ áˆ˜áˆáŠ«áˆ áŒˆáŠ“ ğŸ‘ ğŸ”œ ğŸ’¥áˆˆ3 á‰€áŠ“á‰µ á‰¥á‰» á‹¨áˆšá‰†á‹­ á‰³áˆ‹á‰…...  financial service   \n",
       "16048  á‹µáˆ¬á‹á‰½ğŸ™ á‹µáˆ¬ áŠ¥á‹áŠá‰µáˆ á‹¨áá‰…áˆ­ áŠ¨á‰°áˆ› áˆ³áˆáŠ•á‰±áŠ• áˆ™áˆ‰ á‰ á‹áˆµáŒ¥áˆ½ áŠ¥áŠ•áŒá‹³ áˆ†áŠ˜...    Not Avertisment   \n",
       "16049  ADVERTISMENT áŠ áˆµá‰¸áŠ³á‹­ áˆ½á‹«áŒ­â€¦á‰ áˆá‹© á‰…áŠ“áˆ½ áŠ¥áŠá‹šáˆ…áŠ• áŒ¥áˆ«á‰µ á‹«áˆ‹á‰¸á‹ ...             retail   \n",
       "16050  á‹¨áŠ ááˆªáŠ« áˆ…á‰¥áˆ¨á‰µ áŠ®áˆšáˆ½áŠ• áˆŠá‰€áˆ˜áŠ•á‰ áˆ­ áˆ™áˆ³ á‹áŠª áˆ›áˆƒáˆ›á‰µ á‰ áŠ¢á‰µá‹®áŒµá‹« áŠ¥áŠ“ á‰ áˆ¶...    Not Avertisment   \n",
       "\n",
       "                email  \n",
       "16046  g2@trainee.com  \n",
       "16047  g2@trainee.com  \n",
       "16048  g2@trainee.com  \n",
       "16049  g2@trainee.com  \n",
       "16050  g2@trainee.com  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['label'] = df['label'].fillna(\"Not Advertisement\")\n",
    "df.tail(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset\n",
    "\n",
    "# Create a dictionary containing your Amharic text data\n",
    "data_dict = {\"text\": df['text'].tolist()}\n",
    "\n",
    "# Create a Dataset object\n",
    "dataset = Dataset.from_dict(data_dict)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c897f7ee43a4438e871e39359b139da1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/5357 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataset.save_to_disk(\"../data/train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copyright (c) Meta Platforms, Inc. and affiliates.\n",
    "# This software may be used and distributed according to the terms of the GNU General Public License version 3.\n",
    "\n",
    "from peft import PeftModel\n",
    "from transformers import LlamaForCausalLM, LlamaConfig\n",
    "\n",
    "# Function to load the main model for text generation\n",
    "def load_model(model_name, quantization):\n",
    "    model = LlamaForCausalLM.from_pretrained(\n",
    "        model_name,\n",
    "        return_dict=True,\n",
    "        load_in_8bit=quantization,\n",
    "        device_map=\"auto\",\n",
    "        low_cpu_mem_usage=True,\n",
    "    )\n",
    "    return model\n",
    "\n",
    "\n",
    "# Function to load the PeftModel for performance optimization\n",
    "def load_peft_model(model, peft_model):\n",
    "    peft_model = PeftModel.from_pretrained(model, peft_model)\n",
    "    return peft_model\n",
    "\n",
    "# Loading the model from config to load FSDP checkpoints into that\n",
    "def load_llama_from_config(config_path):\n",
    "    model_config = LlamaConfig.from_pretrained(config_path) \n",
    "    model = LlamaForCausalLM(config=model_config)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['â–áŠ ', 'á–', 'áˆ', 'â–áŠ«áˆˆ', 'â–\"', 'â–áŠ¢áŠ•á‰°áˆ­áŠ”á‰µ', 'â–á‰°á‰‹áˆ­áŒ¦', 'â–áŒˆáŠ•á‹˜á‰¥', 'â–áˆ˜áˆ‹áŠ­', 'áˆ', 'â–áˆ˜á‰€á‰ áˆ', 'áˆ', 'â–áŠ áˆá‰»áˆ', 'áŠ©', '\"', 'â–áˆ›áˆˆá‰µ', 'â–á‹¨áˆˆáˆá¢', 'â–*', '6', '85', '#', 'â–á‰ áˆ˜á‹°', 'á‹ˆáˆ', 'â–áŠ áŒˆáˆáŒáˆá‰±áŠ•', 'â–á‹­áŒ á‰€áˆ™', 'á¢']\n"
     ]
    }
   ],
   "source": [
    "from transformers import LlamaTokenizer\n",
    "\n",
    "\n",
    "MAIN_PATH = '/model/Llama-2-7b-hf'\n",
    "tokenizer = LlamaTokenizer.from_pretrained(MAIN_PATH)\n",
    "\n",
    "example = 'áŠ á–áˆ áŠ«áˆˆ \" áŠ¢áŠ•á‰°áˆ­áŠ”á‰µ á‰°á‰‹áˆ­áŒ¦ áŒˆáŠ•á‹˜á‰¥ áˆ˜áˆ‹áŠ­áˆ áˆ˜á‰€á‰ áˆáˆ áŠ áˆá‰»áˆáŠ©\" áˆ›áˆˆá‰µ á‹¨áˆˆáˆá¢ *685# á‰ áˆ˜á‹°á‹ˆáˆ áŠ áŒˆáˆáŒáˆá‰±áŠ• á‹­áŒ á‰€áˆ™á¢'\n",
    "\n",
    "\n",
    "# garri_tokenizer = AutoTokenizer.from_pretrained(\"__the tokenizer__ you guys used\")\n",
    "tokens = tokenizer.tokenize(example)\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "51008\n"
     ]
    }
   ],
   "source": [
    "print(len(tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['â–áŠ ', 'á–', 'áˆ', 'â–áŠ«áˆˆ', 'â–\"', 'â–áŠ¢áŠ•á‰°áˆ­áŠ”á‰µ', 'â–á‰°á‰‹áˆ­áŒ¦', 'â–áŒˆáŠ•á‹˜á‰¥', 'â–áˆ˜áˆ‹áŠ­', 'áˆ', 'â–áˆ˜á‰€á‰ áˆ', 'áˆ', 'â–áŠ áˆá‰»áˆ', 'áŠ©', '\"', 'â–áˆ›áˆˆá‰µ', 'â–á‹¨áˆˆáˆá¢', 'â–*', '6', '85', '#', 'â–á‰ áˆ˜á‹°', 'á‹ˆáˆ', 'â–áŠ áŒˆáˆáŒáˆá‰±áŠ•', 'â–á‹­áŒ á‰€áˆ™', 'á¢']\n"
     ]
    }
   ],
   "source": [
    "example = 'áŠ á–áˆ áŠ«áˆˆ \" áŠ¢áŠ•á‰°áˆ­áŠ”á‰µ á‰°á‰‹áˆ­áŒ¦ áŒˆáŠ•á‹˜á‰¥ áˆ˜áˆ‹áŠ­áˆ áˆ˜á‰€á‰ áˆáˆ áŠ áˆá‰»áˆáŠ©\" áˆ›áˆˆá‰µ á‹¨áˆˆáˆá¢ *685# á‰ áˆ˜á‹°á‹ˆáˆ áŠ áŒˆáˆáŒáˆá‰±áŠ• á‹­áŒ á‰€áˆ™á¢'\n",
    "\n",
    "\n",
    "tokens = tokenizer.tokenize(example)\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'áˆ˜áˆ°áˆ¨á‰µ á‰°á‹°áˆ­áŒ áŒ¥á‰ƒá‰µ á‰°áˆá…áˆá‰¥áŠ›áˆ á‰ áˆšáˆ á‰ áˆ€áˆ°á‰µ á‹¨áˆ½áŠ•á‰µ áˆ˜áˆ½áŠ› á‰ á‹áˆ»áŠ“ á•áˆ‹áˆµá‰°áˆ­ á‰ áˆ˜áŒ á‰…áˆˆáˆ áŒáŒ­á‰µ áˆˆáˆ˜á‰€áˆµá‰€áˆµ á‹¨áˆáŠ¨áˆ¨á‹ áŒáˆˆáˆµá‰¥ á–áˆŠáˆµ á‰ á‰áŒ¥áŒ¥áˆ­ áˆµáˆ­ á‹‹áˆˆá¢ á‰°áŒ áˆ­áŒ£áˆª áˆáŠ•áˆ áŠ á‹­áŠá‰µ áŒ‰á‹³á‰µ áˆ³á‹­á‹°áˆ­áˆµá‰ á‰µ áˆ†áŠ• á‰¥áˆ á‰¥áˆ„áˆ¬áŠ• áˆ˜áˆ°áˆ¨á‰µ áŠ á‹µáˆ­áŒˆá‹ á‰ á‰¢áˆ‹á‹‹ á‹¨áˆ½áŠ•á‰µ áˆ˜áˆ½áŠ› á‰¥áˆá‰´áŠ• á‰†áˆ­áŒ á‹ áŒ‰á‹³á‰µ áŠ á‹µáˆ­áˆ°á‹á‰¥áŠ›áˆ á‰ áˆšáˆ á‹µáˆ­áŒŠá‰±áŠ• á‹«áˆáˆá€áˆ™ áŒáˆˆáˆ°á‰¦á‰½áŠ• áˆµáˆ á‰ áˆ˜áŒ¥á‰€áˆµ áˆˆá–áˆŠáˆµ áŠ á‰¤á‰±á‰³ á‹«á‰€áˆ¨á‰ á‹á¢ á–áˆŠáˆµ á‹¨á‰€áˆ¨á‰ á‹áŠ• áŠ á‰¤á‰±á‰³ á‰°á‰€á‰¥áˆ áˆáŠ”á‰³á‹áŠ• áˆˆáˆ›áŒ£áˆ«á‰µ á‰°áŠ¨áˆ³áˆ½áŠ• á‹ˆá‹° áˆ…áŠ­áˆáŠ“ á‹¨áˆ‹áŠ¨ áˆ²áˆ†áŠ• á‰ á‰°á‹°áˆ¨áŒˆ á‹¨áˆ…áŠ­áˆáŠ“ áˆáˆ­áˆ˜áˆ« áŒáˆˆáˆ°á‰¡ áˆ‹á‹­ áˆáŠ•áˆ áŠ á‹­áŠá‰µ áŒ¥á‰ƒá‰µ áˆ›áˆ¨áŒ‹áŒˆáŒ¥ áˆ˜á‰»áˆ‰áŠ• á‰°áŠ“áŒáˆ¯áˆá¢ áŠ áˆáŠ• áˆ‹á‹­ áˆµáˆ­ á‹áˆ áˆáˆ­áˆ˜áˆ«á‹ áŠ¥á‹¨á‰°áŒ£áˆ« áˆ˜áˆ†áŠ‘áŠ• á–áˆŠáˆµ áŒ¨áˆáˆ® áŒˆáˆá† áˆ›áŠ•áŠ›á‹áˆ áˆ°á‹ á‰ á‹œáŒá‰½ áˆ˜áŠ«áŠ¨áˆ áŒáŒ­á‰¶á‰½áŠ“ áŠ áˆˆáˆ˜áŒá‰£á‰£á‰¶á‰½ áŠ¥áŠ•á‹²áˆáŒ áˆ© áŠ¨áˆšá‹«á‹°áˆ­áŒ‰ á‰°áŒá‰£áˆ«á‰µ áˆŠá‰†áŒ á‰¥áŠ“ áˆŠáˆ­á‰… áŠ¥áŠ•á‹°áˆšáŒˆá‰£ áŠ áˆ³áˆµá‰§áˆ:: áˆáŠ•áŒ­á¦ á‹µáˆ¬ á–áˆŠáˆµ'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['text'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "unsupported operand type(s) for +=: 'int' and 'list'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 20\u001b[0m\n\u001b[1;32m     18\u001b[0m     total_word_count \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m word_count\n\u001b[1;32m     19\u001b[0m     tokens \u001b[38;5;241m=\u001b[39m tokenizer\u001b[38;5;241m.\u001b[39mtokenize(text)\n\u001b[0;32m---> 20\u001b[0m     total_tokens\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39mtokens\n\u001b[1;32m     21\u001b[0m     \u001b[38;5;28mprint\u001b[39m(tokens)\n\u001b[1;32m     25\u001b[0m \u001b[38;5;66;03m# Display the total word count\u001b[39;00m\n",
      "\u001b[0;31mTypeError\u001b[0m: unsupported operand type(s) for +=: 'int' and 'list'"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(\"/data/wasu_mohammed_labeled.csv\")\n",
    "\n",
    "# Initialize a variable to accumulate the total word count\n",
    "total_word_count = 0\n",
    "total_tokens = 0\n",
    "# Iterate through each row of the DataFrame\n",
    "for index, row in df.iterrows():\n",
    "    # Get the text from the specified column\n",
    "    text = row['text']\n",
    "    if not isinstance(text, str): \n",
    "        continue\n",
    "\n",
    "    \n",
    "    # Count the number of words in the text\n",
    "    word_count = len(text.split())\n",
    "\n",
    "    # Accumulate the word count\n",
    "    total_word_count += word_count\n",
    "    tokens = tokenizer.tokenize(text)\n",
    "    total_tokens+=tokens\n",
    "    print(tokens)\n",
    "    \n",
    "\n",
    "\n",
    "# Display the total word count\n",
    "print(\"Total Word Count:\", total_word_count)\n",
    "print(\"Total tokens count: \",total_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(16051, 6)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    BitsAndBytesConfig,\n",
    "    HfArgumentParser,\n",
    "    TrainingArguments,\n",
    "    pipeline,\n",
    "    LlamaForCausalLM, \n",
    "    LlamaTokenizer,\n",
    "    logging,\n",
    ")\n",
    "from peft import (\n",
    "    LoraConfig,\n",
    "    TaskType,\n",
    "    prepare_model_for_int8_training,\n",
    "    PeftModel\n",
    ")\n",
    "import torch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "783c2391b3494a82a3782f2d96ed894b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "resize the embedding size by the size of the tokenizer\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The model 'PeftModelForCausalLM' is not supported for text-generation. Supported models are ['BartForCausalLM', 'BertLMHeadModel', 'BertGenerationDecoder', 'BigBirdForCausalLM', 'BigBirdPegasusForCausalLM', 'BioGptForCausalLM', 'BlenderbotForCausalLM', 'BlenderbotSmallForCausalLM', 'BloomForCausalLM', 'CamembertForCausalLM', 'LlamaForCausalLM', 'CodeGenForCausalLM', 'CpmAntForCausalLM', 'CTRLLMHeadModel', 'Data2VecTextForCausalLM', 'ElectraForCausalLM', 'ErnieForCausalLM', 'FalconForCausalLM', 'FuyuForCausalLM', 'GitForCausalLM', 'GPT2LMHeadModel', 'GPT2LMHeadModel', 'GPTBigCodeForCausalLM', 'GPTNeoForCausalLM', 'GPTNeoXForCausalLM', 'GPTNeoXJapaneseForCausalLM', 'GPTJForCausalLM', 'LlamaForCausalLM', 'MarianForCausalLM', 'MBartForCausalLM', 'MegaForCausalLM', 'MegatronBertForCausalLM', 'MistralForCausalLM', 'MixtralForCausalLM', 'MptForCausalLM', 'MusicgenForCausalLM', 'MvpForCausalLM', 'OpenLlamaForCausalLM', 'OpenAIGPTLMHeadModel', 'OPTForCausalLM', 'PegasusForCausalLM', 'PersimmonForCausalLM', 'PhiForCausalLM', 'PLBartForCausalLM', 'ProphetNetForCausalLM', 'QDQBertLMHeadModel', 'Qwen2ForCausalLM', 'ReformerModelWithLMHead', 'RemBertForCausalLM', 'RobertaForCausalLM', 'RobertaPreLayerNormForCausalLM', 'RoCBertForCausalLM', 'RoFormerForCausalLM', 'RwkvForCausalLM', 'Speech2Text2ForCausalLM', 'TransfoXLLMHeadModel', 'TrOCRForCausalLM', 'WhisperForCausalLM', 'XGLMForCausalLM', 'XLMWithLMHeadModel', 'XLMProphetNetForCausalLM', 'XLMRobertaForCausalLM', 'XLMRobertaXLForCausalLM', 'XLNetLMHeadModel', 'XmodForCausalLM'].\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "LlamaForCausalLM.forward() got an unexpected keyword argument 'return_tensors'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 18\u001b[0m\n\u001b[1;32m     16\u001b[0m prompt \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWho is Leonardo Da Vinci?\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     17\u001b[0m pipe \u001b[38;5;241m=\u001b[39m pipeline(task\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtext-generation\u001b[39m\u001b[38;5;124m\"\u001b[39m, model\u001b[38;5;241m=\u001b[39mmodel, tokenizer\u001b[38;5;241m=\u001b[39mmodel, max_length\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m200\u001b[39m)\n\u001b[0;32m---> 18\u001b[0m result \u001b[38;5;241m=\u001b[39m pipe(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m<s>[INST] \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mprompt\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m [/INST]\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28mprint\u001b[39m(result[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgenerated_text\u001b[39m\u001b[38;5;124m'\u001b[39m])\n",
      "File \u001b[0;32m/opt/miniconda/lib/python3.11/site-packages/transformers/pipelines/text_generation.py:219\u001b[0m, in \u001b[0;36mTextGenerationPipeline.__call__\u001b[0;34m(self, text_inputs, **kwargs)\u001b[0m\n\u001b[1;32m    178\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, text_inputs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    179\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    180\u001b[0m \u001b[38;5;124;03m    Complete the prompt(s) given as inputs.\u001b[39;00m\n\u001b[1;32m    181\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    217\u001b[0m \u001b[38;5;124;03m          ids of the generated text.\u001b[39;00m\n\u001b[1;32m    218\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 219\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m(text_inputs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m/opt/miniconda/lib/python3.11/site-packages/transformers/pipelines/base.py:1162\u001b[0m, in \u001b[0;36mPipeline.__call__\u001b[0;34m(self, inputs, num_workers, batch_size, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1154\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mnext\u001b[39m(\n\u001b[1;32m   1155\u001b[0m         \u001b[38;5;28miter\u001b[39m(\n\u001b[1;32m   1156\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_iterator(\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1159\u001b[0m         )\n\u001b[1;32m   1160\u001b[0m     )\n\u001b[1;32m   1161\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1162\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrun_single(inputs, preprocess_params, forward_params, postprocess_params)\n",
      "File \u001b[0;32m/opt/miniconda/lib/python3.11/site-packages/transformers/pipelines/base.py:1168\u001b[0m, in \u001b[0;36mPipeline.run_single\u001b[0;34m(self, inputs, preprocess_params, forward_params, postprocess_params)\u001b[0m\n\u001b[1;32m   1167\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrun_single\u001b[39m(\u001b[38;5;28mself\u001b[39m, inputs, preprocess_params, forward_params, postprocess_params):\n\u001b[0;32m-> 1168\u001b[0m     model_inputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpreprocess(inputs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mpreprocess_params)\n\u001b[1;32m   1169\u001b[0m     model_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mforward(model_inputs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mforward_params)\n\u001b[1;32m   1170\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpostprocess(model_outputs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mpostprocess_params)\n",
      "File \u001b[0;32m/opt/miniconda/lib/python3.11/site-packages/transformers/pipelines/text_generation.py:232\u001b[0m, in \u001b[0;36mTextGenerationPipeline.preprocess\u001b[0;34m(self, prompt_text, prefix, handle_long_generation, add_special_tokens, truncation, padding, max_length, **generate_kwargs)\u001b[0m\n\u001b[1;32m    221\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpreprocess\u001b[39m(\n\u001b[1;32m    222\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    223\u001b[0m     prompt_text,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    230\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mgenerate_kwargs,\n\u001b[1;32m    231\u001b[0m ):\n\u001b[0;32m--> 232\u001b[0m     inputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtokenizer(\n\u001b[1;32m    233\u001b[0m         prefix \u001b[38;5;241m+\u001b[39m prompt_text,\n\u001b[1;32m    234\u001b[0m         return_tensors\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mframework,\n\u001b[1;32m    235\u001b[0m         truncation\u001b[38;5;241m=\u001b[39mtruncation,\n\u001b[1;32m    236\u001b[0m         padding\u001b[38;5;241m=\u001b[39mpadding,\n\u001b[1;32m    237\u001b[0m         max_length\u001b[38;5;241m=\u001b[39mmax_length,\n\u001b[1;32m    238\u001b[0m         add_special_tokens\u001b[38;5;241m=\u001b[39madd_special_tokens,\n\u001b[1;32m    239\u001b[0m     )\n\u001b[1;32m    240\u001b[0m     inputs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprompt_text\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m prompt_text\n\u001b[1;32m    242\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m handle_long_generation \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhole\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "File \u001b[0;32m/opt/miniconda/lib/python3.11/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m/opt/miniconda/lib/python3.11/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/opt/miniconda/lib/python3.11/site-packages/peft/peft_model.py:1083\u001b[0m, in \u001b[0;36mPeftModelForCausalLM.forward\u001b[0;34m(self, input_ids, attention_mask, inputs_embeds, labels, output_attentions, output_hidden_states, return_dict, task_ids, **kwargs)\u001b[0m\n\u001b[1;32m   1081\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m peft_config\u001b[38;5;241m.\u001b[39mpeft_type \u001b[38;5;241m==\u001b[39m PeftType\u001b[38;5;241m.\u001b[39mPOLY:\n\u001b[1;32m   1082\u001b[0m         kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtask_ids\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m task_ids\n\u001b[0;32m-> 1083\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbase_model(\n\u001b[1;32m   1084\u001b[0m         input_ids\u001b[38;5;241m=\u001b[39minput_ids,\n\u001b[1;32m   1085\u001b[0m         attention_mask\u001b[38;5;241m=\u001b[39mattention_mask,\n\u001b[1;32m   1086\u001b[0m         inputs_embeds\u001b[38;5;241m=\u001b[39minputs_embeds,\n\u001b[1;32m   1087\u001b[0m         labels\u001b[38;5;241m=\u001b[39mlabels,\n\u001b[1;32m   1088\u001b[0m         output_attentions\u001b[38;5;241m=\u001b[39moutput_attentions,\n\u001b[1;32m   1089\u001b[0m         output_hidden_states\u001b[38;5;241m=\u001b[39moutput_hidden_states,\n\u001b[1;32m   1090\u001b[0m         return_dict\u001b[38;5;241m=\u001b[39mreturn_dict,\n\u001b[1;32m   1091\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m   1092\u001b[0m     )\n\u001b[1;32m   1094\u001b[0m batch_size \u001b[38;5;241m=\u001b[39m _get_batch_size(input_ids, inputs_embeds)\n\u001b[1;32m   1095\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m attention_mask \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1096\u001b[0m     \u001b[38;5;66;03m# concat prompt attention mask\u001b[39;00m\n",
      "File \u001b[0;32m/opt/miniconda/lib/python3.11/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m/opt/miniconda/lib/python3.11/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/opt/miniconda/lib/python3.11/site-packages/peft/tuners/tuners_utils.py:161\u001b[0m, in \u001b[0;36mBaseTuner.forward\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    160\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any):\n\u001b[0;32m--> 161\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mforward(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m/opt/miniconda/lib/python3.11/site-packages/accelerate/hooks.py:166\u001b[0m, in \u001b[0;36madd_hook_to_module.<locals>.new_forward\u001b[0;34m(module, *args, **kwargs)\u001b[0m\n\u001b[1;32m    164\u001b[0m         output \u001b[38;5;241m=\u001b[39m module\u001b[38;5;241m.\u001b[39m_old_forward(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    165\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 166\u001b[0m     output \u001b[38;5;241m=\u001b[39m module\u001b[38;5;241m.\u001b[39m_old_forward(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    167\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m module\u001b[38;5;241m.\u001b[39m_hf_hook\u001b[38;5;241m.\u001b[39mpost_forward(module, output)\n",
      "\u001b[0;31mTypeError\u001b[0m: LlamaForCausalLM.forward() got an unexpected keyword argument 'return_tensors'"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "LLAMA_DIR = '/model/Llama-2-7b-hf'\n",
    "tokenizer = LlamaTokenizer.from_pretrained(LLAMA_DIR)\n",
    "\n",
    "model = LlamaForCausalLM.from_pretrained(LLAMA_DIR, load_in_8bit=True, device_map='auto', torch_dtype=torch.float16)\n",
    "embedding_size = model.get_input_embeddings().weight.shape[0]\n",
    "\n",
    "if len(tokenizer) != embedding_size:\n",
    "    print(\"resize the embedding size by the size of the tokenizer\")\n",
    "    model.resize_token_embeddings(len(tokenizer))\n",
    "\n",
    "\n",
    "new_model ='/home/abdulhamid_mussa/LLM_Finetuning_For_Amharic_Ad_Generation/output'\n",
    "model = PeftModel.from_pretrained(model, new_model)\n",
    "\n",
    "# Run text generation pipeline with our next model\n",
    "prompt = \"Who is Leonardo Da Vinci?\"\n",
    "pipe = pipeline(task=\"text-generation\", model=model, tokenizer=model, max_length=200)\n",
    "result = pipe(f\"<s>[INST] {prompt} [/INST]\")\n",
    "print(result[0]['generated_text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "á‰¨áˆ­á‰¹á‹‹áˆ áˆ¨á‹³á‰¶á‰½ áŠ¥áŠ•á‹° Amazon&#39;s Alexaá£ Apple&#39;s Siri áŠ¥áŠ“ Google Assistant á‹«áˆ‰ á‹¨á‰°áˆˆá‹«á‹© á‰£áˆ…áˆªá‹«á‰µ áŠ¥áŠ“ á‰½áˆá‰³á‹á‰½ áŠ áˆá‰¸á‹á¢ áŠáŒˆáˆ­ áŒáŠ• á‰ áŠ áŒ á‰ƒáˆ‹á‹­á£ áˆáŠ“á‰£á‹Š áˆ¨á‹³á‰¶á‰½ áŠ¥áŠ•á‹° á‹¨áŒáˆ áˆ¨á‹³á‰¶á‰½ áˆ†áŠá‹ á‹«áŒˆáˆˆáŒáˆ‹áˆ‰á£ áˆˆá‰°áŒ á‰ƒáˆšá‹á‰½ áŠ¥áŠ•á‹° áŠ áˆµá‰³á‹‹áˆ¾á‰½á£ áˆ™á‹šá‰ƒ áˆ˜áŒ«á‹ˆá‰µá£ á‹¨áŠ á‹¨áˆ­ áˆáŠ”á‰³ á‹áˆ˜áŠ“á‹á‰½ áŠ¥áŠ“ áŒ¥á‹«á‰„á‹á‰½áŠ• áˆ˜áˆ˜áˆˆáˆµ á‹«áˆ‰ á‰°áŒá‰£áˆ«á‰µáŠ• á‹«áŠ¨áŠ“á‹áŠ“áˆ‰á¢ áˆáŠ“á‰£á‹Š áˆ¨á‹³á‰¶á‰½ áŠ¥áŠ•á‹° á‹¨áŒáˆ áˆ¨á‹³á‰¶á‰½ áˆ†áŠá‹ á‹«áŒˆáˆˆáŒáˆ‹áˆ‰á£ áˆˆá‰°áŒ á‰ƒáˆšá‹á‰½ áŠ¥áŠ•á‹° áŠ áˆµá‰³á‹‹áˆ¾á‰½á£ áˆ™á‹šá‰ƒ áˆ˜áŒ«á‹ˆá‰µá£ á‹¨áŠ á‹¨áˆ­ áˆáŠ”á‰³ á‹áˆ˜áŠ“á‹á‰½ áŠ¥áŠ“ áŒ¥á‹«á‰„á‹á‰½áŠ• áˆ˜áˆ˜áˆˆáˆµ á‹«áˆ‰ á‰°áŒá‰£áˆ«á‰µáŠ• á‹«áŠ¨áŠ“á‹áŠ“áˆ‰á¢ áˆáŠ“á‰£á‹Š áˆ¨á‹³á‰¶á‰½ áŠ¥áŠ•á‹° á‹¨áŒáˆ áˆ¨á‹³á‰¶á‰½ áˆ†áŠá‹ á‹«áŒˆáˆˆáŒáˆ‹áˆ‰á£ áˆˆá‰°áŒ á‰ƒáˆšá‹á‰½ áŠ¥áŠ•á‹° áŠ áˆµá‰³á‹‹áˆ¾á‰½á£ áˆ™á‹šá‰ƒ áˆ˜áŒ«á‹ˆá‰µá£ á‹¨áŠ á‹¨áˆ­ áˆáŠ”á‰³ á‹áˆ˜áŠ“á‹á‰½ áŠ¥áŠ“ áŒ¥á‹«á‰„á‹á‰½áŠ• áˆ˜áˆ˜áˆˆáˆµ á‹«áˆ‰ á‰°áŒá‰£áˆ«á‰µáŠ• á‹«áŠ¨áŠ“á‹áŠ“áˆ‰á¢"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
