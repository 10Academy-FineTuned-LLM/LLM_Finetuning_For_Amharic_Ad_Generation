{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "7918b9b1-82cd-495c-aca1-bf81a1f33b19",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4b9f66b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: datasets in /home/eyaya_eneyew/week_7/LLM_Finetuning_For_Amharic_Ad_Generation/venv/lib/python3.11/site-packages (2.16.1)\n",
      "Requirement already satisfied: transformers[sentencepiece] in /home/eyaya_eneyew/week_7/LLM_Finetuning_For_Amharic_Ad_Generation/venv/lib/python3.11/site-packages (4.31.0)\n",
      "Requirement already satisfied: filelock in /home/eyaya_eneyew/week_7/LLM_Finetuning_For_Amharic_Ad_Generation/venv/lib/python3.11/site-packages (from datasets) (3.13.1)\n",
      "Requirement already satisfied: numpy>=1.17 in /home/eyaya_eneyew/week_7/LLM_Finetuning_For_Amharic_Ad_Generation/venv/lib/python3.11/site-packages (from datasets) (1.26.3)\n",
      "Requirement already satisfied: pyarrow>=8.0.0 in /home/eyaya_eneyew/week_7/LLM_Finetuning_For_Amharic_Ad_Generation/venv/lib/python3.11/site-packages (from datasets) (15.0.0)\n",
      "Requirement already satisfied: pyarrow-hotfix in /home/eyaya_eneyew/week_7/LLM_Finetuning_For_Amharic_Ad_Generation/venv/lib/python3.11/site-packages (from datasets) (0.6)\n",
      "Requirement already satisfied: dill<0.3.8,>=0.3.0 in /home/eyaya_eneyew/week_7/LLM_Finetuning_For_Amharic_Ad_Generation/venv/lib/python3.11/site-packages (from datasets) (0.3.7)\n",
      "Requirement already satisfied: pandas in /home/eyaya_eneyew/week_7/LLM_Finetuning_For_Amharic_Ad_Generation/venv/lib/python3.11/site-packages (from datasets) (2.2.0)\n",
      "Requirement already satisfied: requests>=2.19.0 in /home/eyaya_eneyew/week_7/LLM_Finetuning_For_Amharic_Ad_Generation/venv/lib/python3.11/site-packages (from datasets) (2.31.0)\n",
      "Requirement already satisfied: tqdm>=4.62.1 in /home/eyaya_eneyew/week_7/LLM_Finetuning_For_Amharic_Ad_Generation/venv/lib/python3.11/site-packages (from datasets) (4.66.1)\n",
      "Requirement already satisfied: xxhash in /home/eyaya_eneyew/week_7/LLM_Finetuning_For_Amharic_Ad_Generation/venv/lib/python3.11/site-packages (from datasets) (3.4.1)\n",
      "Requirement already satisfied: multiprocess in /home/eyaya_eneyew/week_7/LLM_Finetuning_For_Amharic_Ad_Generation/venv/lib/python3.11/site-packages (from datasets) (0.70.15)\n",
      "Requirement already satisfied: fsspec<=2023.10.0,>=2023.1.0 in /home/eyaya_eneyew/week_7/LLM_Finetuning_For_Amharic_Ad_Generation/venv/lib/python3.11/site-packages (from fsspec[http]<=2023.10.0,>=2023.1.0->datasets) (2023.10.0)\n",
      "Requirement already satisfied: aiohttp in /home/eyaya_eneyew/week_7/LLM_Finetuning_For_Amharic_Ad_Generation/venv/lib/python3.11/site-packages (from datasets) (3.9.2)\n",
      "Requirement already satisfied: huggingface-hub>=0.19.4 in /home/eyaya_eneyew/week_7/LLM_Finetuning_For_Amharic_Ad_Generation/venv/lib/python3.11/site-packages (from datasets) (0.20.3)\n",
      "Requirement already satisfied: packaging in /home/eyaya_eneyew/week_7/LLM_Finetuning_For_Amharic_Ad_Generation/venv/lib/python3.11/site-packages (from datasets) (23.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home/eyaya_eneyew/week_7/LLM_Finetuning_For_Amharic_Ad_Generation/venv/lib/python3.11/site-packages (from datasets) (6.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /home/eyaya_eneyew/week_7/LLM_Finetuning_For_Amharic_Ad_Generation/venv/lib/python3.11/site-packages (from transformers[sentencepiece]) (2023.12.25)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /home/eyaya_eneyew/week_7/LLM_Finetuning_For_Amharic_Ad_Generation/venv/lib/python3.11/site-packages (from transformers[sentencepiece]) (0.13.3)\n",
      "Requirement already satisfied: safetensors>=0.3.1 in /home/eyaya_eneyew/week_7/LLM_Finetuning_For_Amharic_Ad_Generation/venv/lib/python3.11/site-packages (from transformers[sentencepiece]) (0.4.2)\n",
      "Requirement already satisfied: sentencepiece!=0.1.92,>=0.1.91 in /home/eyaya_eneyew/week_7/LLM_Finetuning_For_Amharic_Ad_Generation/venv/lib/python3.11/site-packages (from transformers[sentencepiece]) (0.1.99)\n",
      "Requirement already satisfied: protobuf in /home/eyaya_eneyew/week_7/LLM_Finetuning_For_Amharic_Ad_Generation/venv/lib/python3.11/site-packages (from transformers[sentencepiece]) (4.25.2)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /home/eyaya_eneyew/week_7/LLM_Finetuning_For_Amharic_Ad_Generation/venv/lib/python3.11/site-packages (from aiohttp->datasets) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /home/eyaya_eneyew/week_7/LLM_Finetuning_For_Amharic_Ad_Generation/venv/lib/python3.11/site-packages (from aiohttp->datasets) (23.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /home/eyaya_eneyew/week_7/LLM_Finetuning_For_Amharic_Ad_Generation/venv/lib/python3.11/site-packages (from aiohttp->datasets) (1.4.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /home/eyaya_eneyew/week_7/LLM_Finetuning_For_Amharic_Ad_Generation/venv/lib/python3.11/site-packages (from aiohttp->datasets) (6.0.4)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /home/eyaya_eneyew/week_7/LLM_Finetuning_For_Amharic_Ad_Generation/venv/lib/python3.11/site-packages (from aiohttp->datasets) (1.9.4)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/eyaya_eneyew/week_7/LLM_Finetuning_For_Amharic_Ad_Generation/venv/lib/python3.11/site-packages (from huggingface-hub>=0.19.4->datasets) (4.9.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/eyaya_eneyew/week_7/LLM_Finetuning_For_Amharic_Ad_Generation/venv/lib/python3.11/site-packages (from requests>=2.19.0->datasets) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/eyaya_eneyew/week_7/LLM_Finetuning_For_Amharic_Ad_Generation/venv/lib/python3.11/site-packages (from requests>=2.19.0->datasets) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/eyaya_eneyew/week_7/LLM_Finetuning_For_Amharic_Ad_Generation/venv/lib/python3.11/site-packages (from requests>=2.19.0->datasets) (2.1.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/eyaya_eneyew/week_7/LLM_Finetuning_For_Amharic_Ad_Generation/venv/lib/python3.11/site-packages (from requests>=2.19.0->datasets) (2023.11.17)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /home/eyaya_eneyew/week_7/LLM_Finetuning_For_Amharic_Ad_Generation/venv/lib/python3.11/site-packages (from pandas->datasets) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /home/eyaya_eneyew/week_7/LLM_Finetuning_For_Amharic_Ad_Generation/venv/lib/python3.11/site-packages (from pandas->datasets) (2023.3.post1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /home/eyaya_eneyew/week_7/LLM_Finetuning_For_Amharic_Ad_Generation/venv/lib/python3.11/site-packages (from pandas->datasets) (2023.4)\n",
      "Requirement already satisfied: six>=1.5 in /home/eyaya_eneyew/week_7/LLM_Finetuning_For_Amharic_Ad_Generation/venv/lib/python3.11/site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n",
      "Collecting git+https://github.com/huggingface/peft\n",
      "  Cloning https://github.com/huggingface/peft to /tmp/pip-req-build-xle1d4dy\n",
      "  Running command git clone --filter=blob:none --quiet https://github.com/huggingface/peft /tmp/pip-req-build-xle1d4dy\n",
      "  Resolved https://github.com/huggingface/peft to commit ce925d844a0bc54b951fcb69229dfe740c9afa45\n",
      "  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /home/eyaya_eneyew/week_7/LLM_Finetuning_For_Amharic_Ad_Generation/venv/lib/python3.11/site-packages (from peft==0.8.2) (1.26.3)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/eyaya_eneyew/week_7/LLM_Finetuning_For_Amharic_Ad_Generation/venv/lib/python3.11/site-packages (from peft==0.8.2) (23.2)\n",
      "Requirement already satisfied: psutil in /home/eyaya_eneyew/week_7/LLM_Finetuning_For_Amharic_Ad_Generation/venv/lib/python3.11/site-packages (from peft==0.8.2) (5.9.0)\n",
      "Requirement already satisfied: pyyaml in /home/eyaya_eneyew/week_7/LLM_Finetuning_For_Amharic_Ad_Generation/venv/lib/python3.11/site-packages (from peft==0.8.2) (6.0.1)\n",
      "Requirement already satisfied: torch>=1.13.0 in /home/eyaya_eneyew/week_7/LLM_Finetuning_For_Amharic_Ad_Generation/venv/lib/python3.11/site-packages (from peft==0.8.2) (2.1.2)\n",
      "Requirement already satisfied: transformers in /home/eyaya_eneyew/week_7/LLM_Finetuning_For_Amharic_Ad_Generation/venv/lib/python3.11/site-packages (from peft==0.8.2) (4.31.0)\n",
      "Requirement already satisfied: tqdm in /home/eyaya_eneyew/week_7/LLM_Finetuning_For_Amharic_Ad_Generation/venv/lib/python3.11/site-packages (from peft==0.8.2) (4.66.1)\n",
      "Requirement already satisfied: accelerate>=0.21.0 in /home/eyaya_eneyew/week_7/LLM_Finetuning_For_Amharic_Ad_Generation/venv/lib/python3.11/site-packages (from peft==0.8.2) (0.21.0)\n",
      "Requirement already satisfied: safetensors in /home/eyaya_eneyew/week_7/LLM_Finetuning_For_Amharic_Ad_Generation/venv/lib/python3.11/site-packages (from peft==0.8.2) (0.4.2)\n",
      "Requirement already satisfied: huggingface-hub>=0.17.0 in /home/eyaya_eneyew/week_7/LLM_Finetuning_For_Amharic_Ad_Generation/venv/lib/python3.11/site-packages (from peft==0.8.2) (0.20.3)\n",
      "Requirement already satisfied: filelock in /home/eyaya_eneyew/week_7/LLM_Finetuning_For_Amharic_Ad_Generation/venv/lib/python3.11/site-packages (from huggingface-hub>=0.17.0->peft==0.8.2) (3.13.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /home/eyaya_eneyew/week_7/LLM_Finetuning_For_Amharic_Ad_Generation/venv/lib/python3.11/site-packages (from huggingface-hub>=0.17.0->peft==0.8.2) (2023.10.0)\n",
      "Requirement already satisfied: requests in /home/eyaya_eneyew/week_7/LLM_Finetuning_For_Amharic_Ad_Generation/venv/lib/python3.11/site-packages (from huggingface-hub>=0.17.0->peft==0.8.2) (2.31.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/eyaya_eneyew/week_7/LLM_Finetuning_For_Amharic_Ad_Generation/venv/lib/python3.11/site-packages (from huggingface-hub>=0.17.0->peft==0.8.2) (4.9.0)\n",
      "Requirement already satisfied: sympy in /home/eyaya_eneyew/week_7/LLM_Finetuning_For_Amharic_Ad_Generation/venv/lib/python3.11/site-packages (from torch>=1.13.0->peft==0.8.2) (1.12)\n",
      "Requirement already satisfied: networkx in /home/eyaya_eneyew/week_7/LLM_Finetuning_For_Amharic_Ad_Generation/venv/lib/python3.11/site-packages (from torch>=1.13.0->peft==0.8.2) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in /home/eyaya_eneyew/week_7/LLM_Finetuning_For_Amharic_Ad_Generation/venv/lib/python3.11/site-packages (from torch>=1.13.0->peft==0.8.2) (3.1.3)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /home/eyaya_eneyew/week_7/LLM_Finetuning_For_Amharic_Ad_Generation/venv/lib/python3.11/site-packages (from torch>=1.13.0->peft==0.8.2) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /home/eyaya_eneyew/week_7/LLM_Finetuning_For_Amharic_Ad_Generation/venv/lib/python3.11/site-packages (from torch>=1.13.0->peft==0.8.2) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /home/eyaya_eneyew/week_7/LLM_Finetuning_For_Amharic_Ad_Generation/venv/lib/python3.11/site-packages (from torch>=1.13.0->peft==0.8.2) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /home/eyaya_eneyew/week_7/LLM_Finetuning_For_Amharic_Ad_Generation/venv/lib/python3.11/site-packages (from torch>=1.13.0->peft==0.8.2) (8.9.2.26)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /home/eyaya_eneyew/week_7/LLM_Finetuning_For_Amharic_Ad_Generation/venv/lib/python3.11/site-packages (from torch>=1.13.0->peft==0.8.2) (12.1.3.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /home/eyaya_eneyew/week_7/LLM_Finetuning_For_Amharic_Ad_Generation/venv/lib/python3.11/site-packages (from torch>=1.13.0->peft==0.8.2) (11.0.2.54)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /home/eyaya_eneyew/week_7/LLM_Finetuning_For_Amharic_Ad_Generation/venv/lib/python3.11/site-packages (from torch>=1.13.0->peft==0.8.2) (10.3.2.106)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /home/eyaya_eneyew/week_7/LLM_Finetuning_For_Amharic_Ad_Generation/venv/lib/python3.11/site-packages (from torch>=1.13.0->peft==0.8.2) (11.4.5.107)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /home/eyaya_eneyew/week_7/LLM_Finetuning_For_Amharic_Ad_Generation/venv/lib/python3.11/site-packages (from torch>=1.13.0->peft==0.8.2) (12.1.0.106)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.18.1 in /home/eyaya_eneyew/week_7/LLM_Finetuning_For_Amharic_Ad_Generation/venv/lib/python3.11/site-packages (from torch>=1.13.0->peft==0.8.2) (2.18.1)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /home/eyaya_eneyew/week_7/LLM_Finetuning_For_Amharic_Ad_Generation/venv/lib/python3.11/site-packages (from torch>=1.13.0->peft==0.8.2) (12.1.105)\n",
      "Requirement already satisfied: triton==2.1.0 in /home/eyaya_eneyew/week_7/LLM_Finetuning_For_Amharic_Ad_Generation/venv/lib/python3.11/site-packages (from torch>=1.13.0->peft==0.8.2) (2.1.0)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12 in /home/eyaya_eneyew/week_7/LLM_Finetuning_For_Amharic_Ad_Generation/venv/lib/python3.11/site-packages (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.13.0->peft==0.8.2) (12.3.101)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /home/eyaya_eneyew/week_7/LLM_Finetuning_For_Amharic_Ad_Generation/venv/lib/python3.11/site-packages (from transformers->peft==0.8.2) (2023.12.25)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /home/eyaya_eneyew/week_7/LLM_Finetuning_For_Amharic_Ad_Generation/venv/lib/python3.11/site-packages (from transformers->peft==0.8.2) (0.13.3)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/eyaya_eneyew/week_7/LLM_Finetuning_For_Amharic_Ad_Generation/venv/lib/python3.11/site-packages (from jinja2->torch>=1.13.0->peft==0.8.2) (2.1.4)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/eyaya_eneyew/week_7/LLM_Finetuning_For_Amharic_Ad_Generation/venv/lib/python3.11/site-packages (from requests->huggingface-hub>=0.17.0->peft==0.8.2) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/eyaya_eneyew/week_7/LLM_Finetuning_For_Amharic_Ad_Generation/venv/lib/python3.11/site-packages (from requests->huggingface-hub>=0.17.0->peft==0.8.2) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/eyaya_eneyew/week_7/LLM_Finetuning_For_Amharic_Ad_Generation/venv/lib/python3.11/site-packages (from requests->huggingface-hub>=0.17.0->peft==0.8.2) (2.1.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/eyaya_eneyew/week_7/LLM_Finetuning_For_Amharic_Ad_Generation/venv/lib/python3.11/site-packages (from requests->huggingface-hub>=0.17.0->peft==0.8.2) (2023.11.17)\n",
      "Requirement already satisfied: mpmath>=0.19 in /home/eyaya_eneyew/week_7/LLM_Finetuning_For_Amharic_Ad_Generation/venv/lib/python3.11/site-packages (from sympy->torch>=1.13.0->peft==0.8.2) (1.3.0)\n",
      "Building wheels for collected packages: peft\n",
      "  Building wheel for peft (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for peft: filename=peft-0.8.2-py3-none-any.whl size=183371 sha256=cbaa54731222ebb1784b5853bff5881090f95e7e6759eaa2f4a60d84a7a66f7f\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-7pe76gmi/wheels/e1/0a/cc/243fa4389de86b6a8c8a6ac6511d2227d3e10934d3b19e5f5e\n",
      "Successfully built peft\n",
      "Installing collected packages: peft\n",
      "  Attempting uninstall: peft\n",
      "    Found existing installation: peft 0.4.0\n",
      "    Uninstalling peft-0.4.0:\n",
      "      Successfully uninstalled peft-0.4.0\n",
      "Successfully installed peft-0.8.2\n",
      "Collecting git+https://github.com/huggingface/accelerate\n",
      "  Cloning https://github.com/huggingface/accelerate to /tmp/pip-req-build-q408r1e0\n",
      "  Running command git clone --filter=blob:none --quiet https://github.com/huggingface/accelerate /tmp/pip-req-build-q408r1e0\n",
      "  Resolved https://github.com/huggingface/accelerate to commit 68f54720dc9e4e887f8b7e3177798d745913e8ad\n",
      "  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /home/eyaya_eneyew/week_7/LLM_Finetuning_For_Amharic_Ad_Generation/venv/lib/python3.11/site-packages (from accelerate==0.27.0.dev0) (1.26.3)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/eyaya_eneyew/week_7/LLM_Finetuning_For_Amharic_Ad_Generation/venv/lib/python3.11/site-packages (from accelerate==0.27.0.dev0) (23.2)\n",
      "Requirement already satisfied: psutil in /home/eyaya_eneyew/week_7/LLM_Finetuning_For_Amharic_Ad_Generation/venv/lib/python3.11/site-packages (from accelerate==0.27.0.dev0) (5.9.0)\n",
      "Requirement already satisfied: pyyaml in /home/eyaya_eneyew/week_7/LLM_Finetuning_For_Amharic_Ad_Generation/venv/lib/python3.11/site-packages (from accelerate==0.27.0.dev0) (6.0.1)\n",
      "Requirement already satisfied: torch<2.2.0,>=1.10.0 in /home/eyaya_eneyew/week_7/LLM_Finetuning_For_Amharic_Ad_Generation/venv/lib/python3.11/site-packages (from accelerate==0.27.0.dev0) (2.1.2)\n",
      "Requirement already satisfied: huggingface-hub in /home/eyaya_eneyew/week_7/LLM_Finetuning_For_Amharic_Ad_Generation/venv/lib/python3.11/site-packages (from accelerate==0.27.0.dev0) (0.20.3)\n",
      "Requirement already satisfied: safetensors>=0.3.1 in /home/eyaya_eneyew/week_7/LLM_Finetuning_For_Amharic_Ad_Generation/venv/lib/python3.11/site-packages (from accelerate==0.27.0.dev0) (0.4.2)\n",
      "Requirement already satisfied: filelock in /home/eyaya_eneyew/week_7/LLM_Finetuning_For_Amharic_Ad_Generation/venv/lib/python3.11/site-packages (from torch<2.2.0,>=1.10.0->accelerate==0.27.0.dev0) (3.13.1)\n",
      "Requirement already satisfied: typing-extensions in /home/eyaya_eneyew/week_7/LLM_Finetuning_For_Amharic_Ad_Generation/venv/lib/python3.11/site-packages (from torch<2.2.0,>=1.10.0->accelerate==0.27.0.dev0) (4.9.0)\n",
      "Requirement already satisfied: sympy in /home/eyaya_eneyew/week_7/LLM_Finetuning_For_Amharic_Ad_Generation/venv/lib/python3.11/site-packages (from torch<2.2.0,>=1.10.0->accelerate==0.27.0.dev0) (1.12)\n",
      "Requirement already satisfied: networkx in /home/eyaya_eneyew/week_7/LLM_Finetuning_For_Amharic_Ad_Generation/venv/lib/python3.11/site-packages (from torch<2.2.0,>=1.10.0->accelerate==0.27.0.dev0) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in /home/eyaya_eneyew/week_7/LLM_Finetuning_For_Amharic_Ad_Generation/venv/lib/python3.11/site-packages (from torch<2.2.0,>=1.10.0->accelerate==0.27.0.dev0) (3.1.3)\n",
      "Requirement already satisfied: fsspec in /home/eyaya_eneyew/week_7/LLM_Finetuning_For_Amharic_Ad_Generation/venv/lib/python3.11/site-packages (from torch<2.2.0,>=1.10.0->accelerate==0.27.0.dev0) (2023.10.0)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /home/eyaya_eneyew/week_7/LLM_Finetuning_For_Amharic_Ad_Generation/venv/lib/python3.11/site-packages (from torch<2.2.0,>=1.10.0->accelerate==0.27.0.dev0) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /home/eyaya_eneyew/week_7/LLM_Finetuning_For_Amharic_Ad_Generation/venv/lib/python3.11/site-packages (from torch<2.2.0,>=1.10.0->accelerate==0.27.0.dev0) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /home/eyaya_eneyew/week_7/LLM_Finetuning_For_Amharic_Ad_Generation/venv/lib/python3.11/site-packages (from torch<2.2.0,>=1.10.0->accelerate==0.27.0.dev0) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /home/eyaya_eneyew/week_7/LLM_Finetuning_For_Amharic_Ad_Generation/venv/lib/python3.11/site-packages (from torch<2.2.0,>=1.10.0->accelerate==0.27.0.dev0) (8.9.2.26)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /home/eyaya_eneyew/week_7/LLM_Finetuning_For_Amharic_Ad_Generation/venv/lib/python3.11/site-packages (from torch<2.2.0,>=1.10.0->accelerate==0.27.0.dev0) (12.1.3.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /home/eyaya_eneyew/week_7/LLM_Finetuning_For_Amharic_Ad_Generation/venv/lib/python3.11/site-packages (from torch<2.2.0,>=1.10.0->accelerate==0.27.0.dev0) (11.0.2.54)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /home/eyaya_eneyew/week_7/LLM_Finetuning_For_Amharic_Ad_Generation/venv/lib/python3.11/site-packages (from torch<2.2.0,>=1.10.0->accelerate==0.27.0.dev0) (10.3.2.106)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /home/eyaya_eneyew/week_7/LLM_Finetuning_For_Amharic_Ad_Generation/venv/lib/python3.11/site-packages (from torch<2.2.0,>=1.10.0->accelerate==0.27.0.dev0) (11.4.5.107)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /home/eyaya_eneyew/week_7/LLM_Finetuning_For_Amharic_Ad_Generation/venv/lib/python3.11/site-packages (from torch<2.2.0,>=1.10.0->accelerate==0.27.0.dev0) (12.1.0.106)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.18.1 in /home/eyaya_eneyew/week_7/LLM_Finetuning_For_Amharic_Ad_Generation/venv/lib/python3.11/site-packages (from torch<2.2.0,>=1.10.0->accelerate==0.27.0.dev0) (2.18.1)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /home/eyaya_eneyew/week_7/LLM_Finetuning_For_Amharic_Ad_Generation/venv/lib/python3.11/site-packages (from torch<2.2.0,>=1.10.0->accelerate==0.27.0.dev0) (12.1.105)\n",
      "Requirement already satisfied: triton==2.1.0 in /home/eyaya_eneyew/week_7/LLM_Finetuning_For_Amharic_Ad_Generation/venv/lib/python3.11/site-packages (from torch<2.2.0,>=1.10.0->accelerate==0.27.0.dev0) (2.1.0)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12 in /home/eyaya_eneyew/week_7/LLM_Finetuning_For_Amharic_Ad_Generation/venv/lib/python3.11/site-packages (from nvidia-cusolver-cu12==11.4.5.107->torch<2.2.0,>=1.10.0->accelerate==0.27.0.dev0) (12.3.101)\n",
      "Requirement already satisfied: requests in /home/eyaya_eneyew/week_7/LLM_Finetuning_For_Amharic_Ad_Generation/venv/lib/python3.11/site-packages (from huggingface-hub->accelerate==0.27.0.dev0) (2.31.0)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in /home/eyaya_eneyew/week_7/LLM_Finetuning_For_Amharic_Ad_Generation/venv/lib/python3.11/site-packages (from huggingface-hub->accelerate==0.27.0.dev0) (4.66.1)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/eyaya_eneyew/week_7/LLM_Finetuning_For_Amharic_Ad_Generation/venv/lib/python3.11/site-packages (from jinja2->torch<2.2.0,>=1.10.0->accelerate==0.27.0.dev0) (2.1.4)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/eyaya_eneyew/week_7/LLM_Finetuning_For_Amharic_Ad_Generation/venv/lib/python3.11/site-packages (from requests->huggingface-hub->accelerate==0.27.0.dev0) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/eyaya_eneyew/week_7/LLM_Finetuning_For_Amharic_Ad_Generation/venv/lib/python3.11/site-packages (from requests->huggingface-hub->accelerate==0.27.0.dev0) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/eyaya_eneyew/week_7/LLM_Finetuning_For_Amharic_Ad_Generation/venv/lib/python3.11/site-packages (from requests->huggingface-hub->accelerate==0.27.0.dev0) (2.1.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/eyaya_eneyew/week_7/LLM_Finetuning_For_Amharic_Ad_Generation/venv/lib/python3.11/site-packages (from requests->huggingface-hub->accelerate==0.27.0.dev0) (2023.11.17)\n",
      "Requirement already satisfied: mpmath>=0.19 in /home/eyaya_eneyew/week_7/LLM_Finetuning_For_Amharic_Ad_Generation/venv/lib/python3.11/site-packages (from sympy->torch<2.2.0,>=1.10.0->accelerate==0.27.0.dev0) (1.3.0)\n",
      "Building wheels for collected packages: accelerate\n",
      "  Building wheel for accelerate (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for accelerate: filename=accelerate-0.27.0.dev0-py3-none-any.whl size=274019 sha256=93692532dddc909f0721b9be466e33c53de8b84aa26bb0bdc0d31eb28912aea9\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-ohhgtdxb/wheels/18/af/f7/facfc4ea8d2484e23fc8489825221fe5826625fad79301dd99\n",
      "Successfully built accelerate\n",
      "Installing collected packages: accelerate\n",
      "  Attempting uninstall: accelerate\n",
      "    Found existing installation: accelerate 0.21.0\n",
      "    Uninstalling accelerate-0.21.0:\n",
      "      Successfully uninstalled accelerate-0.21.0\n",
      "Successfully installed accelerate-0.27.0.dev0\n",
      "Looking in indexes: https://download.pytorch.org/whl/cu118\n",
      "Requirement already satisfied: torch in /home/eyaya_eneyew/week_7/LLM_Finetuning_For_Amharic_Ad_Generation/venv/lib/python3.11/site-packages (2.1.2)\n",
      "Requirement already satisfied: torchvision in /home/eyaya_eneyew/week_7/LLM_Finetuning_For_Amharic_Ad_Generation/venv/lib/python3.11/site-packages (0.16.2)\n",
      "Collecting torchaudio\n",
      "  Using cached https://download.pytorch.org/whl/cu118/torchaudio-2.2.0%2Bcu118-cp311-cp311-linux_x86_64.whl (3.3 MB)\n",
      "Requirement already satisfied: filelock in /home/eyaya_eneyew/week_7/LLM_Finetuning_For_Amharic_Ad_Generation/venv/lib/python3.11/site-packages (from torch) (3.13.1)\n",
      "Requirement already satisfied: typing-extensions in /home/eyaya_eneyew/week_7/LLM_Finetuning_For_Amharic_Ad_Generation/venv/lib/python3.11/site-packages (from torch) (4.9.0)\n",
      "Requirement already satisfied: sympy in /home/eyaya_eneyew/week_7/LLM_Finetuning_For_Amharic_Ad_Generation/venv/lib/python3.11/site-packages (from torch) (1.12)\n",
      "Requirement already satisfied: networkx in /home/eyaya_eneyew/week_7/LLM_Finetuning_For_Amharic_Ad_Generation/venv/lib/python3.11/site-packages (from torch) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in /home/eyaya_eneyew/week_7/LLM_Finetuning_For_Amharic_Ad_Generation/venv/lib/python3.11/site-packages (from torch) (3.1.3)\n",
      "Requirement already satisfied: fsspec in /home/eyaya_eneyew/week_7/LLM_Finetuning_For_Amharic_Ad_Generation/venv/lib/python3.11/site-packages (from torch) (2023.10.0)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /home/eyaya_eneyew/week_7/LLM_Finetuning_For_Amharic_Ad_Generation/venv/lib/python3.11/site-packages (from torch) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /home/eyaya_eneyew/week_7/LLM_Finetuning_For_Amharic_Ad_Generation/venv/lib/python3.11/site-packages (from torch) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /home/eyaya_eneyew/week_7/LLM_Finetuning_For_Amharic_Ad_Generation/venv/lib/python3.11/site-packages (from torch) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /home/eyaya_eneyew/week_7/LLM_Finetuning_For_Amharic_Ad_Generation/venv/lib/python3.11/site-packages (from torch) (8.9.2.26)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /home/eyaya_eneyew/week_7/LLM_Finetuning_For_Amharic_Ad_Generation/venv/lib/python3.11/site-packages (from torch) (12.1.3.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /home/eyaya_eneyew/week_7/LLM_Finetuning_For_Amharic_Ad_Generation/venv/lib/python3.11/site-packages (from torch) (11.0.2.54)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /home/eyaya_eneyew/week_7/LLM_Finetuning_For_Amharic_Ad_Generation/venv/lib/python3.11/site-packages (from torch) (10.3.2.106)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /home/eyaya_eneyew/week_7/LLM_Finetuning_For_Amharic_Ad_Generation/venv/lib/python3.11/site-packages (from torch) (11.4.5.107)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /home/eyaya_eneyew/week_7/LLM_Finetuning_For_Amharic_Ad_Generation/venv/lib/python3.11/site-packages (from torch) (12.1.0.106)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.18.1 in /home/eyaya_eneyew/week_7/LLM_Finetuning_For_Amharic_Ad_Generation/venv/lib/python3.11/site-packages (from torch) (2.18.1)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /home/eyaya_eneyew/week_7/LLM_Finetuning_For_Amharic_Ad_Generation/venv/lib/python3.11/site-packages (from torch) (12.1.105)\n",
      "Requirement already satisfied: triton==2.1.0 in /home/eyaya_eneyew/week_7/LLM_Finetuning_For_Amharic_Ad_Generation/venv/lib/python3.11/site-packages (from torch) (2.1.0)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12 in /home/eyaya_eneyew/week_7/LLM_Finetuning_For_Amharic_Ad_Generation/venv/lib/python3.11/site-packages (from nvidia-cusolver-cu12==11.4.5.107->torch) (12.3.101)\n",
      "Requirement already satisfied: numpy in /home/eyaya_eneyew/week_7/LLM_Finetuning_For_Amharic_Ad_Generation/venv/lib/python3.11/site-packages (from torchvision) (1.26.3)\n",
      "Requirement already satisfied: requests in /home/eyaya_eneyew/week_7/LLM_Finetuning_For_Amharic_Ad_Generation/venv/lib/python3.11/site-packages (from torchvision) (2.31.0)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /home/eyaya_eneyew/week_7/LLM_Finetuning_For_Amharic_Ad_Generation/venv/lib/python3.11/site-packages (from torchvision) (10.2.0)\n",
      "INFO: pip is looking at multiple versions of torchaudio to determine which version is compatible with other requirements. This could take a while.\n",
      "  Downloading https://download.pytorch.org/whl/cu118/torchaudio-2.1.2%2Bcu118-cp311-cp311-linux_x86_64.whl (3.2 MB)\n",
      "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m3.2/3.2 MB\u001b[0m \u001b[31m80.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: MarkupSafe>=2.0 in /home/eyaya_eneyew/week_7/LLM_Finetuning_For_Amharic_Ad_Generation/venv/lib/python3.11/site-packages (from jinja2->torch) (2.1.4)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/eyaya_eneyew/week_7/LLM_Finetuning_For_Amharic_Ad_Generation/venv/lib/python3.11/site-packages (from requests->torchvision) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/eyaya_eneyew/week_7/LLM_Finetuning_For_Amharic_Ad_Generation/venv/lib/python3.11/site-packages (from requests->torchvision) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/eyaya_eneyew/week_7/LLM_Finetuning_For_Amharic_Ad_Generation/venv/lib/python3.11/site-packages (from requests->torchvision) (2.1.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/eyaya_eneyew/week_7/LLM_Finetuning_For_Amharic_Ad_Generation/venv/lib/python3.11/site-packages (from requests->torchvision) (2023.11.17)\n",
      "Requirement already satisfied: mpmath>=0.19 in /home/eyaya_eneyew/week_7/LLM_Finetuning_For_Amharic_Ad_Generation/venv/lib/python3.11/site-packages (from sympy->torch) (1.3.0)\n",
      "Installing collected packages: torchaudio\n",
      "Successfully installed torchaudio-2.1.2+cu118\n"
     ]
    }
   ],
   "source": [
    "! pip install datasets transformers[sentencepiece]\n",
    "!pip install -q accelerate==0.21.0 --progress-bar off\n",
    "!pip install -q peft==0.4.0 --progress-bar off\n",
    "!pip install -q bitsandbytes==0.40.2 --progress-bar off\n",
    "!pip install -q transformers==4.31.0 --progress-bar off\n",
    "!pip install -q trl==0.4.7 --progress-bar off\n",
    "!pip install git+https://github.com/huggingface/peft\n",
    "!pip install git+https://github.com/huggingface/accelerate\n",
    "!pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b53b748e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/eyaya_eneyew/week_7/LLM_Finetuning_For_Amharic_Ad_Generation/venv/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from random import randrange\n",
    "from functools import partial\n",
    "import torch\n",
    "from datasets import load_dataset\n",
    "from transformers import (AutoModelForCausalLM,\n",
    "                          AutoTokenizer,\n",
    "                          BitsAndBytesConfig,\n",
    "                          HfArgumentParser,\n",
    "                          Trainer,\n",
    "                          TrainingArguments,\n",
    "                          DataCollatorForLanguageModeling,\n",
    "                          EarlyStoppingCallback,\n",
    "                          pipeline,\n",
    "                          logging,\n",
    "                          set_seed)\n",
    "\n",
    "import bitsandbytes as bnb\n",
    "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training, PeftModel, AutoPeftModelForCausalLM\n",
    "from trl import SFTTrainer\n",
    "from contextlib import nullcontext\n",
    "from transformers import (\n",
    "    LlamaForCausalLM, \n",
    "    LlamaTokenizer, \n",
    "    TrainerCallback, \n",
    "    default_data_collator, \n",
    "    Trainer, \n",
    "    TrainingArguments,\n",
    "    BitsAndBytesConfig\n",
    ")\n",
    "from peft import (\n",
    "    LoraConfig,\n",
    "    TaskType,\n",
    "    prepare_model_for_int8_training,\n",
    "    PeftModel\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f4b3881b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_bnb_config(load_in_4bit, bnb_4bit_use_double_quant, bnb_4bit_quant_type, bnb_4bit_compute_dtype):\n",
    "    \"\"\"\n",
    "    Configures model quantization method using bitsandbytes to speed up training and inference\n",
    "\n",
    "    :param load_in_4bit: Load model in 4-bit precision mode\n",
    "    :param bnb_4bit_use_double_quant: Nested quantization for 4-bit model\n",
    "    :param bnb_4bit_quant_type: Quantization data type for 4-bit model\n",
    "    :param bnb_4bit_compute_dtype: Computation data type for 4-bit model\n",
    "    \"\"\"\n",
    "\n",
    "    bnb_config = BitsAndBytesConfig(\n",
    "        load_in_4bit = load_in_4bit,\n",
    "        bnb_4bit_use_double_quant = bnb_4bit_use_double_quant,\n",
    "        bnb_4bit_quant_type = bnb_4bit_quant_type,\n",
    "        bnb_4bit_compute_dtype = bnb_4bit_compute_dtype,\n",
    "    )\n",
    "\n",
    "    return bnb_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0b13706f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model(bnb_config):\n",
    "    \"\"\"\n",
    "    Loads model and model tokenizer\n",
    "\n",
    "    :param model_name: Hugging Face model name\n",
    "    :param bnb_config: Bitsandbytes configuration\n",
    "    \"\"\"\n",
    "    model_name = \"/model/Llama-2-7b-hf\"\n",
    "    #PT_DIR = \"/model/llama-2-amharic-3784m\"\n",
    "    # Get number of GPU device and set maximum memory\n",
    "    n_gpus = torch.cuda.device_count()\n",
    "    max_memory = f'{21960}MB'\n",
    "    tokenizer = LlamaTokenizer.from_pretrained(model_name)\n",
    "    # Load model\n",
    "    model = LlamaForCausalLM.from_pretrained(\n",
    "        model_name,\n",
    "        quantization_config = bnb_config,\n",
    "        device_map = \"auto\", # dispatch the model efficiently on the available resources\n",
    "        torch_dtype=torch.float16 #max_memory = {i: max_memory for i in range(n_gpus)},\n",
    "    )\n",
    "    #model = PeftModel.from_pretrained(model, PT_DIR)\n",
    "    # Load model tokenizer with the user authentication token\n",
    "    \n",
    "    #tokenizer = LlamaTokenizer.from_pretrained(model_name)\n",
    "    # Set padding token as EOS token\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    return model, tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d32a8e12",
   "metadata": {},
   "outputs": [],
   "source": [
    "################################################################################\n",
    "# transformers parameters\n",
    "################################################################################\n",
    "\n",
    "# The pre-trained model from the Hugging Face Hub to load and fine-tune\n",
    "# model_name = \"/model/Llama-2-7b-hf\"\n",
    "\n",
    "################################################################################\n",
    "# bitsandbytes parameters\n",
    "################################################################################\n",
    "\n",
    "# Activate 4-bit precision base model loading\n",
    "load_in_4bit = True\n",
    "\n",
    "# Activate nested quantization for 4-bit base models (double quantization)\n",
    "bnb_4bit_use_double_quant = True\n",
    "\n",
    "# Quantization type (fp4 or nf4)\n",
    "bnb_4bit_quant_type = \"nf4\"\n",
    "\n",
    "# Compute data type for 4-bit base models\n",
    "bnb_4bit_compute_dtype = torch.bfloat16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2127529e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using the legacy behaviour of the <class 'transformers.models.llama.tokenization_llama.LlamaTokenizer'>. This means that tokens that come after special tokens will not be properly handled. We recommend you to read the related pull request available at https://github.com/huggingface/transformers/pull/24565\n",
      "Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:02<00:00,  1.06s/it]\n"
     ]
    }
   ],
   "source": [
    "bnb_config = create_bnb_config(load_in_4bit, bnb_4bit_use_double_quant, bnb_4bit_quant_type, bnb_4bit_compute_dtype)\n",
    "\n",
    "model, tokenizer = load_model(bnb_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "283255a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset\n",
    "dataset_name = \"/data/fine_tun_data2.json\"\n",
    "dataset = load_dataset(\"json\", data_files = dataset_name, split = \"train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ab1f43ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of prompts: 1611\n",
      "Column names are: ['output', 'instruction', 'input']\n"
     ]
    }
   ],
   "source": [
    "print(f'Number of prompts: {len(dataset)}')\n",
    "print(f'Column names are: {dataset.column_names}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cadeb60d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'output': 'advertisement',\n",
       " 'instruction': 'Categorize the input into one of the 2 categories:\\n\\nadvertisement\\nnot advertisement\\n\\n',\n",
       " 'input': 'ADVERTISMENT áˆ˜áˆáŠ«áˆ áŠáŒˆáˆ­ áŠ¨áˆ‰áˆ² á‹¨áŠ áŒ¥áŠ•á‰µáŠ“ á‹¨áˆ˜áŒˆáŒ£áŒ áˆšá‹« á‰€á‹¶ áˆ•áŠ­áˆáŠ“ áˆ›áŠ¥áŠ¨áˆ áˆˆáŠ áŒ¥áŠ•á‰µáŠ“ áˆˆáˆ˜áŒˆáŒ£áŒ áˆšá‹« á‰½áŒáˆ®á‰½áŠ“ áˆ…áˆ˜áˆžá‰½ áŠ á‹­áŠá‰°áŠ› áˆ˜áá‰µáˆ„ á‹¨áˆáŠ•áˆ°áŒ£á‰¸á‹ á‹¨áˆ…áŠ­áˆáŠ“ áŠ áŒˆáˆáŒáˆŽá‰¶á‰½ âœ”áˆ›áŠ•áŠ›á‹áˆ áˆµá‰¥áˆ«á‰µáŠ“ á‹áˆá‰ƒá‰µ áˆ•áŠ­áˆáŠ“ âœ”á‹¨áŒ‰áˆá‰ á‰µáŠ“ á‹³áˆŒ áˆ˜áŒˆáŒ£áŒ áˆšá‹« á‰½áŒáˆ­ âœ”á‹áˆµá‰¥áˆµá‰¥ áŠ¨á‰£á‹µ áˆµá‰¥áˆ«á‰¶á‰½áŠ“ á‹áˆá‰ƒá‰¶á‰½ âœ”á‹¨áŒ¡áŠ•á‰»áŠ“ á‹¨áŒ…áˆ›á‰µ áŒ‰á‹³á‰µ áˆ…áˆ˜áˆžá‰½áŠ• áˆ›áŠ¨áˆ âœ”á‹¨áˆ•áƒáŠ“á‰µ áŠ¥áŠ“ áŠ á‹‹á‰‚á‹Žá‰½ áŠ¥áŒ…áŠ“ áŠ¥áŒáˆ­ áˆ˜áŒ£áˆ˜áˆ áˆ›áˆµá‰°áŠ«áŠ¨áˆ âœ”áˆµá‰¥áˆ«á‰¶á‰½áŠ•áŠ“ á‹áˆá‰ƒá‰¶á‰½áŠ• á‹«áˆˆ á‰€á‹¶ áˆ•áŠ­áˆáŠ“ áˆ›áŠ¨áˆ âœ”á‰ á‰€á‹¶ áˆ•áŠ­áˆáŠ“ á‹¨áŒˆá‰¡ á‹¨á‰°áˆˆá‹«á‹© á‰¥áˆ¨á‰¶á‰½áŠ• áˆ›á‹áŒ£á‰µ âœ”á‹¨á•áˆ‹áˆµá‰²áŠ­ áˆ°áˆ­áŒ€áˆª áˆ•áŠ­áˆáŠ“ âœ”á‰ á‹³áˆŒáŠ“ á‹³áˆŒ áŒˆáŠ•á‹³ áˆµá‰¥áˆ«á‰¶á‰½áŠ• á‰ ááˆŽáˆ®áˆµáŠ®á’ á‹¨á‰³áŒˆá‹˜ áˆ•áŠ­áˆáŠ“ áˆ›á‹µáˆ¨áŒ ðŸ“Œáˆáˆá‹µáŠ“ á‰¥á‰ƒá‰µ á‰£áˆ‹á‰¸á‹ áˆµá”áˆ»áˆŠáˆµá‰µ áŠ¥áŠ“ áˆ°á‰¥ áˆµá”áˆ»áˆŠáˆµá‰µ áˆ€áŠªáˆžá‰½ áŠ¥áŒ…áŒ á‹˜áˆ˜áŠ“á‹Š á‰ áˆ†áŠ‘ á‹¨áˆ…áŠ­áˆáŠ“ áˆ˜áˆ³áˆªá‹«á‹ˆá‰½ á‰ áˆ˜á‰³áŒˆá‹ áˆáˆ‰áŠ•áˆ á‹¨áˆ…áŠ­áˆáŠ“ áŠ áŒˆáˆáŒáˆŽá‰¶á‰½ á‰ á‰°áˆ˜áŒ£áŒ£áŠ á‹‹áŒ‹ á‹«áŒˆáŠ›áˆ‰â€¼ áŠ á‹µáˆ«áˆ» : áŠ á‹¨áˆ­ áŒ¤áŠ“ áŒ…áˆ›á‰ áˆ­ á–áˆŠáˆµ áŒ£á‰¢á‹« áŒŽáŠ• ðŸ“² 0913468103 0953912229'}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[randrange(len(dataset))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "be03889d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1611"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "94d04175",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_prompt_formats(sample):\n",
    "    \"\"\"\n",
    "    Creates a formatted prompt template for a prompt in the instruction dataset\n",
    "\n",
    "    :param sample: Prompt or sample from the instruction dataset\n",
    "    \"\"\"\n",
    "\n",
    "    # Initialize static strings for the prompt template\n",
    "    INTRO_BLURB = \"Below is an instruction that describes a task. Write a response that appropriately completes the request.\"\n",
    "    INSTRUCTION_KEY = \"### Instruction:\"\n",
    "    INPUT_KEY = \"Input:\"\n",
    "    RESPONSE_KEY = \"### Response:\"\n",
    "    END_KEY = \"### End\"\n",
    "\n",
    "    # Combine a prompt with the static strings\n",
    "    blurb = f\"{INTRO_BLURB}\"\n",
    "    instruction = f\"{INSTRUCTION_KEY}\\n{sample['instruction']}\"\n",
    "    input_context = f\"{INPUT_KEY}\\n{sample['input']}\" if sample[\"input\"] else None\n",
    "    response = f\"{RESPONSE_KEY}\\n{sample['output']}\"\n",
    "    end = f\"{END_KEY}\"\n",
    "\n",
    "    # Create a list of prompt template elements\n",
    "    parts = [part for part in [blurb, instruction, input_context, response, end] if part]\n",
    "\n",
    "    # Join prompt template elements into a single string to create the prompt template\n",
    "    formatted_prompt = \"\\n\\n\".join(parts)\n",
    "\n",
    "    # Store the formatted prompt template in a new key \"text\"\n",
    "    sample[\"text\"] = formatted_prompt\n",
    "\n",
    "    return sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c3879bf0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'output': 'advertisement',\n",
       " 'instruction': 'Categorize the input into one of the 2 categories:\\n\\nadvertisement\\nnot advertisement\\n\\n',\n",
       " 'input': 'ADVERTISMENT áŠ áŠá‹« áˆ™áˆ€áˆ˜á‹µ áŠ¨áá‰°áŠ› á‹¨á‰£áˆ…áˆ áˆ•áŠ­áˆáŠ“ áŠ¥áŠ“ á‹˜áˆ˜áŠ“á‹Š á‹¨á‹‹áŒáˆá‰µ áŠ áŒˆáˆáŒáˆŽá‰µ á‹¨áˆáŠ•áˆ°áŒ£á‰¸á‹ á‹¨á‰£áˆ…áˆ áˆ…áŠ­áˆáŠ“á‹Žá‰½ âž¢ áˆˆá‹áŒ­áŠ“ áˆˆá‹áˆµáŒ¥ áŠªáŠ•á‰³áˆ®á‰µ âž¢ áˆˆáˆ›á‹µá‹«á‰µ âž¢ áˆˆáˆ±áŠ³áˆ­ á‰ áˆ½á‰³ âž¢áˆˆáŒ‰á‰ á‰µ(áˆˆá‹ˆá á‰ áˆ½á‰³) âž¢áˆˆáŒ¨áŒŽáˆ« áˆ…áˆ˜áˆ âž¢áˆˆáˆµáˆá‰°á‹ˆáˆ²á‰¥ âž¢áˆˆá‹°áˆ áŒáŠá‰µ âž¢áˆˆáŠ áˆµáˆ á‹ˆá‹­áˆ áˆ³á‹­áŠáˆµ âž¢áˆˆáˆšáŒ¥áˆ á‰ áˆºá‰³ âž¢ áˆˆáŠ¥áˆªáˆ…áŠ“ á‰áˆ­áŒ¥áˆ›á‰µ âž¢áˆˆáˆ«áˆµ áˆ…áˆ˜áˆ (áˆ›á‹­áŒáˆªáŠ•) âž¢áˆˆá‰ºáŒ áŠ“ áˆˆáŒ­áˆ­á‰µ âž¢áˆˆá‰‹á‰á‰»áŠ“ áŽáˆ¨áŽáˆ­ âž¢áˆˆáŠ¥áŒ¢áŠ“ áˆˆáŠ¥á‰£áŒ­ âž¢áˆˆá‹ˆáŒˆá‰¥ áˆ…áˆ˜áˆ âž¢áˆˆáˆ˜áŠ«áŠ•áŠá‰µ áˆˆá‹ˆá‹µáˆ áˆˆáˆ´á‰µáˆ âž¢áˆˆáŒ†áˆ®áŠ“ áˆˆáŠ á‹­áŠ• áˆ…áˆ˜áˆ âž¢áˆˆáˆ†á‹µ áˆ…áˆ˜áˆ âž¢á‹˜áˆ˜áŠ“á‹Š á‹¨á‹‹áŒáˆá‰µ áŠ áŒˆáˆáŒáˆŽá‰µ á‰ á‰°á‰‹áˆ›á‰½áŠ• áŠ¥áŠ•áˆ°áŒ£áˆˆáŠ•á¢ ðŸ‘‰áŠ¨áŠ¢á‰µá‹®áŒµá‹« á‰£áˆ…áˆ‹á‹Š áˆ…áŠ­áˆáŠ“ áŠ á‹‹á‰‚á‹Žá‰½ áˆ›áˆ…á‰ áˆ­ á‰ á‹˜áˆ­á‰ áˆ…áŒ‹á‹Š á‹¨á‰£áˆ…áˆ áˆ…áŠ­áˆáŠ“ áá‰ƒá‹µ á‹«áˆˆáŠ• áŠáŠ•á¢ áŠ á‹µáˆ«áˆ»:áŠ á‹²áˆµ áŠ á‰ á‰£ áŠ á‹¨áˆ­ áŒ¤áŠ“ áˆµáˆáŠ­ á‰áŒ¥áˆ­ ðŸ“²0927506650 ðŸ“²0987133734 ðŸ“²0939605455 á‰´áˆŒáŒáˆ«áˆ á‰»áŠ“áˆ‹á‰½áŠ• ntvE5NmM0',\n",
       " 'text': 'Below is an instruction that describes a task. Write a response that appropriately completes the request.\\n\\n### Instruction:\\nCategorize the input into one of the 2 categories:\\n\\nadvertisement\\nnot advertisement\\n\\n\\n\\nInput:\\nADVERTISMENT áŠ áŠá‹« áˆ™áˆ€áˆ˜á‹µ áŠ¨áá‰°áŠ› á‹¨á‰£áˆ…áˆ áˆ•áŠ­áˆáŠ“ áŠ¥áŠ“ á‹˜áˆ˜áŠ“á‹Š á‹¨á‹‹áŒáˆá‰µ áŠ áŒˆáˆáŒáˆŽá‰µ á‹¨áˆáŠ•áˆ°áŒ£á‰¸á‹ á‹¨á‰£áˆ…áˆ áˆ…áŠ­áˆáŠ“á‹Žá‰½ âž¢ áˆˆá‹áŒ­áŠ“ áˆˆá‹áˆµáŒ¥ áŠªáŠ•á‰³áˆ®á‰µ âž¢ áˆˆáˆ›á‹µá‹«á‰µ âž¢ áˆˆáˆ±áŠ³áˆ­ á‰ áˆ½á‰³ âž¢áˆˆáŒ‰á‰ á‰µ(áˆˆá‹ˆá á‰ áˆ½á‰³) âž¢áˆˆáŒ¨áŒŽáˆ« áˆ…áˆ˜áˆ âž¢áˆˆáˆµáˆá‰°á‹ˆáˆ²á‰¥ âž¢áˆˆá‹°áˆ áŒáŠá‰µ âž¢áˆˆáŠ áˆµáˆ á‹ˆá‹­áˆ áˆ³á‹­áŠáˆµ âž¢áˆˆáˆšáŒ¥áˆ á‰ áˆºá‰³ âž¢ áˆˆáŠ¥áˆªáˆ…áŠ“ á‰áˆ­áŒ¥áˆ›á‰µ âž¢áˆˆáˆ«áˆµ áˆ…áˆ˜áˆ (áˆ›á‹­áŒáˆªáŠ•) âž¢áˆˆá‰ºáŒ áŠ“ áˆˆáŒ­áˆ­á‰µ âž¢áˆˆá‰‹á‰á‰»áŠ“ áŽáˆ¨áŽáˆ­ âž¢áˆˆáŠ¥áŒ¢áŠ“ áˆˆáŠ¥á‰£áŒ­ âž¢áˆˆá‹ˆáŒˆá‰¥ áˆ…áˆ˜áˆ âž¢áˆˆáˆ˜áŠ«áŠ•áŠá‰µ áˆˆá‹ˆá‹µáˆ áˆˆáˆ´á‰µáˆ âž¢áˆˆáŒ†áˆ®áŠ“ áˆˆáŠ á‹­áŠ• áˆ…áˆ˜áˆ âž¢áˆˆáˆ†á‹µ áˆ…áˆ˜áˆ âž¢á‹˜áˆ˜áŠ“á‹Š á‹¨á‹‹áŒáˆá‰µ áŠ áŒˆáˆáŒáˆŽá‰µ á‰ á‰°á‰‹áˆ›á‰½áŠ• áŠ¥áŠ•áˆ°áŒ£áˆˆáŠ•á¢ ðŸ‘‰áŠ¨áŠ¢á‰µá‹®áŒµá‹« á‰£áˆ…áˆ‹á‹Š áˆ…áŠ­áˆáŠ“ áŠ á‹‹á‰‚á‹Žá‰½ áˆ›áˆ…á‰ áˆ­ á‰ á‹˜áˆ­á‰ áˆ…áŒ‹á‹Š á‹¨á‰£áˆ…áˆ áˆ…áŠ­áˆáŠ“ áá‰ƒá‹µ á‹«áˆˆáŠ• áŠáŠ•á¢ áŠ á‹µáˆ«áˆ»:áŠ á‹²áˆµ áŠ á‰ á‰£ áŠ á‹¨áˆ­ áŒ¤áŠ“ áˆµáˆáŠ­ á‰áŒ¥áˆ­ ðŸ“²0927506650 ðŸ“²0987133734 ðŸ“²0939605455 á‰´áˆŒáŒáˆ«áˆ á‰»áŠ“áˆ‹á‰½áŠ• ntvE5NmM0\\n\\n### Response:\\nadvertisement\\n\\n### End'}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "create_prompt_formats(dataset[randrange(len(dataset))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e54a2523",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_max_length(model):\n",
    "    \"\"\"\n",
    "    Extracts maximum token length from the model configuration\n",
    "\n",
    "    :param model: Hugging Face model\n",
    "    \"\"\"\n",
    "\n",
    "    # Pull model configuration\n",
    "    conf = model.config\n",
    "    # Initialize a \"max_length\" variable to store maximum sequence length as null\n",
    "    max_length = None\n",
    "    # Find maximum sequence length in the model configuration and save it in \"max_length\" if found\n",
    "    for length_setting in [\"n_positions\", \"max_position_embeddings\", \"seq_length\"]:\n",
    "        max_length = getattr(model.config, length_setting, None)\n",
    "        if max_length:\n",
    "            print(f\"Found max lenth: {max_length}\")\n",
    "            break\n",
    "    # Set \"max_length\" to 1024 (default value) if maximum sequence length is not found in the model configuration\n",
    "    if not max_length:\n",
    "        max_length = 1024\n",
    "        print(f\"Using default max length: {max_length}\")\n",
    "    return max_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a0986992",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_batch(batch, tokenizer, max_length):\n",
    "    \"\"\"\n",
    "    Tokenizes dataset batch\n",
    "\n",
    "    :param batch: Dataset batch\n",
    "    :param tokenizer: Model tokenizer\n",
    "    :param max_length: Maximum number of tokens to emit from the tokenizer\n",
    "    \"\"\"\n",
    "\n",
    "    return tokenizer(\n",
    "        batch[\"text\"],\n",
    "        max_length = max_length,\n",
    "        truncation = True,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7fca9005",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_dataset(tokenizer, max_length, seed, dataset):\n",
    "    \"\"\"\n",
    "    Tokenizes dataset for fine-tuning\n",
    "\n",
    "    :param tokenizer (AutoTokenizer): Model tokenizer\n",
    "    :param max_length (int): Maximum number of tokens to emit from the tokenizer\n",
    "    :param seed: Random seed for reproducibility\n",
    "    :param dataset (str): Instruction dataset\n",
    "    \"\"\"\n",
    "\n",
    "    # Add prompt to each sample\n",
    "    print(\"Preprocessing dataset...\")\n",
    "    dataset = dataset.map(create_prompt_formats)\n",
    "\n",
    "    # Apply preprocessing to each batch of the dataset & and remove \"instruction\", \"input\", \"output\", and \"text\" fields\n",
    "    _preprocessing_function = partial(preprocess_batch, max_length = max_length, tokenizer = tokenizer)\n",
    "    dataset = dataset.map(\n",
    "        _preprocessing_function,\n",
    "        batched = True,\n",
    "        remove_columns = [\"instruction\", \"input\", \"output\", \"text\"],\n",
    "    )\n",
    "\n",
    "    # Filter out samples that have \"input_ids\" exceeding \"max_length\"\n",
    "    dataset = dataset.filter(lambda sample: len(sample[\"input_ids\"]) < max_length)\n",
    "\n",
    "    # Shuffle dataset\n",
    "    dataset = dataset.shuffle(seed = seed)\n",
    "\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a930433b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found max lenth: 2048\n",
      "Preprocessing dataset...\n"
     ]
    }
   ],
   "source": [
    "# Random seed\n",
    "seed = 22\n",
    "\n",
    "max_length = get_max_length(model)\n",
    "preprocessed_dataset = preprocess_dataset(tokenizer, max_length, seed, dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "57e75551",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['input_ids', 'attention_mask'],\n",
      "    num_rows: 1611\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "print(preprocessed_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5fe8fbfe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s>Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
      "\n",
      "### Instruction:\n",
      "Categorize the input into one of the 2 categories:\n",
      "\n",
      "advertisement\n",
      "not advertisement\n",
      "\n",
      "\n",
      "\n",
      "Input:\n",
      "á‹¨áˆ›áˆµá‰³á‹ˆá‰‚á‹« áŒá‰¥á‹£ðŸ‘‡ á‹¨á‰°áŠ¨á‰ áˆ«á‰½áˆ á‰¤á‰°áˆ°á‰¦á‰½ áŠ¨53,700 á‰ áˆ‹á‹­ á‰°áŠ¨á‰³á‹­ á‰£áˆˆá‹ áŠ¥áŠ“ á‰ á‹°á‰‚á‰ƒá‹Žá‰½ á‹áˆµáŒ¥ á‰ áˆºáˆ… á‹¨áˆšá‰†áŒ áˆ© áŒŽá‰¥áŠ á‰£áˆˆá‹ á‰ á‹šáˆ… á‹¨á‰´áˆŒáŒáˆ«áˆ á‰»áŠ“áˆ áˆ‹á‹­ á‰ áˆá‹© á‰…áŠ“áˆ½ áˆáˆ­á‰µáŠ“ áŠ áŒˆáˆáŒáˆŽá‰³á‰½áˆáŠ• áŠ¥áŠ•á‹µá‰³áˆµá‰°á‹‹á‹á‰ á‰ áŠ áŠ­á‰¥áˆ®á‰µ áŠ¥áŒ‹á‰¥á‹›áˆˆáˆâ€¼ âœ”Youtube á‰»áŠ“áˆ â€¦ âœ”á‹¨á‰µáˆáˆ…áˆ­á‰µ á‰°á‰‹áˆ›á‰µâ€¦ âœ”á‹¨áˆ…áŠ­áˆáŠ“ áˆ›áŠ¥áŠ¨áˆ‹á‰µ (á‰£áˆ…áˆ‹á‹Š /á‹˜áˆ˜áŠ“á‹Š)â€¦ âœ”á‹¨á‰µáˆ«áŠ•áˆµá–áˆ­á‰µ áŠ áŒˆáˆáŒáˆŽá‰µâ€¦ âœ”á‹¨áˆµáˆ« áŠ áŒˆáŠ“áŠ áŠ¤áŒ€áŠ•áˆ²á‹Žá‰½â€¦â€¦ âœ”áˆ†á‰´áˆá£áŠ«áŒáŠ“ áˆ¬áˆµá‰¶áˆ«áŠ•á‰¶á‰½ âœ”áˆŒáˆŽá‰½ á‹¨áˆá‰µáˆ°áŒ§á‰¸á‹áŠ•áˆ áŠ áŒˆáˆáŒáˆŽá‰¶á‰½áŠ• áˆ›áˆµá‰°á‹‹á‹ˆá‰… áˆˆáˆá‰µáˆáˆáŒ‰ áŠ‘ áŠ á‰¥áˆ¨áŠ• áŠ¥áŠ•áˆµáˆ« á‰¥á‹«áˆˆáˆâ€¼ áˆ›áˆµá‰³á‹ˆá‰‚á‹«á‹ŽáŠ• á‰ á‹šáˆ… á‹­áˆ‹áŠ©ðŸ‘‡\n",
      "\n",
      "### Response:\n",
      "advertisement\n",
      "\n",
      "### End\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.decode(preprocessed_dataset[24]['input_ids']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8a005a43",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_peft_config(r, lora_alpha, target_modules, lora_dropout, bias, task_type):\n",
    "    \"\"\"\n",
    "    Creates Parameter-Efficient Fine-Tuning configuration for the model\n",
    "\n",
    "    :param r: LoRA attention dimension\n",
    "    :param lora_alpha: Alpha parameter for LoRA scaling\n",
    "    :param modules: Names of the modules to apply LoRA to\n",
    "    :param lora_dropout: Dropout Probability for LoRA layers\n",
    "    :param bias: Specifies if the bias parameters should be trained\n",
    "    \"\"\"\n",
    "    config = LoraConfig(\n",
    "        r = r,\n",
    "        lora_alpha = lora_alpha,\n",
    "        target_modules = target_modules,\n",
    "        lora_dropout = lora_dropout,\n",
    "        bias = bias,\n",
    "        task_type = task_type,\n",
    "    )\n",
    "\n",
    "    return config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f3c16af0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_all_linear_names(model):\n",
    "    \"\"\"\n",
    "    Find modules to apply LoRA to.\n",
    "\n",
    "    :param model: PEFT model\n",
    "    \"\"\"\n",
    "\n",
    "    cls = bnb.nn.Linear4bit\n",
    "    lora_module_names = set()\n",
    "    for name, module in model.named_modules():\n",
    "        if isinstance(module, cls):\n",
    "            names = name.split('.')\n",
    "            lora_module_names.add(names[0] if len(names) == 1 else names[-1])\n",
    "\n",
    "    if 'lm_head' in lora_module_names:\n",
    "        lora_module_names.remove('lm_head')\n",
    "    print(f\"LoRA module names: {list(lora_module_names)}\")\n",
    "    return list(lora_module_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "08645396",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_trainable_parameters(model, use_4bit = False):\n",
    "    \"\"\"\n",
    "    Prints the number of trainable parameters in the model.\n",
    "\n",
    "    :param model: PEFT model\n",
    "    \"\"\"\n",
    "\n",
    "    trainable_params = 0\n",
    "    all_param = 0\n",
    "\n",
    "    for _, param in model.named_parameters():\n",
    "        num_params = param.numel()\n",
    "        if num_params == 0 and hasattr(param, \"ds_numel\"):\n",
    "            num_params = param.ds_numel\n",
    "        all_param += num_params\n",
    "        if param.requires_grad:\n",
    "            trainable_params += num_params\n",
    "\n",
    "    if use_4bit:\n",
    "        trainable_params /= 2\n",
    "\n",
    "    print(\n",
    "        f\"All Parameters: {all_param:,d} || Trainable Parameters: {trainable_params:,d} || Trainable Parameters %: {100 * trainable_params / all_param}\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1bd43c3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fine_tune(model,\n",
    "          tokenizer,\n",
    "          dataset,\n",
    "          lora_r,\n",
    "          lora_alpha,\n",
    "          lora_dropout,\n",
    "          bias,\n",
    "          task_type,\n",
    "          per_device_train_batch_size,\n",
    "          gradient_accumulation_steps,\n",
    "          warmup_steps,\n",
    "          #warmup_ratio,\n",
    "          max_steps,\n",
    "          learning_rate,\n",
    "          fp16,\n",
    "          logging_steps,\n",
    "          output_dir,\n",
    "          optim):\n",
    "    \"\"\"\n",
    "    Prepares and fine-tune the pre-trained model.\n",
    "\n",
    "    :param model: Pre-trained Hugging Face model\n",
    "    :param tokenizer: Model tokenizer\n",
    "    :param dataset: Preprocessed training dataset\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Enable gradient checkpointing to reduce memory usage during fine-tuning\n",
    "        \n",
    "        model.train()\n",
    "        embedding_size = model.get_input_embeddings().weight.shape[0]\n",
    "\n",
    "        if len(tokenizer) != embedding_size:\n",
    "            print(\"resize the embedding size by the size of the tokenizer\")\n",
    "            model.resize_token_embeddings(len(tokenizer))\n",
    "            \n",
    "        model.resize_token_embeddings(len(tokenizer))\n",
    "\n",
    "        print('loading the pretrained model from config')\n",
    "        # Prepare the model for training\n",
    "        model = prepare_model_for_kbit_training(model)\n",
    "        model = PeftModel.from_pretrained(model, \"/model/llama-2-amharic-3784m\")\n",
    "        # Get LoRA module names\n",
    "        target_modules = find_all_linear_names(model)\n",
    "\n",
    "        # Create PEFT configuration for these modules and wrap the model to PEFT\n",
    "        peft_config = create_peft_config(lora_r, lora_alpha, target_modules, lora_dropout, bias, task_type)\n",
    "        model = get_peft_model(model, peft_config)\n",
    "\n",
    "        # Print information about the percentage of trainable parameters\n",
    "        print_trainable_parameters(model)\n",
    "\n",
    "        # Training parameters\n",
    "        trainer = Trainer(\n",
    "            model = model,\n",
    "            train_dataset = dataset,\n",
    "            args = TrainingArguments(\n",
    "                per_device_train_batch_size = per_device_train_batch_size,\n",
    "                gradient_accumulation_steps = gradient_accumulation_steps,\n",
    "                warmup_steps = warmup_steps,\n",
    "                max_steps = max_steps,\n",
    "                learning_rate = learning_rate,\n",
    "                fp16 = fp16,\n",
    "                logging_steps = logging_steps,\n",
    "                output_dir = output_dir,\n",
    "                optim = optim,\n",
    "            ),\n",
    "            data_collator = DataCollatorForLanguageModeling(tokenizer, mlm = False)\n",
    "        )\n",
    "\n",
    "        model.config.use_cache = False\n",
    "\n",
    "        do_train = True\n",
    "\n",
    "        # Launch training and log metrics\n",
    "        print(\"Training...\")\n",
    "\n",
    "        if do_train:\n",
    "            train_result = trainer.train()\n",
    "            metrics = train_result.metrics\n",
    "            trainer.log_metrics(\"train\", metrics)\n",
    "            trainer.save_metrics(\"train\", metrics)\n",
    "            trainer.save_state()\n",
    "            print(metrics)\n",
    "\n",
    "        # Save model\n",
    "        print(\"Saving last checkpoint of the model...\")\n",
    "        os.makedirs(output_dir, exist_ok = True)\n",
    "        trainer.model.save_pretrained(output_dir)\n",
    "\n",
    "        # Free memory for merging weights\n",
    "    except Exception as err:\n",
    "        print(\"Error:\",err)\n",
    "        del model\n",
    "        del trainer\n",
    "        torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "8b0d691a",
   "metadata": {},
   "outputs": [],
   "source": [
    "################################################################################\n",
    "# QLoRA parameters\n",
    "################################################################################\n",
    "\n",
    "# LoRA attention dimension\n",
    "lora_r = 16\n",
    "\n",
    "# Alpha parameter for LoRA scaling\n",
    "lora_alpha = 64\n",
    "\n",
    "# Dropout probability for LoRA layers\n",
    "lora_dropout = 0.05\n",
    "\n",
    "# Bias\n",
    "bias = \"none\"\n",
    "\n",
    "# Task type\n",
    "task_type = \"CAUSAL_LM\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "6496b86a",
   "metadata": {},
   "outputs": [],
   "source": [
    "################################################################################\n",
    "# TrainingArguments parameters\n",
    "################################################################################\n",
    "\n",
    "# Output directory where the model predictions and checkpoints will be stored\n",
    "output_dir = \"./results\"\n",
    "\n",
    "# Batch size per GPU for training\n",
    "per_device_train_batch_size = 1\n",
    "\n",
    "# Number of update steps to accumulate the gradients for\n",
    "gradient_accumulation_steps = 1\n",
    "\n",
    "# Initial learning rate (AdamW optimizer)\n",
    "learning_rate = 1e-5\n",
    "\n",
    "# Optimizer to use\n",
    "optim = \"paged_adamw_32bit\"\n",
    "\n",
    "# Number of training steps (overrides num_train_epochs)\n",
    "max_steps = 100\n",
    "\n",
    "# Linear warmup steps from 0 to learning_rate\n",
    "warmup_steps = 20\n",
    "\n",
    "# Enable fp16/bf16 training (set bf16 to True with an A100)\n",
    "fp16 = True\n",
    "\n",
    "# Log every X updates steps\n",
    "logging_steps = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "792d9ce5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading the pretrained model from config\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LoRA module names: ['base_layer']\n",
      "All Parameters: 4,453,765,120 || Trainable Parameters: 359,792,640 || Trainable Parameters %: 8.078392782419563\n",
      "Training...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='100' max='100' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [100/100 03:44, Epoch 0/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>8.400400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>7.342400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>13.039400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>8.154800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>8.285000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>6.589800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>6.836900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>8.888700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>6.511000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>7.240900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>4.983800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>4.793600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>4.500400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>4.379400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>4.317100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>4.661400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>4.146600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>3.521500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>4.064600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>3.986800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21</td>\n",
       "      <td>3.345000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22</td>\n",
       "      <td>3.806700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23</td>\n",
       "      <td>3.063500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24</td>\n",
       "      <td>3.433700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>2.546700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26</td>\n",
       "      <td>2.562700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27</td>\n",
       "      <td>3.022900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28</td>\n",
       "      <td>3.333800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29</td>\n",
       "      <td>3.976600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>1.289900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>31</td>\n",
       "      <td>2.174800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>32</td>\n",
       "      <td>2.464900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>33</td>\n",
       "      <td>2.355400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>34</td>\n",
       "      <td>2.381300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>35</td>\n",
       "      <td>2.113400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>36</td>\n",
       "      <td>2.873200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>37</td>\n",
       "      <td>1.121200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>38</td>\n",
       "      <td>2.744100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>39</td>\n",
       "      <td>2.352100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>3.411900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>41</td>\n",
       "      <td>1.628600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>42</td>\n",
       "      <td>2.149500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>43</td>\n",
       "      <td>1.817300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>44</td>\n",
       "      <td>1.505400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>45</td>\n",
       "      <td>1.483800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>46</td>\n",
       "      <td>3.219300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>47</td>\n",
       "      <td>2.675000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>48</td>\n",
       "      <td>1.337400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>49</td>\n",
       "      <td>3.105900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>2.259700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>51</td>\n",
       "      <td>3.393500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>52</td>\n",
       "      <td>3.000100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>53</td>\n",
       "      <td>3.403200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>54</td>\n",
       "      <td>2.193000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>55</td>\n",
       "      <td>2.949600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>56</td>\n",
       "      <td>3.144900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>57</td>\n",
       "      <td>1.478700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>58</td>\n",
       "      <td>3.526500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>59</td>\n",
       "      <td>2.768000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>1.068900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>61</td>\n",
       "      <td>2.047100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>62</td>\n",
       "      <td>1.530900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>63</td>\n",
       "      <td>1.359300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>64</td>\n",
       "      <td>3.149200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>65</td>\n",
       "      <td>2.251200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>66</td>\n",
       "      <td>1.162300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>67</td>\n",
       "      <td>1.755500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>68</td>\n",
       "      <td>0.978400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>69</td>\n",
       "      <td>1.908300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>70</td>\n",
       "      <td>1.846500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>71</td>\n",
       "      <td>2.553300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>72</td>\n",
       "      <td>0.704100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>73</td>\n",
       "      <td>3.209500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>74</td>\n",
       "      <td>0.699700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>75</td>\n",
       "      <td>1.950200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>76</td>\n",
       "      <td>2.190700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>77</td>\n",
       "      <td>0.979500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>78</td>\n",
       "      <td>0.772000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>79</td>\n",
       "      <td>2.934900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>0.766000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>81</td>\n",
       "      <td>3.318100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>82</td>\n",
       "      <td>3.338400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>83</td>\n",
       "      <td>1.855500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>84</td>\n",
       "      <td>2.491100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>85</td>\n",
       "      <td>0.846400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>86</td>\n",
       "      <td>2.520400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>87</td>\n",
       "      <td>2.302000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>88</td>\n",
       "      <td>2.713000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>89</td>\n",
       "      <td>2.316600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>90</td>\n",
       "      <td>2.493800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>91</td>\n",
       "      <td>2.607500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>92</td>\n",
       "      <td>2.840500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>93</td>\n",
       "      <td>2.171300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>94</td>\n",
       "      <td>2.432000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>95</td>\n",
       "      <td>0.774900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>96</td>\n",
       "      <td>1.945800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>97</td>\n",
       "      <td>0.614300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>98</td>\n",
       "      <td>2.825700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>99</td>\n",
       "      <td>2.144300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>1.240000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***** train metrics *****\n",
      "  epoch                    =       0.06\n",
      "  total_flos               =   532552GF\n",
      "  train_loss               =     3.0567\n",
      "  train_runtime            = 0:03:46.86\n",
      "  train_samples_per_second =      0.441\n",
      "  train_steps_per_second   =      0.441\n",
      "{'train_runtime': 226.8638, 'train_samples_per_second': 0.441, 'train_steps_per_second': 0.441, 'total_flos': 571823530131456.0, 'train_loss': 3.056667756438255, 'epoch': 0.06}\n",
      "Saving last checkpoint of the model...\n"
     ]
    }
   ],
   "source": [
    "fine_tune(model,\n",
    "      tokenizer,\n",
    "      preprocessed_dataset,\n",
    "      lora_r,\n",
    "      lora_alpha,\n",
    "      lora_dropout,\n",
    "      bias,\n",
    "      task_type,\n",
    "      per_device_train_batch_size,\n",
    "      gradient_accumulation_steps,\n",
    "      warmup_steps,\n",
    "      max_steps,\n",
    "      learning_rate,\n",
    "      fp16,\n",
    "      logging_steps,\n",
    "      output_dir,\n",
    "      optim)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "910fbddd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_prompt_formats_for_test(sample):\n",
    "    'Categorize the input into one of the 2 categories:\\n\\nadvertisement\\nnot advertisement\\n\\n'\n",
    "    \"\"\"\n",
    "    Format various fields of the sample ('input', 'label',)\n",
    "    Then concatenate them using two newline characters\n",
    "    :param sample: Sample dictionnary\n",
    "    \"\"\"\n",
    "\n",
    "    # Initialize static strings for the prompt template\n",
    "    INTRO_BLURB = \"Below is an instruction that describes a task. Write a response that appropriately completes the request.\"\n",
    "    INSTRUCTION_KEY = \"### Instruction:\"\n",
    "    INPUT_KEY = \"Input:\"\n",
    "    RESPONSE_KEY = \"### Response:\"\n",
    "    END_KEY = \"### End\"\n",
    "\n",
    "    # Combine a prompt with the static strings\n",
    "    blurb = f\"{INTRO_BLURB}\"\n",
    "    instruction = f\"{INSTRUCTION_KEY}\\nCategorize the input into one of the two categories output only either advertisement or not advertisement. NOTHING ELSE.\\n\\n\"\n",
    "    input_context = f\"{INPUT_KEY}\\n{sample['input']}\" if sample[\"input\"] else None\n",
    "    response = f\"{RESPONSE_KEY}\"\n",
    "    end = f\"{END_KEY}\"\n",
    "\n",
    "    # Create a list of prompt template elements\n",
    "    parts = [part for part in [blurb, instruction, input_context,response,end] if part]\n",
    "\n",
    "    # Join prompt template elements into a single string to create the prompt template\n",
    "    formatted_prompt = \"\\n\\n\".join(parts)\n",
    "\n",
    "    # Store the formatted prompt template in a new key \"text\"\n",
    "    sample[\"text\"] = formatted_prompt\n",
    "\n",
    "    #sample[\"input\"] = formatted_prompt\n",
    "\n",
    "    return sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "7ca84d70",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'output': 'advertisement',\n",
       " 'instruction': 'Categorize the input into one of the 2 categories:\\n\\nadvertisement\\nnot advertisement\\n\\n',\n",
       " 'input': 'á‹¨áˆáˆµáˆ«á‰½! ðŸ‡ªðŸ‡¹ á‰ áˆ€áŒˆáˆ«á‰½áŠ• áˆáˆ­á‰µ áŠ¥áŠ•áŒ á‰€áˆ! áŠ¥áŠ•áŠ©áˆ«! á‹¨RingAround App á‹­áˆžáŠ­áˆ©áŠ• tikvah downloadringaroundapp á‰ á‹áŒ­ áˆ€áŒˆáˆ­ áˆˆáˆá‰µáŠ–áˆ© áŠ¢á‰µá‹®áŒµá‹«á‹á‹«áŠ•! áˆ³á‹«áˆ˜áˆáŒ£á‰½áˆ á‰ áˆ€áŒˆáˆ«á‰½áŠ• á‹¨á‰´áŠ­áŠ–áˆŽáŒ‚ áŒ á‰¢á‰£áŠ• á‰°á‹˜áŒ‹áŒ…á‰¶ á‹¨á‰€áˆ¨á‰ á‹áŠ• áŠ áˆˆáˆ áŠ á‰€á‰áŠ• á‹¨RingAround AppáŠ• á‰ áˆµáˆáŠ«á‰½áˆ áˆ‹á‹­ á‰¶áˆŽ á‰ áˆ˜áŒ«áŠ• á‹ˆá‹° áˆ€áŒˆáˆ­ á‰¤á‰µ á‹­á‹°á‹áˆ‰!'}"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "0beb08a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = dataset[10]\n",
    "\n",
    "prompt = create_prompt_formats_for_test(sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b42aaf9",
   "metadata": {},
   "source": [
    "## Inference using Instruction or Question Only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "0bfa766f",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_text = f\"Instruction: {prompt['text']}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "cbb9dba2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Instruction: Below is an instruction that describes a task. Write a response that appropriately completes the request.\\n\\n### Instruction:\\nCategorize the input into one of the 2 categories output only either advertisement or not advertisement. NOTHING ELSE.\\n\\n\\n\\nInput:\\ná‹¨áˆáˆµáˆ«á‰½! ðŸ‡ªðŸ‡¹ á‰ áˆ€áŒˆáˆ«á‰½áŠ• áˆáˆ­á‰µ áŠ¥áŠ•áŒ á‰€áˆ! áŠ¥áŠ•áŠ©áˆ«! á‹¨RingAround App á‹­áˆžáŠ­áˆ©áŠ• tikvah downloadringaroundapp á‰ á‹áŒ­ áˆ€áŒˆáˆ­ áˆˆáˆá‰µáŠ–áˆ© áŠ¢á‰µá‹®áŒµá‹«á‹á‹«áŠ•! áˆ³á‹«áˆ˜áˆáŒ£á‰½áˆ á‰ áˆ€áŒˆáˆ«á‰½áŠ• á‹¨á‰´áŠ­áŠ–áˆŽáŒ‚ áŒ á‰¢á‰£áŠ• á‰°á‹˜áŒ‹áŒ…á‰¶ á‹¨á‰€áˆ¨á‰ á‹áŠ• áŠ áˆˆáˆ áŠ á‰€á‰áŠ• á‹¨RingAround AppáŠ• á‰ áˆµáˆáŠ«á‰½áˆ áˆ‹á‹­ á‰¶áˆŽ á‰ áˆ˜áŒ«áŠ• á‹ˆá‹° áˆ€áŒˆáˆ­ á‰¤á‰µ á‹­á‹°á‹áˆ‰!\\n\\n### Response:'"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "25103de1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#text_i = 'ADVERTISMENT ðŸ‘ áˆ˜áˆáŠ«áˆ áŒˆáŠ“ ðŸ‘ ðŸ”œ ðŸ’¥áˆˆ3 á‰€áŠ“á‰µ á‰¥á‰» á‹¨áˆšá‰†á‹­ á‰³áˆ‹á‰…'\n",
    "text_i = 'áˆ›áˆµá‰³á‹ˆá‰‚á‹«! á‹µáˆ®áŒ‹ á‹¨áŠá‹šá‹®á‰´áˆ«á’ áˆ…áŠ­áˆáŠ“ áˆá‹© áŠ­áˆŠáŠ’áŠ­á¦ á‹˜áˆ˜áŠ‘ á‰£áˆáˆ«á‰¸á‹ á‹¨áˆ˜áŒ¨áˆ¨áˆ» á‹¨áˆ…áŠ­áˆáŠ“ á‰´áŠ­áŠ–áˆŽáŒ‚ á‰ áˆ˜á‰³áŒˆá‹áŠ“ á‰ áˆ™á‹«á‹ áˆµá”áˆ»áˆ‹á‹­á‹ á‰£á‹°áˆ¨áŒ‰ á‹¨áŠá‹á‹®á‰´áˆ«á’ áˆƒáŠªáˆžá‰½ á‰ áˆ…áƒáŠ“á‰µ áˆ‹á‹­ á‰ áˆšáŠ¨áˆ°á‰± á‹¨áŒ­áŠ•á‰…áˆ‹á‰µ áˆ‹á‹­ áŒ‰á‹³á‰¶á‰½ áŠ áˆá‹«áˆ áˆ›áŠ•áŠ›á‹áŠ•áˆ á‹¨áŠ¥áŠ•á‰…áˆµá‰ƒáˆ´ áŒ‰á‹µáˆˆá‰µáŠ“ á‹¨áŠ áŠ«áˆ áŠ áˆˆáˆ˜á‰³á‹˜á‹ á‰½áŒáˆ®á‰½ áá‰±áŠ• á‹¨áˆ†áŠ áˆ…áŠ­áˆáŠ“ á‰ áˆ›á‹µáˆ¨áŒ á‹ˆá‹° á‰°áˆŸáˆ‹ á‹¨áˆ°á‹áŠá‰µ áŠ¥áŠ•á‰…áˆµá‰ƒáˆ´ áŠ¥áŠ•áˆ˜áˆáˆ³áˆˆáŠ•á¢ áŠ áŠ  áˆšáŒáˆ¬áˆ¸áŠ• áŒ€áˆ­á‰£ áˆˆá‰ áˆˆáŒ  áˆ˜áˆ¨áŒƒ 0974 95 95 95 like us on FB @drogaphysiotherapy'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "6ec5633f",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_i = 'á‰³áˆ‹á‰… á‰…áŠ“áˆ½ áŠ¨á‹² áŠ¤áˆ áˆ² (DMC) áˆªáˆ áŠ¥áˆµá‰´á‰µ 5% áŠ¨áˆáˆˆá‹ á‹­áˆ˜á‹áŒˆá‰¡ á‹¨áˆšáˆáˆáŒ‰á‰µáŠ• á‰¤á‰µ á‹­áˆáˆ¨áŒ¡ áŠ¨ áˆµá‰²á‹µá‹® áŠ¥áˆµáŠ¨ á‰£áˆˆ 4 áˆ˜áŠá‰³ á‰¤á‰¶á‰½ á‰ á‰°áˆˆá‹«á‹¨ á‹¨áŠ«áˆ¬ áŠ£áˆ›áˆ«áŒ­ á‹˜áˆ˜áŠ“á‹Š áŠ á“áˆ­á‰µáˆ˜áŠ•á‰¶á‰½áŠ• á‰ áˆ˜áˆ¸áŒ¥ áˆ‹á‹­ áŠáŠ•'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "fc5900c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = f'Instruction: Below is an instruction that describes a task. Write a response that appropriately completes the request.\\n\\n### Instruction:\\nCategorize the input into one of the 2 categories output only either advertisement or not advertisement. NOTHING ELSE.\\n\\n\\n\\nInput:\\n{text_3}\\n\\n### Response:'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "d53c42c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = 'what medicine should I take if I got a flu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "1f77d235",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_3 = 'á‹°áŠ“ á‹‹áˆ‹á‰¹'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "36b6233c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize the input\n",
    "input_ids = tokenizer.encode(query, return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "# Measure inference time\n",
    "start_time = time.time()\n",
    "batch = tokenizer(query, return_tensors=\"pt\")\n",
    "batch = {k: v.to(\"cuda\") for k, v in batch.items()}\n",
    "#start = time.perf_counter()\n",
    "# Generate predictions\n",
    "#output = model.generate(input_ids, max_length=500, temperature=1.0, top_k=50, top_p=0.95, num_return_sequences=1)\n",
    "output = model.generate(**batch,  max_new_tokens=400,\n",
    "                do_sample=True,\n",
    "                top_p=1,\n",
    "                temperature=1,\n",
    "                use_cache=True,\n",
    "                top_k=1,\n",
    "                repetition_penalty=1,\n",
    "                length_penalty=1)\n",
    "generated_output = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "\n",
    "end_time = time.time()\n",
    "\n",
    "# Calculate and print the inference time\n",
    "inference_time = end_time - start_time\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "cd7f38f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Instruction: Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
      "\n",
      "### Instruction:\n",
      "Categorize the input into one of the 2 categories output only either advertisement or not advertisement. NOTHING ELSE.\n",
      "\n",
      "\n",
      "\n",
      "Input:\n",
      "á‹°áŠ“ á‹‹áˆ‹á‰¹\n",
      "\n",
      "### Response:\n",
      "advertisement\n",
      "\n",
      "### End###\n",
      "### Instruction:\n",
      "Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
      "\n",
      "### Instruction:\n",
      "Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
      "\n",
      "### End###\n",
      "### Instruction:\n",
      "Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
      "\n",
      "### End###\n",
      "### Instruction:\n",
      "Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
      "\n",
      "### End###\n",
      "### Instruction:\n",
      "Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
      "\n",
      "### End###\n",
      "### Instruction:\n",
      "Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
      "\n",
      "### End###\n",
      "### Instruction:\n",
      "Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
      "\n",
      "### End###\n",
      "### Instruction:\n",
      "Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
      "\n",
      "### End###\n",
      "### Instruction:\n",
      "Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
      "\n",
      "### End###\n",
      "### Instruction:\n",
      "Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
      "\n",
      "### End###\n",
      "### Instruction:\n",
      "Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
      "\n",
      "### End###\n",
      "### Instruction:\n",
      "Below\n"
     ]
    }
   ],
   "source": [
    "print(generated_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "id": "1890d24b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======\n",
      "Input:\n",
      "======\n",
      "Instruction: Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
      "\n",
      "### Instruction:\n",
      "áŒ¥á‰†áˆ›ðŸ“Œá‹¨á‹©áŠ’á‰¨áˆ­áˆ²á‰² á‰°áˆ›áˆªá‹Žá‰½ á‹¨áŒ¥áˆª á‰€áŠ“á‰½áˆáŠ• áŠ¨á‰°á‰‹áˆ›á‰½áˆ á‰µáŠ­áŠ­áˆˆáŠ› á‹¨áŒáˆµá‰¡áŠ­ áŒˆá… á‹ˆá‹­áˆ áŠ¨á‹µáˆ…áˆ¨áŒˆá… áˆ‹á‹­ áŠ«áˆ‹áŒˆáŠ›á‰½áˆ áŠ¥áŠ•á‹²áˆáˆ á‰ áˆ›áˆ…á‰°áˆ á‹¨á‰°á‹°áŒˆá‰ á‹¨áˆ›áˆµá‰³á‹ˆá‰‚á‹« á…áˆáŽá‰½áŠ• áŠ«áˆ‹á‹«á‰½áˆ áˆá‰³áˆáŠ‘ áŠ á‹­áŒˆá‰£áˆá¢ á‰ áˆ˜áˆ†áŠ‘áŠ•áˆ á‹¨á‰°áŒ áˆ«á‰½áˆá‰ á‰µáŠ• á‰€áŠ• áˆˆáˆ›á‹ˆá‰… á‰€áŒ¥á‰³ á‰ á‹©áŠ’á‰¨áˆ­áˆ²á‰³á‹«á‰½áˆ á‹¨áŒáˆµá‰¡áŠ­ áŒˆá… áŠ¥á‹¨áŒˆá‰£á‰½áˆ áŠ áˆ¨áŒ‹áŒáŒ¡á¢ áŠ¥áŠ•á‹²áˆáˆ á‹¨á‰°áˆ›áˆª áˆ…á‰¥áˆ¨á‰µ á‰°á‹ˆáŠ«á‹­ áˆµáˆáŠ­ áŠ¥áŠ“ á‹¨áˆ¬áŒ…áˆµá‰µáˆ«áˆ­ á‰¢áˆ® áˆµáˆáŠ­ á‰áŒ¥áˆ®á‰½áŠ• áˆ˜á‹«á‹ á‰£áˆ…áˆ‹á‰½áˆ áŠ á‹µáˆ­áŒ‰á¢ @tsegabwolde @tikvahethiopia\n",
      "\n",
      "======================\n",
      "Generated Output:\n",
      "======================\n",
      "Instruction: Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
      "\n",
      "### Instruction:\n",
      "áŒ¥á‰†áˆ›ðŸ“Œá‹¨á‹©áŠ’á‰¨áˆ­áˆ²á‰² á‰°áˆ›áˆªá‹Žá‰½ á‹¨áŒ¥áˆª á‰€áŠ“á‰½áˆáŠ• áŠ¨á‰°á‰‹áˆ›á‰½áˆ á‰µáŠ­áŠ­áˆˆáŠ› á‹¨áŒáˆµá‰¡áŠ­ áŒˆá… á‹ˆá‹­áˆ áŠ¨á‹µáˆ…áˆ¨áŒˆá… áˆ‹á‹­ áŠ«áˆ‹áŒˆáŠ›á‰½áˆ áŠ¥áŠ•á‹²áˆáˆ á‰ áˆ›áˆ…á‰°áˆ á‹¨á‰°á‹°áŒˆá‰ á‹¨áˆ›áˆµá‰³á‹ˆá‰‚á‹« á…áˆáŽá‰½áŠ• áŠ«áˆ‹á‹«á‰½áˆ áˆá‰³áˆáŠ‘ áŠ á‹­áŒˆá‰£áˆá¢ á‰ áˆ˜áˆ†áŠ‘áŠ•áˆ á‹¨á‰°áŒ áˆ«á‰½áˆá‰ á‰µáŠ• á‰€áŠ• áˆˆáˆ›á‹ˆá‰… á‰€áŒ¥á‰³ á‰ á‹©áŠ’á‰¨áˆ­áˆ²á‰³á‹«á‰½áˆ á‹¨áŒáˆµá‰¡áŠ­ áŒˆá… áŠ¥á‹¨áŒˆá‰£á‰½áˆ áŠ áˆ¨áŒ‹áŒáŒ¡á¢ áŠ¥áŠ•á‹²áˆáˆ á‹¨á‰°áˆ›áˆª áˆ…á‰¥áˆ¨á‰µ á‰°á‹ˆáŠ«á‹­ áˆµáˆáŠ­ áŠ¥áŠ“ á‹¨áˆ¬áŒ…áˆµá‰µáˆ«áˆ­ á‰¢áˆ® áˆµáˆáŠ­ á‰áŒ¥áˆ®á‰½áŠ• áˆ˜á‹«á‹ á‰£áˆ…áˆ‹á‰½áˆ áŠ á‹µáˆ­áŒ‰á¢ @tsegabwolde @tikvahethiopia\n",
      "\n",
      "### Response:\n",
      "á‰°áˆ›áˆªá‹Žá‰½áŠ• áˆˆáŒ¥áˆª á‰€áŠ“á‰½áˆáŠ• áŠ¨á‰°á‰‹áˆ›á‰½áˆ á‰µáŠ­áŠ­áˆˆáŠ› á‹¨áŒáˆµá‰¡áŠ­ áŒˆá… á‹ˆá‹­áˆ áŠ¨á‹µáˆ…áˆ¨áŒˆá… áˆ‹á‹­ áŠ«áˆ‹á‰€á‹˜á‰ áŠ¥áŠ•á‹²áˆáˆ á‰ áˆ›áˆ…á‰°áˆ á‹¨á‰°á‹°áŒˆá‰ á‹¨áˆ›áˆµá‰³á‹ˆá‰‚á‹« á…áˆáŽá‰½áŠ• áŠ«áˆ‹á‹«á‰½áˆ áˆá‰³áˆáŠ‘áŠ• áŠ á‹­á‹°áˆˆáˆá¢ á‰ á‹šáˆ…áˆáˆ á‰°áŒ áˆ­áŒ£á‰½áˆáŠ• á‹¨áˆ˜á‹ˆáŒ£á‰½áˆáŠ• á‰€áŠ• áˆˆáˆ›á‹ˆá‰… á‰€áŒ¥á‰³ á‰ á‹©áŠ’á‰¨áˆ­áˆ²á‰³á‹«á‰½áˆ á‹¨áŒáˆµá‰¡áŠ­ áŒˆá… á‹áˆµáŒ¥ áŒˆá‰¥á‰°á‹³á‰½áˆá¢ áŠ¥áŠ•á‹²áˆáˆ á‹¨á‰°áˆ›áˆª áˆ…á‰¥áˆ¨á‰µ á‰°á‹ˆáŠ«á‹­ áˆµáˆáŠ­ áŠ¥áŠ“ áˆ¬áŒ…áˆµá‰µáˆ«áˆ­ á‰¢áˆ® áˆµáˆáŠ­ á‰áŒ¥áˆ®á‰½áŠ• áˆ˜á‹«á‹ á‰£áˆ…áˆ‹á‰½áˆ áŠ á‹µáˆ­áŒ‰á¢ @tikvahethiopia\n",
      "\n",
      "### End of Instruction\n",
      "\n",
      "### End of Response\n",
      "\n",
      "### End of CháŠ“áˆˆ\n",
      "\n",
      "### End of Tikvahethiopia\n",
      "\n",
      "### End of Tikvahethiopia\n",
      "\n",
      "### End of Tikvahethiopia\n",
      "### End of Tikvahethiopia\n",
      "### End of Tikvahethiopia\n",
      "### End of Tikvahethiopia\n",
      "### End of Tikvahethiopia\n",
      "### End of Tikvahethiopia\n",
      "### End of Tikvahethiopia\n",
      "### End of Tikvahethiopia\n",
      "### End of Tikvahethiopia\n",
      "### End of Tikvahethiopia\n",
      "### End of Tikvahethiopia\n",
      "### End of Tikvahethiopia\n",
      "### End of Tikvahethiopia\n",
      "### End of Tikvahethiopia\n",
      "### End of Tikvahethiopia\n",
      "### End of Tikvahethiopia\n",
      "### End of Tikvahethiopia\n",
      "### End of Tikvahethiopia\n",
      "### End of Tikvahethiopia\n",
      "###\n",
      "\n",
      "=========================================\n",
      "Inference Time:103.68379998207092 seconds\n",
      "==========================================\n"
     ]
    }
   ],
   "source": [
    "# Print the formatted input\n",
    "print(f\"======\")\n",
    "print(f\"Input:\\n======\\n{input_text}\\n\")\n",
    "print(f\"======================\")\n",
    "print(f\"Generated Output:\\n======================\\n{generated_output}\\n\")\n",
    "print(f\"=========================================\")\n",
    "print(f\"Inference Time:{inference_time} seconds\\n==========================================\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ebf475f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
