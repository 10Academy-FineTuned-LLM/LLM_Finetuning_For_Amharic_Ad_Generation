{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "7918b9b1-82cd-495c-aca1-bf81a1f33b19",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4b9f66b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: datasets in /home/eyaya_eneyew/week_7/LLM_Finetuning_For_Amharic_Ad_Generation/venv/lib/python3.11/site-packages (2.16.1)\n",
      "Requirement already satisfied: transformers[sentencepiece] in /home/eyaya_eneyew/week_7/LLM_Finetuning_For_Amharic_Ad_Generation/venv/lib/python3.11/site-packages (4.31.0)\n",
      "Requirement already satisfied: filelock in /home/eyaya_eneyew/week_7/LLM_Finetuning_For_Amharic_Ad_Generation/venv/lib/python3.11/site-packages (from datasets) (3.13.1)\n",
      "Requirement already satisfied: numpy>=1.17 in /home/eyaya_eneyew/week_7/LLM_Finetuning_For_Amharic_Ad_Generation/venv/lib/python3.11/site-packages (from datasets) (1.26.3)\n",
      "Requirement already satisfied: pyarrow>=8.0.0 in /home/eyaya_eneyew/week_7/LLM_Finetuning_For_Amharic_Ad_Generation/venv/lib/python3.11/site-packages (from datasets) (15.0.0)\n",
      "Requirement already satisfied: pyarrow-hotfix in /home/eyaya_eneyew/week_7/LLM_Finetuning_For_Amharic_Ad_Generation/venv/lib/python3.11/site-packages (from datasets) (0.6)\n",
      "Requirement already satisfied: dill<0.3.8,>=0.3.0 in /home/eyaya_eneyew/week_7/LLM_Finetuning_For_Amharic_Ad_Generation/venv/lib/python3.11/site-packages (from datasets) (0.3.7)\n",
      "Requirement already satisfied: pandas in /home/eyaya_eneyew/week_7/LLM_Finetuning_For_Amharic_Ad_Generation/venv/lib/python3.11/site-packages (from datasets) (2.2.0)\n",
      "Requirement already satisfied: requests>=2.19.0 in /home/eyaya_eneyew/week_7/LLM_Finetuning_For_Amharic_Ad_Generation/venv/lib/python3.11/site-packages (from datasets) (2.31.0)\n",
      "Requirement already satisfied: tqdm>=4.62.1 in /home/eyaya_eneyew/week_7/LLM_Finetuning_For_Amharic_Ad_Generation/venv/lib/python3.11/site-packages (from datasets) (4.66.1)\n",
      "Requirement already satisfied: xxhash in /home/eyaya_eneyew/week_7/LLM_Finetuning_For_Amharic_Ad_Generation/venv/lib/python3.11/site-packages (from datasets) (3.4.1)\n",
      "Requirement already satisfied: multiprocess in /home/eyaya_eneyew/week_7/LLM_Finetuning_For_Amharic_Ad_Generation/venv/lib/python3.11/site-packages (from datasets) (0.70.15)\n",
      "Requirement already satisfied: fsspec<=2023.10.0,>=2023.1.0 in /home/eyaya_eneyew/week_7/LLM_Finetuning_For_Amharic_Ad_Generation/venv/lib/python3.11/site-packages (from fsspec[http]<=2023.10.0,>=2023.1.0->datasets) (2023.10.0)\n",
      "Requirement already satisfied: aiohttp in /home/eyaya_eneyew/week_7/LLM_Finetuning_For_Amharic_Ad_Generation/venv/lib/python3.11/site-packages (from datasets) (3.9.2)\n",
      "Requirement already satisfied: huggingface-hub>=0.19.4 in /home/eyaya_eneyew/week_7/LLM_Finetuning_For_Amharic_Ad_Generation/venv/lib/python3.11/site-packages (from datasets) (0.20.3)\n",
      "Requirement already satisfied: packaging in /home/eyaya_eneyew/week_7/LLM_Finetuning_For_Amharic_Ad_Generation/venv/lib/python3.11/site-packages (from datasets) (23.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home/eyaya_eneyew/week_7/LLM_Finetuning_For_Amharic_Ad_Generation/venv/lib/python3.11/site-packages (from datasets) (6.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /home/eyaya_eneyew/week_7/LLM_Finetuning_For_Amharic_Ad_Generation/venv/lib/python3.11/site-packages (from transformers[sentencepiece]) (2023.12.25)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /home/eyaya_eneyew/week_7/LLM_Finetuning_For_Amharic_Ad_Generation/venv/lib/python3.11/site-packages (from transformers[sentencepiece]) (0.13.3)\n",
      "Requirement already satisfied: safetensors>=0.3.1 in /home/eyaya_eneyew/week_7/LLM_Finetuning_For_Amharic_Ad_Generation/venv/lib/python3.11/site-packages (from transformers[sentencepiece]) (0.4.2)\n",
      "Requirement already satisfied: sentencepiece!=0.1.92,>=0.1.91 in /home/eyaya_eneyew/week_7/LLM_Finetuning_For_Amharic_Ad_Generation/venv/lib/python3.11/site-packages (from transformers[sentencepiece]) (0.1.99)\n",
      "Requirement already satisfied: protobuf in /home/eyaya_eneyew/week_7/LLM_Finetuning_For_Amharic_Ad_Generation/venv/lib/python3.11/site-packages (from transformers[sentencepiece]) (4.25.2)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /home/eyaya_eneyew/week_7/LLM_Finetuning_For_Amharic_Ad_Generation/venv/lib/python3.11/site-packages (from aiohttp->datasets) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /home/eyaya_eneyew/week_7/LLM_Finetuning_For_Amharic_Ad_Generation/venv/lib/python3.11/site-packages (from aiohttp->datasets) (23.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /home/eyaya_eneyew/week_7/LLM_Finetuning_For_Amharic_Ad_Generation/venv/lib/python3.11/site-packages (from aiohttp->datasets) (1.4.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /home/eyaya_eneyew/week_7/LLM_Finetuning_For_Amharic_Ad_Generation/venv/lib/python3.11/site-packages (from aiohttp->datasets) (6.0.4)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /home/eyaya_eneyew/week_7/LLM_Finetuning_For_Amharic_Ad_Generation/venv/lib/python3.11/site-packages (from aiohttp->datasets) (1.9.4)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/eyaya_eneyew/week_7/LLM_Finetuning_For_Amharic_Ad_Generation/venv/lib/python3.11/site-packages (from huggingface-hub>=0.19.4->datasets) (4.9.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/eyaya_eneyew/week_7/LLM_Finetuning_For_Amharic_Ad_Generation/venv/lib/python3.11/site-packages (from requests>=2.19.0->datasets) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/eyaya_eneyew/week_7/LLM_Finetuning_For_Amharic_Ad_Generation/venv/lib/python3.11/site-packages (from requests>=2.19.0->datasets) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/eyaya_eneyew/week_7/LLM_Finetuning_For_Amharic_Ad_Generation/venv/lib/python3.11/site-packages (from requests>=2.19.0->datasets) (2.1.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/eyaya_eneyew/week_7/LLM_Finetuning_For_Amharic_Ad_Generation/venv/lib/python3.11/site-packages (from requests>=2.19.0->datasets) (2023.11.17)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /home/eyaya_eneyew/week_7/LLM_Finetuning_For_Amharic_Ad_Generation/venv/lib/python3.11/site-packages (from pandas->datasets) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /home/eyaya_eneyew/week_7/LLM_Finetuning_For_Amharic_Ad_Generation/venv/lib/python3.11/site-packages (from pandas->datasets) (2023.3.post1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /home/eyaya_eneyew/week_7/LLM_Finetuning_For_Amharic_Ad_Generation/venv/lib/python3.11/site-packages (from pandas->datasets) (2023.4)\n",
      "Requirement already satisfied: six>=1.5 in /home/eyaya_eneyew/week_7/LLM_Finetuning_For_Amharic_Ad_Generation/venv/lib/python3.11/site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n",
      "Collecting git+https://github.com/huggingface/peft\n",
      "  Cloning https://github.com/huggingface/peft to /tmp/pip-req-build-xle1d4dy\n",
      "  Running command git clone --filter=blob:none --quiet https://github.com/huggingface/peft /tmp/pip-req-build-xle1d4dy\n",
      "  Resolved https://github.com/huggingface/peft to commit ce925d844a0bc54b951fcb69229dfe740c9afa45\n",
      "  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /home/eyaya_eneyew/week_7/LLM_Finetuning_For_Amharic_Ad_Generation/venv/lib/python3.11/site-packages (from peft==0.8.2) (1.26.3)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/eyaya_eneyew/week_7/LLM_Finetuning_For_Amharic_Ad_Generation/venv/lib/python3.11/site-packages (from peft==0.8.2) (23.2)\n",
      "Requirement already satisfied: psutil in /home/eyaya_eneyew/week_7/LLM_Finetuning_For_Amharic_Ad_Generation/venv/lib/python3.11/site-packages (from peft==0.8.2) (5.9.0)\n",
      "Requirement already satisfied: pyyaml in /home/eyaya_eneyew/week_7/LLM_Finetuning_For_Amharic_Ad_Generation/venv/lib/python3.11/site-packages (from peft==0.8.2) (6.0.1)\n",
      "Requirement already satisfied: torch>=1.13.0 in /home/eyaya_eneyew/week_7/LLM_Finetuning_For_Amharic_Ad_Generation/venv/lib/python3.11/site-packages (from peft==0.8.2) (2.1.2)\n",
      "Requirement already satisfied: transformers in /home/eyaya_eneyew/week_7/LLM_Finetuning_For_Amharic_Ad_Generation/venv/lib/python3.11/site-packages (from peft==0.8.2) (4.31.0)\n",
      "Requirement already satisfied: tqdm in /home/eyaya_eneyew/week_7/LLM_Finetuning_For_Amharic_Ad_Generation/venv/lib/python3.11/site-packages (from peft==0.8.2) (4.66.1)\n",
      "Requirement already satisfied: accelerate>=0.21.0 in /home/eyaya_eneyew/week_7/LLM_Finetuning_For_Amharic_Ad_Generation/venv/lib/python3.11/site-packages (from peft==0.8.2) (0.21.0)\n",
      "Requirement already satisfied: safetensors in /home/eyaya_eneyew/week_7/LLM_Finetuning_For_Amharic_Ad_Generation/venv/lib/python3.11/site-packages (from peft==0.8.2) (0.4.2)\n",
      "Requirement already satisfied: huggingface-hub>=0.17.0 in /home/eyaya_eneyew/week_7/LLM_Finetuning_For_Amharic_Ad_Generation/venv/lib/python3.11/site-packages (from peft==0.8.2) (0.20.3)\n",
      "Requirement already satisfied: filelock in /home/eyaya_eneyew/week_7/LLM_Finetuning_For_Amharic_Ad_Generation/venv/lib/python3.11/site-packages (from huggingface-hub>=0.17.0->peft==0.8.2) (3.13.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /home/eyaya_eneyew/week_7/LLM_Finetuning_For_Amharic_Ad_Generation/venv/lib/python3.11/site-packages (from huggingface-hub>=0.17.0->peft==0.8.2) (2023.10.0)\n",
      "Requirement already satisfied: requests in /home/eyaya_eneyew/week_7/LLM_Finetuning_For_Amharic_Ad_Generation/venv/lib/python3.11/site-packages (from huggingface-hub>=0.17.0->peft==0.8.2) (2.31.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/eyaya_eneyew/week_7/LLM_Finetuning_For_Amharic_Ad_Generation/venv/lib/python3.11/site-packages (from huggingface-hub>=0.17.0->peft==0.8.2) (4.9.0)\n",
      "Requirement already satisfied: sympy in /home/eyaya_eneyew/week_7/LLM_Finetuning_For_Amharic_Ad_Generation/venv/lib/python3.11/site-packages (from torch>=1.13.0->peft==0.8.2) (1.12)\n",
      "Requirement already satisfied: networkx in /home/eyaya_eneyew/week_7/LLM_Finetuning_For_Amharic_Ad_Generation/venv/lib/python3.11/site-packages (from torch>=1.13.0->peft==0.8.2) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in /home/eyaya_eneyew/week_7/LLM_Finetuning_For_Amharic_Ad_Generation/venv/lib/python3.11/site-packages (from torch>=1.13.0->peft==0.8.2) (3.1.3)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /home/eyaya_eneyew/week_7/LLM_Finetuning_For_Amharic_Ad_Generation/venv/lib/python3.11/site-packages (from torch>=1.13.0->peft==0.8.2) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /home/eyaya_eneyew/week_7/LLM_Finetuning_For_Amharic_Ad_Generation/venv/lib/python3.11/site-packages (from torch>=1.13.0->peft==0.8.2) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /home/eyaya_eneyew/week_7/LLM_Finetuning_For_Amharic_Ad_Generation/venv/lib/python3.11/site-packages (from torch>=1.13.0->peft==0.8.2) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /home/eyaya_eneyew/week_7/LLM_Finetuning_For_Amharic_Ad_Generation/venv/lib/python3.11/site-packages (from torch>=1.13.0->peft==0.8.2) (8.9.2.26)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /home/eyaya_eneyew/week_7/LLM_Finetuning_For_Amharic_Ad_Generation/venv/lib/python3.11/site-packages (from torch>=1.13.0->peft==0.8.2) (12.1.3.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /home/eyaya_eneyew/week_7/LLM_Finetuning_For_Amharic_Ad_Generation/venv/lib/python3.11/site-packages (from torch>=1.13.0->peft==0.8.2) (11.0.2.54)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /home/eyaya_eneyew/week_7/LLM_Finetuning_For_Amharic_Ad_Generation/venv/lib/python3.11/site-packages (from torch>=1.13.0->peft==0.8.2) (10.3.2.106)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /home/eyaya_eneyew/week_7/LLM_Finetuning_For_Amharic_Ad_Generation/venv/lib/python3.11/site-packages (from torch>=1.13.0->peft==0.8.2) (11.4.5.107)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /home/eyaya_eneyew/week_7/LLM_Finetuning_For_Amharic_Ad_Generation/venv/lib/python3.11/site-packages (from torch>=1.13.0->peft==0.8.2) (12.1.0.106)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.18.1 in /home/eyaya_eneyew/week_7/LLM_Finetuning_For_Amharic_Ad_Generation/venv/lib/python3.11/site-packages (from torch>=1.13.0->peft==0.8.2) (2.18.1)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /home/eyaya_eneyew/week_7/LLM_Finetuning_For_Amharic_Ad_Generation/venv/lib/python3.11/site-packages (from torch>=1.13.0->peft==0.8.2) (12.1.105)\n",
      "Requirement already satisfied: triton==2.1.0 in /home/eyaya_eneyew/week_7/LLM_Finetuning_For_Amharic_Ad_Generation/venv/lib/python3.11/site-packages (from torch>=1.13.0->peft==0.8.2) (2.1.0)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12 in /home/eyaya_eneyew/week_7/LLM_Finetuning_For_Amharic_Ad_Generation/venv/lib/python3.11/site-packages (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.13.0->peft==0.8.2) (12.3.101)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /home/eyaya_eneyew/week_7/LLM_Finetuning_For_Amharic_Ad_Generation/venv/lib/python3.11/site-packages (from transformers->peft==0.8.2) (2023.12.25)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /home/eyaya_eneyew/week_7/LLM_Finetuning_For_Amharic_Ad_Generation/venv/lib/python3.11/site-packages (from transformers->peft==0.8.2) (0.13.3)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/eyaya_eneyew/week_7/LLM_Finetuning_For_Amharic_Ad_Generation/venv/lib/python3.11/site-packages (from jinja2->torch>=1.13.0->peft==0.8.2) (2.1.4)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/eyaya_eneyew/week_7/LLM_Finetuning_For_Amharic_Ad_Generation/venv/lib/python3.11/site-packages (from requests->huggingface-hub>=0.17.0->peft==0.8.2) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/eyaya_eneyew/week_7/LLM_Finetuning_For_Amharic_Ad_Generation/venv/lib/python3.11/site-packages (from requests->huggingface-hub>=0.17.0->peft==0.8.2) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/eyaya_eneyew/week_7/LLM_Finetuning_For_Amharic_Ad_Generation/venv/lib/python3.11/site-packages (from requests->huggingface-hub>=0.17.0->peft==0.8.2) (2.1.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/eyaya_eneyew/week_7/LLM_Finetuning_For_Amharic_Ad_Generation/venv/lib/python3.11/site-packages (from requests->huggingface-hub>=0.17.0->peft==0.8.2) (2023.11.17)\n",
      "Requirement already satisfied: mpmath>=0.19 in /home/eyaya_eneyew/week_7/LLM_Finetuning_For_Amharic_Ad_Generation/venv/lib/python3.11/site-packages (from sympy->torch>=1.13.0->peft==0.8.2) (1.3.0)\n",
      "Building wheels for collected packages: peft\n",
      "  Building wheel for peft (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for peft: filename=peft-0.8.2-py3-none-any.whl size=183371 sha256=cbaa54731222ebb1784b5853bff5881090f95e7e6759eaa2f4a60d84a7a66f7f\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-7pe76gmi/wheels/e1/0a/cc/243fa4389de86b6a8c8a6ac6511d2227d3e10934d3b19e5f5e\n",
      "Successfully built peft\n",
      "Installing collected packages: peft\n",
      "  Attempting uninstall: peft\n",
      "    Found existing installation: peft 0.4.0\n",
      "    Uninstalling peft-0.4.0:\n",
      "      Successfully uninstalled peft-0.4.0\n",
      "Successfully installed peft-0.8.2\n",
      "Collecting git+https://github.com/huggingface/accelerate\n",
      "  Cloning https://github.com/huggingface/accelerate to /tmp/pip-req-build-q408r1e0\n",
      "  Running command git clone --filter=blob:none --quiet https://github.com/huggingface/accelerate /tmp/pip-req-build-q408r1e0\n",
      "  Resolved https://github.com/huggingface/accelerate to commit 68f54720dc9e4e887f8b7e3177798d745913e8ad\n",
      "  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /home/eyaya_eneyew/week_7/LLM_Finetuning_For_Amharic_Ad_Generation/venv/lib/python3.11/site-packages (from accelerate==0.27.0.dev0) (1.26.3)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/eyaya_eneyew/week_7/LLM_Finetuning_For_Amharic_Ad_Generation/venv/lib/python3.11/site-packages (from accelerate==0.27.0.dev0) (23.2)\n",
      "Requirement already satisfied: psutil in /home/eyaya_eneyew/week_7/LLM_Finetuning_For_Amharic_Ad_Generation/venv/lib/python3.11/site-packages (from accelerate==0.27.0.dev0) (5.9.0)\n",
      "Requirement already satisfied: pyyaml in /home/eyaya_eneyew/week_7/LLM_Finetuning_For_Amharic_Ad_Generation/venv/lib/python3.11/site-packages (from accelerate==0.27.0.dev0) (6.0.1)\n",
      "Requirement already satisfied: torch<2.2.0,>=1.10.0 in /home/eyaya_eneyew/week_7/LLM_Finetuning_For_Amharic_Ad_Generation/venv/lib/python3.11/site-packages (from accelerate==0.27.0.dev0) (2.1.2)\n",
      "Requirement already satisfied: huggingface-hub in /home/eyaya_eneyew/week_7/LLM_Finetuning_For_Amharic_Ad_Generation/venv/lib/python3.11/site-packages (from accelerate==0.27.0.dev0) (0.20.3)\n",
      "Requirement already satisfied: safetensors>=0.3.1 in /home/eyaya_eneyew/week_7/LLM_Finetuning_For_Amharic_Ad_Generation/venv/lib/python3.11/site-packages (from accelerate==0.27.0.dev0) (0.4.2)\n",
      "Requirement already satisfied: filelock in /home/eyaya_eneyew/week_7/LLM_Finetuning_For_Amharic_Ad_Generation/venv/lib/python3.11/site-packages (from torch<2.2.0,>=1.10.0->accelerate==0.27.0.dev0) (3.13.1)\n",
      "Requirement already satisfied: typing-extensions in /home/eyaya_eneyew/week_7/LLM_Finetuning_For_Amharic_Ad_Generation/venv/lib/python3.11/site-packages (from torch<2.2.0,>=1.10.0->accelerate==0.27.0.dev0) (4.9.0)\n",
      "Requirement already satisfied: sympy in /home/eyaya_eneyew/week_7/LLM_Finetuning_For_Amharic_Ad_Generation/venv/lib/python3.11/site-packages (from torch<2.2.0,>=1.10.0->accelerate==0.27.0.dev0) (1.12)\n",
      "Requirement already satisfied: networkx in /home/eyaya_eneyew/week_7/LLM_Finetuning_For_Amharic_Ad_Generation/venv/lib/python3.11/site-packages (from torch<2.2.0,>=1.10.0->accelerate==0.27.0.dev0) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in /home/eyaya_eneyew/week_7/LLM_Finetuning_For_Amharic_Ad_Generation/venv/lib/python3.11/site-packages (from torch<2.2.0,>=1.10.0->accelerate==0.27.0.dev0) (3.1.3)\n",
      "Requirement already satisfied: fsspec in /home/eyaya_eneyew/week_7/LLM_Finetuning_For_Amharic_Ad_Generation/venv/lib/python3.11/site-packages (from torch<2.2.0,>=1.10.0->accelerate==0.27.0.dev0) (2023.10.0)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /home/eyaya_eneyew/week_7/LLM_Finetuning_For_Amharic_Ad_Generation/venv/lib/python3.11/site-packages (from torch<2.2.0,>=1.10.0->accelerate==0.27.0.dev0) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /home/eyaya_eneyew/week_7/LLM_Finetuning_For_Amharic_Ad_Generation/venv/lib/python3.11/site-packages (from torch<2.2.0,>=1.10.0->accelerate==0.27.0.dev0) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /home/eyaya_eneyew/week_7/LLM_Finetuning_For_Amharic_Ad_Generation/venv/lib/python3.11/site-packages (from torch<2.2.0,>=1.10.0->accelerate==0.27.0.dev0) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /home/eyaya_eneyew/week_7/LLM_Finetuning_For_Amharic_Ad_Generation/venv/lib/python3.11/site-packages (from torch<2.2.0,>=1.10.0->accelerate==0.27.0.dev0) (8.9.2.26)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /home/eyaya_eneyew/week_7/LLM_Finetuning_For_Amharic_Ad_Generation/venv/lib/python3.11/site-packages (from torch<2.2.0,>=1.10.0->accelerate==0.27.0.dev0) (12.1.3.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /home/eyaya_eneyew/week_7/LLM_Finetuning_For_Amharic_Ad_Generation/venv/lib/python3.11/site-packages (from torch<2.2.0,>=1.10.0->accelerate==0.27.0.dev0) (11.0.2.54)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /home/eyaya_eneyew/week_7/LLM_Finetuning_For_Amharic_Ad_Generation/venv/lib/python3.11/site-packages (from torch<2.2.0,>=1.10.0->accelerate==0.27.0.dev0) (10.3.2.106)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /home/eyaya_eneyew/week_7/LLM_Finetuning_For_Amharic_Ad_Generation/venv/lib/python3.11/site-packages (from torch<2.2.0,>=1.10.0->accelerate==0.27.0.dev0) (11.4.5.107)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /home/eyaya_eneyew/week_7/LLM_Finetuning_For_Amharic_Ad_Generation/venv/lib/python3.11/site-packages (from torch<2.2.0,>=1.10.0->accelerate==0.27.0.dev0) (12.1.0.106)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.18.1 in /home/eyaya_eneyew/week_7/LLM_Finetuning_For_Amharic_Ad_Generation/venv/lib/python3.11/site-packages (from torch<2.2.0,>=1.10.0->accelerate==0.27.0.dev0) (2.18.1)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /home/eyaya_eneyew/week_7/LLM_Finetuning_For_Amharic_Ad_Generation/venv/lib/python3.11/site-packages (from torch<2.2.0,>=1.10.0->accelerate==0.27.0.dev0) (12.1.105)\n",
      "Requirement already satisfied: triton==2.1.0 in /home/eyaya_eneyew/week_7/LLM_Finetuning_For_Amharic_Ad_Generation/venv/lib/python3.11/site-packages (from torch<2.2.0,>=1.10.0->accelerate==0.27.0.dev0) (2.1.0)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12 in /home/eyaya_eneyew/week_7/LLM_Finetuning_For_Amharic_Ad_Generation/venv/lib/python3.11/site-packages (from nvidia-cusolver-cu12==11.4.5.107->torch<2.2.0,>=1.10.0->accelerate==0.27.0.dev0) (12.3.101)\n",
      "Requirement already satisfied: requests in /home/eyaya_eneyew/week_7/LLM_Finetuning_For_Amharic_Ad_Generation/venv/lib/python3.11/site-packages (from huggingface-hub->accelerate==0.27.0.dev0) (2.31.0)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in /home/eyaya_eneyew/week_7/LLM_Finetuning_For_Amharic_Ad_Generation/venv/lib/python3.11/site-packages (from huggingface-hub->accelerate==0.27.0.dev0) (4.66.1)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/eyaya_eneyew/week_7/LLM_Finetuning_For_Amharic_Ad_Generation/venv/lib/python3.11/site-packages (from jinja2->torch<2.2.0,>=1.10.0->accelerate==0.27.0.dev0) (2.1.4)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/eyaya_eneyew/week_7/LLM_Finetuning_For_Amharic_Ad_Generation/venv/lib/python3.11/site-packages (from requests->huggingface-hub->accelerate==0.27.0.dev0) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/eyaya_eneyew/week_7/LLM_Finetuning_For_Amharic_Ad_Generation/venv/lib/python3.11/site-packages (from requests->huggingface-hub->accelerate==0.27.0.dev0) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/eyaya_eneyew/week_7/LLM_Finetuning_For_Amharic_Ad_Generation/venv/lib/python3.11/site-packages (from requests->huggingface-hub->accelerate==0.27.0.dev0) (2.1.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/eyaya_eneyew/week_7/LLM_Finetuning_For_Amharic_Ad_Generation/venv/lib/python3.11/site-packages (from requests->huggingface-hub->accelerate==0.27.0.dev0) (2023.11.17)\n",
      "Requirement already satisfied: mpmath>=0.19 in /home/eyaya_eneyew/week_7/LLM_Finetuning_For_Amharic_Ad_Generation/venv/lib/python3.11/site-packages (from sympy->torch<2.2.0,>=1.10.0->accelerate==0.27.0.dev0) (1.3.0)\n",
      "Building wheels for collected packages: accelerate\n",
      "  Building wheel for accelerate (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for accelerate: filename=accelerate-0.27.0.dev0-py3-none-any.whl size=274019 sha256=93692532dddc909f0721b9be466e33c53de8b84aa26bb0bdc0d31eb28912aea9\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-ohhgtdxb/wheels/18/af/f7/facfc4ea8d2484e23fc8489825221fe5826625fad79301dd99\n",
      "Successfully built accelerate\n",
      "Installing collected packages: accelerate\n",
      "  Attempting uninstall: accelerate\n",
      "    Found existing installation: accelerate 0.21.0\n",
      "    Uninstalling accelerate-0.21.0:\n",
      "      Successfully uninstalled accelerate-0.21.0\n",
      "Successfully installed accelerate-0.27.0.dev0\n",
      "Looking in indexes: https://download.pytorch.org/whl/cu118\n",
      "Requirement already satisfied: torch in /home/eyaya_eneyew/week_7/LLM_Finetuning_For_Amharic_Ad_Generation/venv/lib/python3.11/site-packages (2.1.2)\n",
      "Requirement already satisfied: torchvision in /home/eyaya_eneyew/week_7/LLM_Finetuning_For_Amharic_Ad_Generation/venv/lib/python3.11/site-packages (0.16.2)\n",
      "Collecting torchaudio\n",
      "  Using cached https://download.pytorch.org/whl/cu118/torchaudio-2.2.0%2Bcu118-cp311-cp311-linux_x86_64.whl (3.3 MB)\n",
      "Requirement already satisfied: filelock in /home/eyaya_eneyew/week_7/LLM_Finetuning_For_Amharic_Ad_Generation/venv/lib/python3.11/site-packages (from torch) (3.13.1)\n",
      "Requirement already satisfied: typing-extensions in /home/eyaya_eneyew/week_7/LLM_Finetuning_For_Amharic_Ad_Generation/venv/lib/python3.11/site-packages (from torch) (4.9.0)\n",
      "Requirement already satisfied: sympy in /home/eyaya_eneyew/week_7/LLM_Finetuning_For_Amharic_Ad_Generation/venv/lib/python3.11/site-packages (from torch) (1.12)\n",
      "Requirement already satisfied: networkx in /home/eyaya_eneyew/week_7/LLM_Finetuning_For_Amharic_Ad_Generation/venv/lib/python3.11/site-packages (from torch) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in /home/eyaya_eneyew/week_7/LLM_Finetuning_For_Amharic_Ad_Generation/venv/lib/python3.11/site-packages (from torch) (3.1.3)\n",
      "Requirement already satisfied: fsspec in /home/eyaya_eneyew/week_7/LLM_Finetuning_For_Amharic_Ad_Generation/venv/lib/python3.11/site-packages (from torch) (2023.10.0)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /home/eyaya_eneyew/week_7/LLM_Finetuning_For_Amharic_Ad_Generation/venv/lib/python3.11/site-packages (from torch) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /home/eyaya_eneyew/week_7/LLM_Finetuning_For_Amharic_Ad_Generation/venv/lib/python3.11/site-packages (from torch) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /home/eyaya_eneyew/week_7/LLM_Finetuning_For_Amharic_Ad_Generation/venv/lib/python3.11/site-packages (from torch) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /home/eyaya_eneyew/week_7/LLM_Finetuning_For_Amharic_Ad_Generation/venv/lib/python3.11/site-packages (from torch) (8.9.2.26)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /home/eyaya_eneyew/week_7/LLM_Finetuning_For_Amharic_Ad_Generation/venv/lib/python3.11/site-packages (from torch) (12.1.3.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /home/eyaya_eneyew/week_7/LLM_Finetuning_For_Amharic_Ad_Generation/venv/lib/python3.11/site-packages (from torch) (11.0.2.54)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /home/eyaya_eneyew/week_7/LLM_Finetuning_For_Amharic_Ad_Generation/venv/lib/python3.11/site-packages (from torch) (10.3.2.106)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /home/eyaya_eneyew/week_7/LLM_Finetuning_For_Amharic_Ad_Generation/venv/lib/python3.11/site-packages (from torch) (11.4.5.107)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /home/eyaya_eneyew/week_7/LLM_Finetuning_For_Amharic_Ad_Generation/venv/lib/python3.11/site-packages (from torch) (12.1.0.106)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.18.1 in /home/eyaya_eneyew/week_7/LLM_Finetuning_For_Amharic_Ad_Generation/venv/lib/python3.11/site-packages (from torch) (2.18.1)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /home/eyaya_eneyew/week_7/LLM_Finetuning_For_Amharic_Ad_Generation/venv/lib/python3.11/site-packages (from torch) (12.1.105)\n",
      "Requirement already satisfied: triton==2.1.0 in /home/eyaya_eneyew/week_7/LLM_Finetuning_For_Amharic_Ad_Generation/venv/lib/python3.11/site-packages (from torch) (2.1.0)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12 in /home/eyaya_eneyew/week_7/LLM_Finetuning_For_Amharic_Ad_Generation/venv/lib/python3.11/site-packages (from nvidia-cusolver-cu12==11.4.5.107->torch) (12.3.101)\n",
      "Requirement already satisfied: numpy in /home/eyaya_eneyew/week_7/LLM_Finetuning_For_Amharic_Ad_Generation/venv/lib/python3.11/site-packages (from torchvision) (1.26.3)\n",
      "Requirement already satisfied: requests in /home/eyaya_eneyew/week_7/LLM_Finetuning_For_Amharic_Ad_Generation/venv/lib/python3.11/site-packages (from torchvision) (2.31.0)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /home/eyaya_eneyew/week_7/LLM_Finetuning_For_Amharic_Ad_Generation/venv/lib/python3.11/site-packages (from torchvision) (10.2.0)\n",
      "INFO: pip is looking at multiple versions of torchaudio to determine which version is compatible with other requirements. This could take a while.\n",
      "  Downloading https://download.pytorch.org/whl/cu118/torchaudio-2.1.2%2Bcu118-cp311-cp311-linux_x86_64.whl (3.2 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.2/3.2 MB\u001b[0m \u001b[31m80.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: MarkupSafe>=2.0 in /home/eyaya_eneyew/week_7/LLM_Finetuning_For_Amharic_Ad_Generation/venv/lib/python3.11/site-packages (from jinja2->torch) (2.1.4)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/eyaya_eneyew/week_7/LLM_Finetuning_For_Amharic_Ad_Generation/venv/lib/python3.11/site-packages (from requests->torchvision) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/eyaya_eneyew/week_7/LLM_Finetuning_For_Amharic_Ad_Generation/venv/lib/python3.11/site-packages (from requests->torchvision) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/eyaya_eneyew/week_7/LLM_Finetuning_For_Amharic_Ad_Generation/venv/lib/python3.11/site-packages (from requests->torchvision) (2.1.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/eyaya_eneyew/week_7/LLM_Finetuning_For_Amharic_Ad_Generation/venv/lib/python3.11/site-packages (from requests->torchvision) (2023.11.17)\n",
      "Requirement already satisfied: mpmath>=0.19 in /home/eyaya_eneyew/week_7/LLM_Finetuning_For_Amharic_Ad_Generation/venv/lib/python3.11/site-packages (from sympy->torch) (1.3.0)\n",
      "Installing collected packages: torchaudio\n",
      "Successfully installed torchaudio-2.1.2+cu118\n"
     ]
    }
   ],
   "source": [
    "! pip install datasets transformers[sentencepiece]\n",
    "!pip install -q accelerate==0.21.0 --progress-bar off\n",
    "!pip install -q peft==0.4.0 --progress-bar off\n",
    "!pip install -q bitsandbytes==0.40.2 --progress-bar off\n",
    "!pip install -q transformers==4.31.0 --progress-bar off\n",
    "!pip install -q trl==0.4.7 --progress-bar off\n",
    "!pip install git+https://github.com/huggingface/peft\n",
    "!pip install git+https://github.com/huggingface/accelerate\n",
    "!pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b53b748e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/eyaya_eneyew/week_7/LLM_Finetuning_For_Amharic_Ad_Generation/venv/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from random import randrange\n",
    "from functools import partial\n",
    "import torch\n",
    "from datasets import load_dataset\n",
    "from transformers import (AutoModelForCausalLM,\n",
    "                          AutoTokenizer,\n",
    "                          BitsAndBytesConfig,\n",
    "                          HfArgumentParser,\n",
    "                          Trainer,\n",
    "                          TrainingArguments,\n",
    "                          DataCollatorForLanguageModeling,\n",
    "                          EarlyStoppingCallback,\n",
    "                          pipeline,\n",
    "                          logging,\n",
    "                          set_seed)\n",
    "\n",
    "import bitsandbytes as bnb\n",
    "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training, PeftModel, AutoPeftModelForCausalLM\n",
    "from trl import SFTTrainer\n",
    "from contextlib import nullcontext\n",
    "from transformers import (\n",
    "    LlamaForCausalLM, \n",
    "    LlamaTokenizer, \n",
    "    TrainerCallback, \n",
    "    default_data_collator, \n",
    "    Trainer, \n",
    "    TrainingArguments,\n",
    "    BitsAndBytesConfig\n",
    ")\n",
    "from peft import (\n",
    "    LoraConfig,\n",
    "    TaskType,\n",
    "    prepare_model_for_int8_training,\n",
    "    PeftModel\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f4b3881b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_bnb_config(load_in_4bit, bnb_4bit_use_double_quant, bnb_4bit_quant_type, bnb_4bit_compute_dtype):\n",
    "    \"\"\"\n",
    "    Configures model quantization method using bitsandbytes to speed up training and inference\n",
    "\n",
    "    :param load_in_4bit: Load model in 4-bit precision mode\n",
    "    :param bnb_4bit_use_double_quant: Nested quantization for 4-bit model\n",
    "    :param bnb_4bit_quant_type: Quantization data type for 4-bit model\n",
    "    :param bnb_4bit_compute_dtype: Computation data type for 4-bit model\n",
    "    \"\"\"\n",
    "\n",
    "    bnb_config = BitsAndBytesConfig(\n",
    "        load_in_4bit = load_in_4bit,\n",
    "        bnb_4bit_use_double_quant = bnb_4bit_use_double_quant,\n",
    "        bnb_4bit_quant_type = bnb_4bit_quant_type,\n",
    "        bnb_4bit_compute_dtype = bnb_4bit_compute_dtype,\n",
    "    )\n",
    "\n",
    "    return bnb_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0b13706f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model(bnb_config):\n",
    "    \"\"\"\n",
    "    Loads model and model tokenizer\n",
    "\n",
    "    :param model_name: Hugging Face model name\n",
    "    :param bnb_config: Bitsandbytes configuration\n",
    "    \"\"\"\n",
    "    model_name = \"/model/Llama-2-7b-hf\"\n",
    "    #PT_DIR = \"/model/llama-2-amharic-3784m\"\n",
    "    # Get number of GPU device and set maximum memory\n",
    "    n_gpus = torch.cuda.device_count()\n",
    "    max_memory = f'{21960}MB'\n",
    "    tokenizer = LlamaTokenizer.from_pretrained(model_name)\n",
    "    # Load model\n",
    "    model = LlamaForCausalLM.from_pretrained(\n",
    "        model_name,\n",
    "        quantization_config = bnb_config,\n",
    "        device_map = \"auto\", # dispatch the model efficiently on the available resources\n",
    "        torch_dtype=torch.float16 #max_memory = {i: max_memory for i in range(n_gpus)},\n",
    "    )\n",
    "    #model = PeftModel.from_pretrained(model, PT_DIR)\n",
    "    # Load model tokenizer with the user authentication token\n",
    "    \n",
    "    #tokenizer = LlamaTokenizer.from_pretrained(model_name)\n",
    "    # Set padding token as EOS token\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    return model, tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d32a8e12",
   "metadata": {},
   "outputs": [],
   "source": [
    "################################################################################\n",
    "# transformers parameters\n",
    "################################################################################\n",
    "\n",
    "# The pre-trained model from the Hugging Face Hub to load and fine-tune\n",
    "# model_name = \"/model/Llama-2-7b-hf\"\n",
    "\n",
    "################################################################################\n",
    "# bitsandbytes parameters\n",
    "################################################################################\n",
    "\n",
    "# Activate 4-bit precision base model loading\n",
    "load_in_4bit = True\n",
    "\n",
    "# Activate nested quantization for 4-bit base models (double quantization)\n",
    "bnb_4bit_use_double_quant = True\n",
    "\n",
    "# Quantization type (fp4 or nf4)\n",
    "bnb_4bit_quant_type = \"nf4\"\n",
    "\n",
    "# Compute data type for 4-bit base models\n",
    "bnb_4bit_compute_dtype = torch.bfloat16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2127529e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using the legacy behaviour of the <class 'transformers.models.llama.tokenization_llama.LlamaTokenizer'>. This means that tokens that come after special tokens will not be properly handled. We recommend you to read the related pull request available at https://github.com/huggingface/transformers/pull/24565\n",
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:02<00:00,  1.06s/it]\n"
     ]
    }
   ],
   "source": [
    "bnb_config = create_bnb_config(load_in_4bit, bnb_4bit_use_double_quant, bnb_4bit_quant_type, bnb_4bit_compute_dtype)\n",
    "\n",
    "model, tokenizer = load_model(bnb_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "283255a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset\n",
    "dataset_name = \"/data/fine_tun_data2.json\"\n",
    "dataset = load_dataset(\"json\", data_files = dataset_name, split = \"train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ab1f43ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of prompts: 1611\n",
      "Column names are: ['output', 'instruction', 'input']\n"
     ]
    }
   ],
   "source": [
    "print(f'Number of prompts: {len(dataset)}')\n",
    "print(f'Column names are: {dataset.column_names}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cadeb60d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'output': 'advertisement',\n",
       " 'instruction': 'Categorize the input into one of the 2 categories:\\n\\nadvertisement\\nnot advertisement\\n\\n',\n",
       " 'input': 'ADVERTISMENT መልካም ነገር ከሉሲ የአጥንትና የመገጣጠሚያ ቀዶ ሕክምና ማእከል ለአጥንትና ለመገጣጠሚያ ችግሮችና ህመሞች አይነተኛ መፍትሄ የምንሰጣቸው የህክምና አገልግሎቶች ✔ማንኛውም ስብራትና ውልቃት ሕክምና ✔የጉልበትና ዳሌ መገጣጠሚያ ችግር ✔ውስብስብ ከባድ ስብራቶችና ውልቃቶች ✔የጡንቻና የጅማት ጉዳት ህመሞችን ማከም ✔የሕፃናት እና አዋቂዎች እጅና እግር መጣመም ማስተካከል ✔ስብራቶችንና ውልቃቶችን ያለ ቀዶ ሕክምና ማከም ✔በቀዶ ሕክምና የገቡ የተለያዩ ብረቶችን ማውጣት ✔የፕላስቲክ ሰርጀሪ ሕክምና ✔በዳሌና ዳሌ ገንዳ ስብራቶችን በፍሎሮስኮፒ የታገዘ ሕክምና ማድረግ 📌ልምድና ብቃት ባላቸው ስፔሻሊስት እና ሰብ ስፔሻሊስት ሀኪሞች እጅግ ዘመናዊ በሆኑ የህክምና መሳሪያወች በመታገዝ ሁሉንም የህክምና አገልግሎቶች በተመጣጣኝ ዋጋ ያገኛሉ‼ አድራሻ : አየር ጤና ጅማበር ፖሊስ ጣቢያ ጎን 📲 0913468103 0953912229'}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[randrange(len(dataset))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "be03889d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1611"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "94d04175",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_prompt_formats(sample):\n",
    "    \"\"\"\n",
    "    Creates a formatted prompt template for a prompt in the instruction dataset\n",
    "\n",
    "    :param sample: Prompt or sample from the instruction dataset\n",
    "    \"\"\"\n",
    "\n",
    "    # Initialize static strings for the prompt template\n",
    "    INTRO_BLURB = \"Below is an instruction that describes a task. Write a response that appropriately completes the request.\"\n",
    "    INSTRUCTION_KEY = \"### Instruction:\"\n",
    "    INPUT_KEY = \"Input:\"\n",
    "    RESPONSE_KEY = \"### Response:\"\n",
    "    END_KEY = \"### End\"\n",
    "\n",
    "    # Combine a prompt with the static strings\n",
    "    blurb = f\"{INTRO_BLURB}\"\n",
    "    instruction = f\"{INSTRUCTION_KEY}\\n{sample['instruction']}\"\n",
    "    input_context = f\"{INPUT_KEY}\\n{sample['input']}\" if sample[\"input\"] else None\n",
    "    response = f\"{RESPONSE_KEY}\\n{sample['output']}\"\n",
    "    end = f\"{END_KEY}\"\n",
    "\n",
    "    # Create a list of prompt template elements\n",
    "    parts = [part for part in [blurb, instruction, input_context, response, end] if part]\n",
    "\n",
    "    # Join prompt template elements into a single string to create the prompt template\n",
    "    formatted_prompt = \"\\n\\n\".join(parts)\n",
    "\n",
    "    # Store the formatted prompt template in a new key \"text\"\n",
    "    sample[\"text\"] = formatted_prompt\n",
    "\n",
    "    return sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c3879bf0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'output': 'advertisement',\n",
       " 'instruction': 'Categorize the input into one of the 2 categories:\\n\\nadvertisement\\nnot advertisement\\n\\n',\n",
       " 'input': 'ADVERTISMENT አፊያ ሙሀመድ ከፍተኛ የባህል ሕክምና እና ዘመናዊ የዋግምት አገልግሎት የምንሰጣቸው የባህል ህክምናዎች ➢ ለውጭና ለውስጥ ኪንታሮት ➢ ለማድያት ➢ ለሱኳር በሽታ ➢ለጉበት(ለወፍ በሽታ) ➢ለጨጎራ ህመም ➢ለስፈተወሲብ ➢ለደም ግፊት ➢ለአስም ወይም ሳይነስ ➢ለሚጥል በሺታ ➢ ለእሪህና ቁርጥማት ➢ለራስ ህመም (ማይግሪን) ➢ለቺፌ ና ለጭርት ➢ለቋቁቻና ፎረፎር ➢ለእጢና ለእባጭ ➢ለወገብ ህመም ➢ለመካንነት ለወድም ለሴትም ➢ለጆሮና ለአይን ህመም ➢ለሆድ ህመም ➢ዘመናዊ የዋግምት አገልግሎት በተቋማችን እንሰጣለን። 👉ከኢትዮጵያ ባህላዊ ህክምና አዋቂዎች ማህበር በዘርፉ ህጋዊ የባህል ህክምና ፍቃድ ያለን ነን። አድራሻ:አዲስ አበባ አየር ጤና ስልክ ቁጥር 📲0927506650 📲0987133734 📲0939605455 ቴሌግራም ቻናላችን ntvE5NmM0',\n",
       " 'text': 'Below is an instruction that describes a task. Write a response that appropriately completes the request.\\n\\n### Instruction:\\nCategorize the input into one of the 2 categories:\\n\\nadvertisement\\nnot advertisement\\n\\n\\n\\nInput:\\nADVERTISMENT አፊያ ሙሀመድ ከፍተኛ የባህል ሕክምና እና ዘመናዊ የዋግምት አገልግሎት የምንሰጣቸው የባህል ህክምናዎች ➢ ለውጭና ለውስጥ ኪንታሮት ➢ ለማድያት ➢ ለሱኳር በሽታ ➢ለጉበት(ለወፍ በሽታ) ➢ለጨጎራ ህመም ➢ለስፈተወሲብ ➢ለደም ግፊት ➢ለአስም ወይም ሳይነስ ➢ለሚጥል በሺታ ➢ ለእሪህና ቁርጥማት ➢ለራስ ህመም (ማይግሪን) ➢ለቺፌ ና ለጭርት ➢ለቋቁቻና ፎረፎር ➢ለእጢና ለእባጭ ➢ለወገብ ህመም ➢ለመካንነት ለወድም ለሴትም ➢ለጆሮና ለአይን ህመም ➢ለሆድ ህመም ➢ዘመናዊ የዋግምት አገልግሎት በተቋማችን እንሰጣለን። 👉ከኢትዮጵያ ባህላዊ ህክምና አዋቂዎች ማህበር በዘርፉ ህጋዊ የባህል ህክምና ፍቃድ ያለን ነን። አድራሻ:አዲስ አበባ አየር ጤና ስልክ ቁጥር 📲0927506650 📲0987133734 📲0939605455 ቴሌግራም ቻናላችን ntvE5NmM0\\n\\n### Response:\\nadvertisement\\n\\n### End'}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "create_prompt_formats(dataset[randrange(len(dataset))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e54a2523",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_max_length(model):\n",
    "    \"\"\"\n",
    "    Extracts maximum token length from the model configuration\n",
    "\n",
    "    :param model: Hugging Face model\n",
    "    \"\"\"\n",
    "\n",
    "    # Pull model configuration\n",
    "    conf = model.config\n",
    "    # Initialize a \"max_length\" variable to store maximum sequence length as null\n",
    "    max_length = None\n",
    "    # Find maximum sequence length in the model configuration and save it in \"max_length\" if found\n",
    "    for length_setting in [\"n_positions\", \"max_position_embeddings\", \"seq_length\"]:\n",
    "        max_length = getattr(model.config, length_setting, None)\n",
    "        if max_length:\n",
    "            print(f\"Found max lenth: {max_length}\")\n",
    "            break\n",
    "    # Set \"max_length\" to 1024 (default value) if maximum sequence length is not found in the model configuration\n",
    "    if not max_length:\n",
    "        max_length = 1024\n",
    "        print(f\"Using default max length: {max_length}\")\n",
    "    return max_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a0986992",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_batch(batch, tokenizer, max_length):\n",
    "    \"\"\"\n",
    "    Tokenizes dataset batch\n",
    "\n",
    "    :param batch: Dataset batch\n",
    "    :param tokenizer: Model tokenizer\n",
    "    :param max_length: Maximum number of tokens to emit from the tokenizer\n",
    "    \"\"\"\n",
    "\n",
    "    return tokenizer(\n",
    "        batch[\"text\"],\n",
    "        max_length = max_length,\n",
    "        truncation = True,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7fca9005",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_dataset(tokenizer, max_length, seed, dataset):\n",
    "    \"\"\"\n",
    "    Tokenizes dataset for fine-tuning\n",
    "\n",
    "    :param tokenizer (AutoTokenizer): Model tokenizer\n",
    "    :param max_length (int): Maximum number of tokens to emit from the tokenizer\n",
    "    :param seed: Random seed for reproducibility\n",
    "    :param dataset (str): Instruction dataset\n",
    "    \"\"\"\n",
    "\n",
    "    # Add prompt to each sample\n",
    "    print(\"Preprocessing dataset...\")\n",
    "    dataset = dataset.map(create_prompt_formats)\n",
    "\n",
    "    # Apply preprocessing to each batch of the dataset & and remove \"instruction\", \"input\", \"output\", and \"text\" fields\n",
    "    _preprocessing_function = partial(preprocess_batch, max_length = max_length, tokenizer = tokenizer)\n",
    "    dataset = dataset.map(\n",
    "        _preprocessing_function,\n",
    "        batched = True,\n",
    "        remove_columns = [\"instruction\", \"input\", \"output\", \"text\"],\n",
    "    )\n",
    "\n",
    "    # Filter out samples that have \"input_ids\" exceeding \"max_length\"\n",
    "    dataset = dataset.filter(lambda sample: len(sample[\"input_ids\"]) < max_length)\n",
    "\n",
    "    # Shuffle dataset\n",
    "    dataset = dataset.shuffle(seed = seed)\n",
    "\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a930433b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found max lenth: 2048\n",
      "Preprocessing dataset...\n"
     ]
    }
   ],
   "source": [
    "# Random seed\n",
    "seed = 22\n",
    "\n",
    "max_length = get_max_length(model)\n",
    "preprocessed_dataset = preprocess_dataset(tokenizer, max_length, seed, dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "57e75551",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['input_ids', 'attention_mask'],\n",
      "    num_rows: 1611\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "print(preprocessed_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5fe8fbfe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s>Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
      "\n",
      "### Instruction:\n",
      "Categorize the input into one of the 2 categories:\n",
      "\n",
      "advertisement\n",
      "not advertisement\n",
      "\n",
      "\n",
      "\n",
      "Input:\n",
      "የማስታወቂያ ግብዣ👇 የተከበራችሁ ቤተሰቦች ከ53,700 በላይ ተከታይ ባለው እና በደቂቃዎች ውስጥ በሺህ የሚቆጠሩ ጎብኝ ባለው በዚህ የቴሌግራም ቻናል ላይ በልዩ ቅናሽ ምርትና አገልግሎታችሁን እንድታስተዋውቁ በአክብሮት እጋብዛለሁ‼ ✔Youtube ቻናል … ✔የትምህርት ተቋማት… ✔የህክምና ማእከላት (ባህላዊ /ዘመናዊ)… ✔የትራንስፖርት አገልግሎት… ✔የስራ አገናኝ ኤጀንሲዎች…… ✔ሆቴል፣ካፌና ሬስቶራንቶች ✔ሌሎች የምትሰጧቸውንም አገልግሎቶችን ማስተዋወቅ ለምትፈልጉ ኑ አብረን እንስራ ብያለሁ‼ ማስታወቂያዎን በዚህ ይላኩ👇\n",
      "\n",
      "### Response:\n",
      "advertisement\n",
      "\n",
      "### End\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.decode(preprocessed_dataset[24]['input_ids']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8a005a43",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_peft_config(r, lora_alpha, target_modules, lora_dropout, bias, task_type):\n",
    "    \"\"\"\n",
    "    Creates Parameter-Efficient Fine-Tuning configuration for the model\n",
    "\n",
    "    :param r: LoRA attention dimension\n",
    "    :param lora_alpha: Alpha parameter for LoRA scaling\n",
    "    :param modules: Names of the modules to apply LoRA to\n",
    "    :param lora_dropout: Dropout Probability for LoRA layers\n",
    "    :param bias: Specifies if the bias parameters should be trained\n",
    "    \"\"\"\n",
    "    config = LoraConfig(\n",
    "        r = r,\n",
    "        lora_alpha = lora_alpha,\n",
    "        target_modules = target_modules,\n",
    "        lora_dropout = lora_dropout,\n",
    "        bias = bias,\n",
    "        task_type = task_type,\n",
    "    )\n",
    "\n",
    "    return config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f3c16af0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_all_linear_names(model):\n",
    "    \"\"\"\n",
    "    Find modules to apply LoRA to.\n",
    "\n",
    "    :param model: PEFT model\n",
    "    \"\"\"\n",
    "\n",
    "    cls = bnb.nn.Linear4bit\n",
    "    lora_module_names = set()\n",
    "    for name, module in model.named_modules():\n",
    "        if isinstance(module, cls):\n",
    "            names = name.split('.')\n",
    "            lora_module_names.add(names[0] if len(names) == 1 else names[-1])\n",
    "\n",
    "    if 'lm_head' in lora_module_names:\n",
    "        lora_module_names.remove('lm_head')\n",
    "    print(f\"LoRA module names: {list(lora_module_names)}\")\n",
    "    return list(lora_module_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "08645396",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_trainable_parameters(model, use_4bit = False):\n",
    "    \"\"\"\n",
    "    Prints the number of trainable parameters in the model.\n",
    "\n",
    "    :param model: PEFT model\n",
    "    \"\"\"\n",
    "\n",
    "    trainable_params = 0\n",
    "    all_param = 0\n",
    "\n",
    "    for _, param in model.named_parameters():\n",
    "        num_params = param.numel()\n",
    "        if num_params == 0 and hasattr(param, \"ds_numel\"):\n",
    "            num_params = param.ds_numel\n",
    "        all_param += num_params\n",
    "        if param.requires_grad:\n",
    "            trainable_params += num_params\n",
    "\n",
    "    if use_4bit:\n",
    "        trainable_params /= 2\n",
    "\n",
    "    print(\n",
    "        f\"All Parameters: {all_param:,d} || Trainable Parameters: {trainable_params:,d} || Trainable Parameters %: {100 * trainable_params / all_param}\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1bd43c3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fine_tune(model,\n",
    "          tokenizer,\n",
    "          dataset,\n",
    "          lora_r,\n",
    "          lora_alpha,\n",
    "          lora_dropout,\n",
    "          bias,\n",
    "          task_type,\n",
    "          per_device_train_batch_size,\n",
    "          gradient_accumulation_steps,\n",
    "          warmup_steps,\n",
    "          #warmup_ratio,\n",
    "          max_steps,\n",
    "          learning_rate,\n",
    "          fp16,\n",
    "          logging_steps,\n",
    "          output_dir,\n",
    "          optim):\n",
    "    \"\"\"\n",
    "    Prepares and fine-tune the pre-trained model.\n",
    "\n",
    "    :param model: Pre-trained Hugging Face model\n",
    "    :param tokenizer: Model tokenizer\n",
    "    :param dataset: Preprocessed training dataset\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Enable gradient checkpointing to reduce memory usage during fine-tuning\n",
    "        \n",
    "        model.train()\n",
    "        embedding_size = model.get_input_embeddings().weight.shape[0]\n",
    "\n",
    "        if len(tokenizer) != embedding_size:\n",
    "            print(\"resize the embedding size by the size of the tokenizer\")\n",
    "            model.resize_token_embeddings(len(tokenizer))\n",
    "            \n",
    "        model.resize_token_embeddings(len(tokenizer))\n",
    "\n",
    "        print('loading the pretrained model from config')\n",
    "        # Prepare the model for training\n",
    "        model = prepare_model_for_kbit_training(model)\n",
    "        model = PeftModel.from_pretrained(model, \"/model/llama-2-amharic-3784m\")\n",
    "        # Get LoRA module names\n",
    "        target_modules = find_all_linear_names(model)\n",
    "\n",
    "        # Create PEFT configuration for these modules and wrap the model to PEFT\n",
    "        peft_config = create_peft_config(lora_r, lora_alpha, target_modules, lora_dropout, bias, task_type)\n",
    "        model = get_peft_model(model, peft_config)\n",
    "\n",
    "        # Print information about the percentage of trainable parameters\n",
    "        print_trainable_parameters(model)\n",
    "\n",
    "        # Training parameters\n",
    "        trainer = Trainer(\n",
    "            model = model,\n",
    "            train_dataset = dataset,\n",
    "            args = TrainingArguments(\n",
    "                per_device_train_batch_size = per_device_train_batch_size,\n",
    "                gradient_accumulation_steps = gradient_accumulation_steps,\n",
    "                warmup_steps = warmup_steps,\n",
    "                max_steps = max_steps,\n",
    "                learning_rate = learning_rate,\n",
    "                fp16 = fp16,\n",
    "                logging_steps = logging_steps,\n",
    "                output_dir = output_dir,\n",
    "                optim = optim,\n",
    "            ),\n",
    "            data_collator = DataCollatorForLanguageModeling(tokenizer, mlm = False)\n",
    "        )\n",
    "\n",
    "        model.config.use_cache = False\n",
    "\n",
    "        do_train = True\n",
    "\n",
    "        # Launch training and log metrics\n",
    "        print(\"Training...\")\n",
    "\n",
    "        if do_train:\n",
    "            train_result = trainer.train()\n",
    "            metrics = train_result.metrics\n",
    "            trainer.log_metrics(\"train\", metrics)\n",
    "            trainer.save_metrics(\"train\", metrics)\n",
    "            trainer.save_state()\n",
    "            print(metrics)\n",
    "\n",
    "        # Save model\n",
    "        print(\"Saving last checkpoint of the model...\")\n",
    "        os.makedirs(output_dir, exist_ok = True)\n",
    "        trainer.model.save_pretrained(output_dir)\n",
    "\n",
    "        # Free memory for merging weights\n",
    "    except Exception as err:\n",
    "        print(\"Error:\",err)\n",
    "        del model\n",
    "        del trainer\n",
    "        torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "8b0d691a",
   "metadata": {},
   "outputs": [],
   "source": [
    "################################################################################\n",
    "# QLoRA parameters\n",
    "################################################################################\n",
    "\n",
    "# LoRA attention dimension\n",
    "lora_r = 16\n",
    "\n",
    "# Alpha parameter for LoRA scaling\n",
    "lora_alpha = 64\n",
    "\n",
    "# Dropout probability for LoRA layers\n",
    "lora_dropout = 0.05\n",
    "\n",
    "# Bias\n",
    "bias = \"none\"\n",
    "\n",
    "# Task type\n",
    "task_type = \"CAUSAL_LM\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "6496b86a",
   "metadata": {},
   "outputs": [],
   "source": [
    "################################################################################\n",
    "# TrainingArguments parameters\n",
    "################################################################################\n",
    "\n",
    "# Output directory where the model predictions and checkpoints will be stored\n",
    "output_dir = \"./results\"\n",
    "\n",
    "# Batch size per GPU for training\n",
    "per_device_train_batch_size = 1\n",
    "\n",
    "# Number of update steps to accumulate the gradients for\n",
    "gradient_accumulation_steps = 1\n",
    "\n",
    "# Initial learning rate (AdamW optimizer)\n",
    "learning_rate = 1e-5\n",
    "\n",
    "# Optimizer to use\n",
    "optim = \"paged_adamw_32bit\"\n",
    "\n",
    "# Number of training steps (overrides num_train_epochs)\n",
    "max_steps = 100\n",
    "\n",
    "# Linear warmup steps from 0 to learning_rate\n",
    "warmup_steps = 20\n",
    "\n",
    "# Enable fp16/bf16 training (set bf16 to True with an A100)\n",
    "fp16 = True\n",
    "\n",
    "# Log every X updates steps\n",
    "logging_steps = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "792d9ce5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading the pretrained model from config\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LoRA module names: ['base_layer']\n",
      "All Parameters: 4,453,765,120 || Trainable Parameters: 359,792,640 || Trainable Parameters %: 8.078392782419563\n",
      "Training...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='100' max='100' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [100/100 03:44, Epoch 0/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>8.400400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>7.342400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>13.039400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>8.154800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>8.285000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>6.589800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>6.836900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>8.888700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>6.511000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>7.240900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>4.983800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>4.793600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>4.500400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>4.379400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>4.317100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>4.661400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>4.146600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>3.521500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>4.064600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>3.986800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21</td>\n",
       "      <td>3.345000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22</td>\n",
       "      <td>3.806700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23</td>\n",
       "      <td>3.063500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24</td>\n",
       "      <td>3.433700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>2.546700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26</td>\n",
       "      <td>2.562700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27</td>\n",
       "      <td>3.022900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28</td>\n",
       "      <td>3.333800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29</td>\n",
       "      <td>3.976600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>1.289900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>31</td>\n",
       "      <td>2.174800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>32</td>\n",
       "      <td>2.464900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>33</td>\n",
       "      <td>2.355400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>34</td>\n",
       "      <td>2.381300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>35</td>\n",
       "      <td>2.113400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>36</td>\n",
       "      <td>2.873200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>37</td>\n",
       "      <td>1.121200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>38</td>\n",
       "      <td>2.744100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>39</td>\n",
       "      <td>2.352100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>3.411900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>41</td>\n",
       "      <td>1.628600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>42</td>\n",
       "      <td>2.149500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>43</td>\n",
       "      <td>1.817300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>44</td>\n",
       "      <td>1.505400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>45</td>\n",
       "      <td>1.483800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>46</td>\n",
       "      <td>3.219300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>47</td>\n",
       "      <td>2.675000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>48</td>\n",
       "      <td>1.337400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>49</td>\n",
       "      <td>3.105900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>2.259700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>51</td>\n",
       "      <td>3.393500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>52</td>\n",
       "      <td>3.000100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>53</td>\n",
       "      <td>3.403200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>54</td>\n",
       "      <td>2.193000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>55</td>\n",
       "      <td>2.949600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>56</td>\n",
       "      <td>3.144900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>57</td>\n",
       "      <td>1.478700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>58</td>\n",
       "      <td>3.526500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>59</td>\n",
       "      <td>2.768000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>1.068900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>61</td>\n",
       "      <td>2.047100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>62</td>\n",
       "      <td>1.530900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>63</td>\n",
       "      <td>1.359300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>64</td>\n",
       "      <td>3.149200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>65</td>\n",
       "      <td>2.251200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>66</td>\n",
       "      <td>1.162300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>67</td>\n",
       "      <td>1.755500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>68</td>\n",
       "      <td>0.978400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>69</td>\n",
       "      <td>1.908300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>70</td>\n",
       "      <td>1.846500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>71</td>\n",
       "      <td>2.553300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>72</td>\n",
       "      <td>0.704100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>73</td>\n",
       "      <td>3.209500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>74</td>\n",
       "      <td>0.699700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>75</td>\n",
       "      <td>1.950200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>76</td>\n",
       "      <td>2.190700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>77</td>\n",
       "      <td>0.979500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>78</td>\n",
       "      <td>0.772000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>79</td>\n",
       "      <td>2.934900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>0.766000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>81</td>\n",
       "      <td>3.318100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>82</td>\n",
       "      <td>3.338400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>83</td>\n",
       "      <td>1.855500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>84</td>\n",
       "      <td>2.491100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>85</td>\n",
       "      <td>0.846400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>86</td>\n",
       "      <td>2.520400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>87</td>\n",
       "      <td>2.302000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>88</td>\n",
       "      <td>2.713000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>89</td>\n",
       "      <td>2.316600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>90</td>\n",
       "      <td>2.493800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>91</td>\n",
       "      <td>2.607500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>92</td>\n",
       "      <td>2.840500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>93</td>\n",
       "      <td>2.171300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>94</td>\n",
       "      <td>2.432000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>95</td>\n",
       "      <td>0.774900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>96</td>\n",
       "      <td>1.945800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>97</td>\n",
       "      <td>0.614300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>98</td>\n",
       "      <td>2.825700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>99</td>\n",
       "      <td>2.144300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>1.240000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***** train metrics *****\n",
      "  epoch                    =       0.06\n",
      "  total_flos               =   532552GF\n",
      "  train_loss               =     3.0567\n",
      "  train_runtime            = 0:03:46.86\n",
      "  train_samples_per_second =      0.441\n",
      "  train_steps_per_second   =      0.441\n",
      "{'train_runtime': 226.8638, 'train_samples_per_second': 0.441, 'train_steps_per_second': 0.441, 'total_flos': 571823530131456.0, 'train_loss': 3.056667756438255, 'epoch': 0.06}\n",
      "Saving last checkpoint of the model...\n"
     ]
    }
   ],
   "source": [
    "fine_tune(model,\n",
    "      tokenizer,\n",
    "      preprocessed_dataset,\n",
    "      lora_r,\n",
    "      lora_alpha,\n",
    "      lora_dropout,\n",
    "      bias,\n",
    "      task_type,\n",
    "      per_device_train_batch_size,\n",
    "      gradient_accumulation_steps,\n",
    "      warmup_steps,\n",
    "      max_steps,\n",
    "      learning_rate,\n",
    "      fp16,\n",
    "      logging_steps,\n",
    "      output_dir,\n",
    "      optim)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "910fbddd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_prompt_formats_for_test(sample):\n",
    "    'Categorize the input into one of the 2 categories:\\n\\nadvertisement\\nnot advertisement\\n\\n'\n",
    "    \"\"\"\n",
    "    Format various fields of the sample ('input', 'label',)\n",
    "    Then concatenate them using two newline characters\n",
    "    :param sample: Sample dictionnary\n",
    "    \"\"\"\n",
    "\n",
    "    # Initialize static strings for the prompt template\n",
    "    INTRO_BLURB = \"Below is an instruction that describes a task. Write a response that appropriately completes the request.\"\n",
    "    INSTRUCTION_KEY = \"### Instruction:\"\n",
    "    INPUT_KEY = \"Input:\"\n",
    "    RESPONSE_KEY = \"### Response:\"\n",
    "    END_KEY = \"### End\"\n",
    "\n",
    "    # Combine a prompt with the static strings\n",
    "    blurb = f\"{INTRO_BLURB}\"\n",
    "    instruction = f\"{INSTRUCTION_KEY}\\nCategorize the input into one of the two categories output only either advertisement or not advertisement. NOTHING ELSE.\\n\\n\"\n",
    "    input_context = f\"{INPUT_KEY}\\n{sample['input']}\" if sample[\"input\"] else None\n",
    "    response = f\"{RESPONSE_KEY}\"\n",
    "    end = f\"{END_KEY}\"\n",
    "\n",
    "    # Create a list of prompt template elements\n",
    "    parts = [part for part in [blurb, instruction, input_context,response,end] if part]\n",
    "\n",
    "    # Join prompt template elements into a single string to create the prompt template\n",
    "    formatted_prompt = \"\\n\\n\".join(parts)\n",
    "\n",
    "    # Store the formatted prompt template in a new key \"text\"\n",
    "    sample[\"text\"] = formatted_prompt\n",
    "\n",
    "    #sample[\"input\"] = formatted_prompt\n",
    "\n",
    "    return sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "7ca84d70",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'output': 'advertisement',\n",
       " 'instruction': 'Categorize the input into one of the 2 categories:\\n\\nadvertisement\\nnot advertisement\\n\\n',\n",
       " 'input': 'የምስራች! 🇪🇹 በሀገራችን ምርት እንጠቀም! እንኩራ! የRingAround App ይሞክሩን tikvah downloadringaroundapp በውጭ ሀገር ለምትኖሩ ኢትዮጵያውያን! ሳያመልጣችሁ በሀገራችን የቴክኖሎጂ ጠቢባን ተዘጋጅቶ የቀረበውን አለም አቀፉን የRingAround Appን በስልካችሁ ላይ ቶሎ በመጫን ወደ ሀገር ቤት ይደውሉ!'}"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "0beb08a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = dataset[10]\n",
    "\n",
    "prompt = create_prompt_formats_for_test(sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b42aaf9",
   "metadata": {},
   "source": [
    "## Inference using Instruction or Question Only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "0bfa766f",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_text = f\"Instruction: {prompt['text']}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "cbb9dba2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Instruction: Below is an instruction that describes a task. Write a response that appropriately completes the request.\\n\\n### Instruction:\\nCategorize the input into one of the 2 categories output only either advertisement or not advertisement. NOTHING ELSE.\\n\\n\\n\\nInput:\\nየምስራች! 🇪🇹 በሀገራችን ምርት እንጠቀም! እንኩራ! የRingAround App ይሞክሩን tikvah downloadringaroundapp በውጭ ሀገር ለምትኖሩ ኢትዮጵያውያን! ሳያመልጣችሁ በሀገራችን የቴክኖሎጂ ጠቢባን ተዘጋጅቶ የቀረበውን አለም አቀፉን የRingAround Appን በስልካችሁ ላይ ቶሎ በመጫን ወደ ሀገር ቤት ይደውሉ!\\n\\n### Response:'"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "25103de1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#text_i = 'ADVERTISMENT 🏑 መልካም ገና 🏑 🔜 💥ለ3 ቀናት ብቻ የሚቆይ ታላቅ'\n",
    "text_i = 'ማስታወቂያ! ድሮጋ የፊዚዮቴራፒ ህክምና ልዩ ክሊኒክ፦ ዘመኑ ባፈራቸው የመጨረሻ የህክምና ቴክኖሎጂ በመታገዝና በሙያው ስፔሻላይዝ ባደረጉ የፊዝዮቴራፒ ሃኪሞች በህፃናት ላይ በሚከሰቱ የጭንቅላት ላይ ጉዳቶች አልያም ማንኛውንም የእንቅስቃሴ ጉድለትና የአካል አለመታዘዝ ችግሮች ፍቱን የሆነ ህክምና በማድረግ ወደ ተሟላ የሰውነት እንቅስቃሴ እንመልሳለን። አአ ሚግሬሸን ጀርባ ለበለጠ መረጃ 0974 95 95 95 like us on FB @drogaphysiotherapy'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "6ec5633f",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_i = 'ታላቅ ቅናሽ ከዲ ኤም ሲ (DMC) ሪል እስቴት 5% ከፈለው ይመዝገቡ የሚፈልጉትን ቤት ይምረጡ ከ ስቲድዮ እስከ ባለ 4 መኝታ ቤቶች በተለያየ የካሬ ኣማራጭ ዘመናዊ አፓርትመንቶችን በመሸጥ ላይ ነን'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "fc5900c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = f'Instruction: Below is an instruction that describes a task. Write a response that appropriately completes the request.\\n\\n### Instruction:\\nCategorize the input into one of the 2 categories output only either advertisement or not advertisement. NOTHING ELSE.\\n\\n\\n\\nInput:\\n{text_3}\\n\\n### Response:'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "d53c42c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = 'what medicine should I take if I got a flu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "1f77d235",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_3 = 'ደና ዋላቹ'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "36b6233c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize the input\n",
    "input_ids = tokenizer.encode(query, return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "# Measure inference time\n",
    "start_time = time.time()\n",
    "batch = tokenizer(query, return_tensors=\"pt\")\n",
    "batch = {k: v.to(\"cuda\") for k, v in batch.items()}\n",
    "#start = time.perf_counter()\n",
    "# Generate predictions\n",
    "#output = model.generate(input_ids, max_length=500, temperature=1.0, top_k=50, top_p=0.95, num_return_sequences=1)\n",
    "output = model.generate(**batch,  max_new_tokens=400,\n",
    "                do_sample=True,\n",
    "                top_p=1,\n",
    "                temperature=1,\n",
    "                use_cache=True,\n",
    "                top_k=1,\n",
    "                repetition_penalty=1,\n",
    "                length_penalty=1)\n",
    "generated_output = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "\n",
    "end_time = time.time()\n",
    "\n",
    "# Calculate and print the inference time\n",
    "inference_time = end_time - start_time\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "cd7f38f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Instruction: Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
      "\n",
      "### Instruction:\n",
      "Categorize the input into one of the 2 categories output only either advertisement or not advertisement. NOTHING ELSE.\n",
      "\n",
      "\n",
      "\n",
      "Input:\n",
      "ደና ዋላቹ\n",
      "\n",
      "### Response:\n",
      "advertisement\n",
      "\n",
      "### End###\n",
      "### Instruction:\n",
      "Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
      "\n",
      "### Instruction:\n",
      "Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
      "\n",
      "### End###\n",
      "### Instruction:\n",
      "Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
      "\n",
      "### End###\n",
      "### Instruction:\n",
      "Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
      "\n",
      "### End###\n",
      "### Instruction:\n",
      "Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
      "\n",
      "### End###\n",
      "### Instruction:\n",
      "Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
      "\n",
      "### End###\n",
      "### Instruction:\n",
      "Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
      "\n",
      "### End###\n",
      "### Instruction:\n",
      "Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
      "\n",
      "### End###\n",
      "### Instruction:\n",
      "Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
      "\n",
      "### End###\n",
      "### Instruction:\n",
      "Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
      "\n",
      "### End###\n",
      "### Instruction:\n",
      "Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
      "\n",
      "### End###\n",
      "### Instruction:\n",
      "Below\n"
     ]
    }
   ],
   "source": [
    "print(generated_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "id": "1890d24b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======\n",
      "Input:\n",
      "======\n",
      "Instruction: Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
      "\n",
      "### Instruction:\n",
      "ጥቆማ📌የዩኒቨርሲቲ ተማሪዎች የጥሪ ቀናችሁን ከተቋማችሁ ትክክለኛ የፌስቡክ ገፅ ወይም ከድህረገፅ ላይ ካላገኛችሁ እንዲሁም በማህተም የተደገፉ የማስታወቂያ ፅሁፎችን ካላያችሁ ልታምኑ አይገባም። በመሆኑንም የተጠራችሁበትን ቀን ለማወቅ ቀጥታ በዩኒቨርሲታያችሁ የፌስቡክ ገፅ እየገባችሁ አረጋግጡ። እንዲሁም የተማሪ ህብረት ተወካይ ስልክ እና የሬጅስትራር ቢሮ ስልክ ቁጥሮችን መያዝ ባህላችሁ አድርጉ። @tsegabwolde @tikvahethiopia\n",
      "\n",
      "======================\n",
      "Generated Output:\n",
      "======================\n",
      "Instruction: Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
      "\n",
      "### Instruction:\n",
      "ጥቆማ📌የዩኒቨርሲቲ ተማሪዎች የጥሪ ቀናችሁን ከተቋማችሁ ትክክለኛ የፌስቡክ ገፅ ወይም ከድህረገፅ ላይ ካላገኛችሁ እንዲሁም በማህተም የተደገፉ የማስታወቂያ ፅሁፎችን ካላያችሁ ልታምኑ አይገባም። በመሆኑንም የተጠራችሁበትን ቀን ለማወቅ ቀጥታ በዩኒቨርሲታያችሁ የፌስቡክ ገፅ እየገባችሁ አረጋግጡ። እንዲሁም የተማሪ ህብረት ተወካይ ስልክ እና የሬጅስትራር ቢሮ ስልክ ቁጥሮችን መያዝ ባህላችሁ አድርጉ። @tsegabwolde @tikvahethiopia\n",
      "\n",
      "### Response:\n",
      "ተማሪዎችን ለጥሪ ቀናችሁን ከተቋማችሁ ትክክለኛ የፌስቡክ ገፅ ወይም ከድህረገፅ ላይ ካላቀዘቁ እንዲሁም በማህተም የተደገፉ የማስታወቂያ ፅሁፎችን ካላያችሁ ልታምኑን አይደለም። በዚህምም ተጠርጣችሁን የመወጣችሁን ቀን ለማወቅ ቀጥታ በዩኒቨርሲታያችሁ የፌስቡክ ገፅ ውስጥ ገብተዳችሁ። እንዲሁም የተማሪ ህብረት ተወካይ ስልክ እና ሬጅስትራር ቢሮ ስልክ ቁጥሮችን መያዝ ባህላችሁ አድርጉ። @tikvahethiopia\n",
      "\n",
      "### End of Instruction\n",
      "\n",
      "### End of Response\n",
      "\n",
      "### End of Chናለ\n",
      "\n",
      "### End of Tikvahethiopia\n",
      "\n",
      "### End of Tikvahethiopia\n",
      "\n",
      "### End of Tikvahethiopia\n",
      "### End of Tikvahethiopia\n",
      "### End of Tikvahethiopia\n",
      "### End of Tikvahethiopia\n",
      "### End of Tikvahethiopia\n",
      "### End of Tikvahethiopia\n",
      "### End of Tikvahethiopia\n",
      "### End of Tikvahethiopia\n",
      "### End of Tikvahethiopia\n",
      "### End of Tikvahethiopia\n",
      "### End of Tikvahethiopia\n",
      "### End of Tikvahethiopia\n",
      "### End of Tikvahethiopia\n",
      "### End of Tikvahethiopia\n",
      "### End of Tikvahethiopia\n",
      "### End of Tikvahethiopia\n",
      "### End of Tikvahethiopia\n",
      "### End of Tikvahethiopia\n",
      "### End of Tikvahethiopia\n",
      "###\n",
      "\n",
      "=========================================\n",
      "Inference Time:103.68379998207092 seconds\n",
      "==========================================\n"
     ]
    }
   ],
   "source": [
    "# Print the formatted input\n",
    "print(f\"======\")\n",
    "print(f\"Input:\\n======\\n{input_text}\\n\")\n",
    "print(f\"======================\")\n",
    "print(f\"Generated Output:\\n======================\\n{generated_output}\\n\")\n",
    "print(f\"=========================================\")\n",
    "print(f\"Inference Time:{inference_time} seconds\\n==========================================\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ebf475f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
