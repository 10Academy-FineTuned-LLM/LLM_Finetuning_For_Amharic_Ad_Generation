{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove:\n",
    "* Null values, new_line(\"\\n\"), hashtags(\"#*\"), emojis, other characters\n",
    "### Replace:\n",
    "* ['áˆ', 'áˆ‘', 'áˆ’', 'áˆ“', 'áˆ”', 'áˆ–'] with ['áˆ€', 'áˆ', 'áˆ‚', 'áˆƒ', 'áˆ„', 'áˆ…', 'áˆ†']\n",
    "* ['áŠ€', 'áŠ', 'áŠ‚', 'áŠƒ', 'áŠ„', 'áŠ…', 'áŠ†'] with ['áˆ€', 'áˆ', 'áˆ‚', 'áˆƒ', 'áˆ„', 'áˆ…', 'áˆ†']\n",
    "* ['áˆ ', 'áˆ¡', 'áˆ¢', 'áˆ£', 'áˆ¤', 'áˆ¦', 'áˆ¦', 'áˆ§'] with ['áˆ°, 'áˆ±', 'áˆ²', 'áˆ³', 'áˆ´', 'áˆµ', 'áˆ¶', 'áˆ·']\n",
    "* ['á‹', 'á‹‘', 'á‹’', 'á‹“', 'á‹”', 'á‹•', 'á‹–'] with ['áŠ ', 'áŠ¡', 'áŠ¢', 'áŠ£', 'áŠ¤', 'áŠ¥', 'áŠ¦']\n",
    "* ['áŒ¸', 'áŒ¹', 'áŒº', 'áŒ»', 'áŒ¼', 'áŒ½', 'áŒ¾'] with ['á€', 'á', 'á‚', 'áƒ', 'á„', 'á…', 'á†']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os\n",
    "import pandas as pd\n",
    "sys.path.append(os.path.abspath(os.path.join('../src/')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "letters = [\n",
    "  [['áˆ', 'áˆ‘', 'áˆ’', 'áˆ“', 'áˆ”', 'áˆ–'], ['áˆ€', 'áˆ', 'áˆ‚', 'áˆƒ', 'áˆ„', 'áˆ…', 'áˆ†']],\n",
    "  [['áŠ€', 'áŠ', 'áŠ‚', 'áŠƒ', 'áŠ„', 'áŠ…', 'áŠ†'], ['áˆ€', 'áˆ', 'áˆ‚', 'áˆƒ', 'áˆ„', 'áˆ…', 'áˆ†']],\n",
    "  [['áˆ ', 'áˆ¡', 'áˆ¢', 'áˆ£', 'áˆ¤', 'áˆ¦', 'áˆ¦', 'áˆ§'], ['áˆ°', 'áˆ±', 'áˆ²', 'áˆ³', 'áˆ´', 'áˆµ', 'áˆ¶', 'áˆ·']],\n",
    "  [['á‹', 'á‹‘', 'á‹’', 'á‹“', 'á‹”', 'á‹•', 'á‹–'], ['áŠ ', 'áŠ¡', 'áŠ¢', 'áŠ£', 'áŠ¤', 'áŠ¥', 'áŠ¦']],\n",
    "  [['áŒ¸', 'áŒ¹', 'áŒº', 'áŒ»', 'áŒ¼', 'áŒ½', 'áŒ¾'], ['á€', 'á', 'á‚', 'áƒ', 'á„', 'á…', 'á†']]\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Set directories for parsed data and cleaned data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "parsed_dir = \"../data/parsed\"\n",
    "cleaned_dir = \"../data/cleaned\"\n",
    "util = utils.Util()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_parsed_data(folder_path):\n",
    "    # Check if the provided path is a directory\n",
    "    if not os.path.isdir(folder_path):\n",
    "        print(f\"{folder_path} is not a valid directory.\")\n",
    "        return\n",
    "    \n",
    "    # check if cleaned_dir exists\n",
    "    if not os.path.exists(cleaned_dir):\n",
    "            os.makedirs(cleaned_dir)\n",
    "\n",
    "    # Loop through each file in the directory\n",
    "    for file_name in os.listdir(folder_path):\n",
    "        base_name, extension = os.path.splitext(file_name)\n",
    "        print(base_name,extension)\n",
    "        if extension =='.csv':\n",
    "            df = pd.read_csv(f\"{folder_path}/{file_name}\", index_col='id')\n",
    "            df = df.dropna()\n",
    "            df = df.replace('\\n', ' ', regex=True)\n",
    "            # Extract and remove hashtags\n",
    "            df['hashtags'] = df['text'].apply(lambda x: util.extract_hashtags(x))\n",
    "            df['text'] = df['text'].str.replace(r'\\#\\w+', '', regex=True)\n",
    "            \n",
    "            # Extract and remove emojis using regex\n",
    "            df['emojis'] = df['text'].apply(util.extract_emojis)\n",
    "            df['text'] = df['text'].apply(util.remove_emojis)\n",
    "\n",
    "            for letter in letters:\n",
    "                for i in range(len(letter[0])):\n",
    "                    df['text'] = df['text'].str.replace(letter[0][i], letter[1][i])\n",
    "            # extract and Reove symbols\n",
    "            df['symbols'] = df['text'].apply(util.extract_symbols)\n",
    "            df['text'] = df['text'].apply(util.remove_symbols)\n",
    "            # extract and remove urls\n",
    "            df['links'] = df['text'].apply(util.extract_urls)\n",
    "\n",
    "            df['text'] = df['text'].str.replace('\\s+', ' ', regex=True).str.strip()\n",
    "            df['text'] = df['text'].replace(r'!+', '!', regex=True)\n",
    "            df['text'] = df['text'].replace(r'\\.+', '', regex=True)\n",
    "            base_name, extension = os.path.splitext(file_name)\n",
    "            df.to_csv(f\"{cleaned_dir}/{base_name}.csv\")\n",
    "            df['text'].to_csv(f\"{cleaned_dir}/{base_name}.txt\", index=False, header=False)\n",
    "\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "á‹³áŒ‰ áˆµá–áˆ­á‰µ DAGU SPORT .csv\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "á‰¥áˆµáˆ«á‰µ áˆµá–áˆ­á‰µ .csv\n",
      "DID U KNOWï¸â‰ï¸ .csv\n",
      "ğŸ”¶WonderğŸ¤” .csv\n",
      "ETHIO-MEREJAÂ® .csv\n",
      "Sheger Pressï¸ï¸ .csv\n",
      "QUBEE ACADEMY .csv\n",
      "90 á‹°á‰‚á‰ƒ áˆµá–áˆ­á‰µâ„¢ .csv\n",
      "4-3-3 FAST SPORTâ„¢ .csv\n",
      "THE GOAT LMâ™¾ ğŸ .csv\n",
      "TIKVAH .csv\n",
      "Wasu Mohammed(á‹‹áˆ± áˆ˜áˆ€áˆ˜á‹µ) .csv\n",
      "ETHIO ARSENAL .csv\n",
      "áŠ¢á‰µá‹® áˆ˜áˆ¨áŒƒ - NEWS .csv\n",
      "ğŸ˜Best Profile PicturesğŸ˜ .csv\n",
      "History ğŸ“š .csv\n",
      "DREAM APPâ„¢ .csv\n",
      "áŠ áˆµá‹°áŠ“á‰‚ áŠ¥á‹áŠá‰³á‹á‰½ ğŸŒ .csv\n",
      "áŠ á‹²áˆµ áŠáŒˆáˆ­ áˆ˜áˆ¨áŒƒ .csv\n",
      "áŠ¢á‰µá‹® áˆªá‹«áˆ áˆ›á‹µáˆªá‹µ .csv\n",
      "DREAM SPORT â„¢ .csv\n",
      "Ethio University NewsÂ® .csv\n",
      "ğŸ’•ğŸ„áá‰…áˆ­áŠ• á‰ á‰ƒáˆ‹á‰µğŸ„ğŸ’• .csv\n",
      "YeneTube .csv\n",
      "Manchester United Fansâ„¢ .csv\n"
     ]
    }
   ],
   "source": [
    "clean_parsed_data(parsed_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
